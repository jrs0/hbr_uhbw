title: "Bleeding and Ischaemia Risk Estimation Models"
subtitle: "Trained using hospital episode statistics, secondary care prescriptions and laboratory results, and primary care data"
author: "John Scott"

# Path to load data from and save data to.
save_dir: "../save_data"

# Base name for this analysis. The string "_data" is
# appended to this name to create the file to load. This
# base name is also used as the prefix for saved files.
analysis_name: "icb_hic"

# Test/train split
test_proportion: 0.25

# All randomness is derived from this seed.
seed: 0

# Using a larger number of bootstrap resamples will make
# the stability analysis better, but will take longer to fit.
num_bootstraps: 10

# Choose the number of bins for the calibration calculation.
# Using more bins will resolve the risk estimates more
# precisely, but will reduce the sample size in each bin for
# estimating the prevalence.
num_bins: 5

# References
bib_file: "../risk_management_file/ref.bib"
citation_style: "citation_style.csl"

# Jinja2 templates
templates_folder: "templates"
report_template: "report.qmd"

# Build output locations. The report will be
# placed in a folder called {analysis_name}_report
build_directory: "build"

# Models to include
models:
  random_forest:
    text: "random forest"
    title: "Random Forest"
    abbr: "RF"
    pipe_fn: "pyhbr.analysis.model.make_random_forest_cv"
    config:
      n_estimates: "scipy.stats.randint(50, 500)"
      max_depth: "scipy.stats.randint(1, 20)"
    description: >
      The risk estimate is an average over decision trees, each of which
      estimates the risk by recursively dividing the input predictors into
      ranges and estimating the prevalence within each range of inputs. 100
      trees are used, and each tree is capped at a depth of 10 to avoid overfitting.
  logistic_regression:
    text: "logistic regression"
    title: "Logistic Regression"
    abbr: "LR"
    pipe_fn: "pyhbr.analysis.model.make_logistic_regression"
    description: >
       The risk is obtained by fitting the logistic regression model to the events,
       using the input features are variables. A penalty term is added to handle
       collinearity of the predictors and to penalize overfitting.
  xgboost:
    text: "XGBoost"
    title: "XGBoost"
    abbr: "XGB"
    pipe_fn: "pyhbr.analysis.model.make_xgboost"
    description: >
      An alternative tree-based method, similar to random forest, that uses a
      different algorithm to obtained train the model.
  nearest_neighbours:
    text: "K-nearest neighbours"
    title: "K-Nearest Neighbours"
    abbr: "KNN"
    pipe_fn: "pyhbr.analysis.model.make_nearest_neighbours_cv"
    config:
      n_neighbors: "scipy.stats.randint(1, 15)"
      weights: "['uniform', 'distance']"
    description: >
      A model that predicts the value for an input based on the corresponding
      values for known similar inputs (the nearest neighbours).
  support_vector_machine:
    text: "support vector machine"
    title: "Support Vector Machine"
    abbr: "SVM"
    pipe_fn: "pyhbr.analysis.model.make_svm"
    config: {}
    description: >
      A model that attempts to collect the training data into groups based on the
      maximum margin between the groups, and makes predictions based on these groups.
  multilayer_perceptron:
    text: "multi-layer perceptron"
    title: "Multi-layer Perceptron"
    abbr: "SVM"
    pipe_fn: "pyhbr.analysis.model.make_mlp"
    config:
      alpha: "1e-5"
      hidden_layer_sizes: "(20, 2)"
      solver: "lbfgs"
      max_iter: 1000
    description: >
      A type of neural network with a fixed number of layers, which can learn non-linear
        relationships among the input data and the output classes.

# Outcome names
outcomes:
  bleeding:
    text: "bleeding"
    title: "Bleeding" 
    abbr: "B"
  ischaemia:
    text: "ischaemia"
    title: "Ischaemia"
    abbr: "I"


