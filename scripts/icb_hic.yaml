title: "Estimating bleeding and ischaemia risk following acute coronary syndrome"
subtitle: "A machine learning modelling study using routine electronic health data"
author: "John Scott"

# Path to load data from and save data to.
save_dir: "../save_data"

# Base name for this analysis. The string "_data" is
# appended to this name to create the file to load. This
# base name is also used as the prefix for saved files.
analysis_name: "icb_hic"

# Test/train split
test_proportion: 0.25

# All randomness is derived from this seed.
seed: 0

# Using a larger number of bootstrap resamples will make
# the stability analysis better, but will take longer to fit.
num_bootstraps: 10

# Choose the number of bins for the calibration calculation.
# Using more bins will resolve the risk estimates more
# precisely, but will reduce the sample size in each bin for
# estimating the prevalence.
num_bins: 5

# References
bib_file: "../risk_management_file/ref.bib"
citation_style: "citation_style.csl"

# Jinja2 templates
templates_folder: "templates"
report_template: "report.qmd"

# Build output locations. The report will be
# placed in a folder called {analysis_name}_report
build_directory: "build"

# Models to include
models:
  random_forest:
    text: "random forest"
    title: "Random Forest"
    abbr: "RF"
    pipe_fn: "pyhbr.analysis.model.make_random_forest_cv"
    config:
      n_estimates: "scipy.stats.randint(50, 500)"
      max_depth: "scipy.stats.randint(1, 20)"
    description: >
      The risk estimate is an average over decision trees, each of which
      estimates the risk by recursively dividing the input predictors into
      ranges and estimating the prevalence within each range of inputs. 100
      trees are used, and each tree is capped at a depth of 10 to avoid overfitting.
    roc_conclusion: >
      Test
  ada_boost:
    text: "AdaBoost"
    title: "AdaBoost"
    abbr: "AB"
    pipe_fn: "pyhbr.analysis.model.make_abc"
    config:
      "n_estimators": "50"
      "learning_rate": "1.0"
      "algorithm": "'SAMME.R'"
    description: >
      An ensemble method fitting simple trees to the data, where later iterations of the
      fit focus on more difficult cases.
    roc_conclusion: >
      Test
  logistic_regression:
    text: "logistic regression"
    title: "Logistic Regression"
    abbr: "LR"
    pipe_fn: "pyhbr.analysis.model.make_logistic_regression"
    description: >
       The risk is obtained by fitting the logistic regression model to the events,
       using the input features are variables. A penalty term is added to handle
       collinearity of the predictors and to penalize overfitting.
    roc_conclusion: >
      Test
  xgboost:
    text: "XGBoost"
    title: "XGBoost"
    abbr: "XGB"
    pipe_fn: "pyhbr.analysis.model.make_xgboost"
    description: >
      An alternative tree-based method, similar to random forest, that uses a
      different algorithm to obtained train the model.
    roc_conclusion: >
      Test
  nearest_neighbours:
    text: "K-nearest neighbours"
    title: "K-Nearest Neighbours"
    abbr: "KNN"
    pipe_fn: "pyhbr.analysis.model.make_nearest_neighbours_cv"
    config:
      n_neighbors: "scipy.stats.randint(1, 15)"
      weights: "['uniform', 'distance']"
    description: >
      A model that predicts the value for an input based on the corresponding
      values for known similar inputs (the nearest neighbours).
    roc_conclusion: >
      Test
  support_vector_machine:
    text: "support vector machine"
    title: "Support Vector Machine"
    abbr: "SVM"
    pipe_fn: "pyhbr.analysis.model.make_svm"
    config: {}
    description: >
      A model that attempts to collect the training data into groups based on the
      maximum margin between the groups, and makes predictions based on these groups.
    roc_conclusion: >
      Test
  multilayer_perceptron:
    text: "multi-layer perceptron"
    title: "Multi-layer Perceptron"
    abbr: "MLP"
    pipe_fn: "pyhbr.analysis.model.make_mlp"
    config:
      alpha: "1e-5"
      hidden_layer_sizes: "(20, 2)"
      solver: "'lbfgs'"
      max_iter: "1000"
    description: >
      A type of neural network with a fixed number of layers, which can learn non-linear
      relationships among the input data and the output classes.
    roc_conclusion: >
      Test
  complement_naive_bayes:
    text: "Complement naive bayes"
    title: "Complement Naive Bayes"
    abbr: "CNB"
    pipe_fn: "pyhbr.analysis.model.make_cnb"
    config: {}
    description: >
      An algorithm that calculates the most likely class for each data point using Bayesian
      methods, on the assumption that the features act independently (the "naive" assumption).
      The "complement" version of the algorithm is better suited to unbalanced datasets.
    roc_conclusion: >
      Test


# Outcome names
outcomes:
  bleeding:
    text: "bleeding"
    title: "Bleeding" 
    abbr: "B"
    fatal_group: "bleeding_adaptt"
    non_fatal_group: "bleeding_adaptt"
  ischaemia:
    text: "ischaemia"
    title: "Ischaemia"
    abbr: "I"
    fatal_group: "cv_death_ohm"
    non_fatal_group: "ami_stroke_ohm"


# Names of code groups
code_groups:
  "bleeding_adaptt": "Bleeding (ADAPTT)"
  "bavm": "bAVM"
  "ich": "ICH"
  "portal_hypertension": "Portal Hyp."
  "packed_red_cells_transfusion": "Transfusion"
  "liver_cirrhosis": "Liver cirrhosis"
  "ischaemic_stroke": "Ischaemic stroke"
  "diabetes_type1": "Diabetes (Type 1)"
  "mi_stemi_schnier": "MI (STEMI)"
  "bleeding_al_ani": "Bleeding (al Ani)"
  "diabetes": "Diabetes (Any)"
  "diabetes_type2": "Diabetes (Type 2)"
  "mi_schnier": "MI (Any)"
  "ihd_bezin": "IHD"
  #"hussain_ami_stroke": "AMI/stroke"
  "ami_stroke_ohm": "AMI/Stroke"
  "ckd_before": "CKD"
  "cancer": "Cancer (Any)"
  "all_pci_pathak": "PCI"
  "bleeding_cadth": "Bleeding (Cadth)"
  "acs_bezin": "ACS"
  "mi_nstemi_schnier": "MI (NSTEMI)"

