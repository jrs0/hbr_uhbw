title: "Estimating bleeding and ischaemia risk following acute coronary syndrome"
subtitle: "A machine learning modelling study using routine electronic health data"
author: "John Scott"

### GENERAL ###

# Path to load data from and save data to.
save_dir: "../save_data"

# Base name for this analysis. The string "_data" is
# appended to this name to create the file to load. This
# base name is also used as the prefix for saved files.
analysis_name: "icb_hic"

### DATASET GENERATION ###

# Raw date ranges (yyyy-mm-dd) to restrict the
# SQL data fetch. HIC data only goes back to
# Nov 2019, so set window a bit before that to
# catch everything. End date can be set in the
# future to catch most recent data. Date ranges
# will be automatically restricted to allow a
# window on each side of the index events for
# patient history and outcomes.
start_date: "2019-1-1"
end_date: "2025-1-1"

# Set the name of the ICD-10 and OPCS-4 codes files
# that will be used to define features and outcome
# code groups. The file will be loaded from the
# pyhbr package.
icd10_codes_file: "icd10_arc_hbr.yaml"
opcs4_codes_file: "opcs4_arc_hbr.yaml"

# Set the ACS and PCI code group names that will define
# index events. Events will be considered an ACS event
# if an ACS ICD-10 code is found in the primary position
# of the first episode of the spell. Secondary diagnoses
# are ignored to remove the possibility of identifying
# historical events. PCI index events allow a PCI code
# in any primary or secondary position of the first
# episode of the spell, on the assumption that a coded
# secondary procedure in an episode implies that the
# procedure was performed in that episode.
acs_index_code_group: "acs_bezin"
pci_index_code_group: "all_pci_pathak"

### MODELLING ###


# Test/train split
test_proportion: 0.25

# All randomness is derived from this seed.
seed: 0

# Using a larger number of bootstrap resamples will make
# the stability analysis better, but will take longer to fit.
num_bootstraps: 10

# Choose the number of bins for the calibration calculation.
# Using more bins will resolve the risk estimates more
# precisely, but will reduce the sample size in each bin for
# estimating the prevalence.
num_bins: 5

# References
bib_file: "../risk_management_file/ref.bib"
citation_style: "citation_style.csl"

# Jinja2 templates
templates_folder: "templates"
report_template: "report.qmd"

# Build output locations. The report will be
# placed in a folder called {analysis_name}_report
build_directory: "build"

# List of GP opt-out codes to exclude from the SWD
# data fetch.
gp_opt_outs:
  - "L81087" # Beechwood Medical Practice
  - "L81632" # Emersons Green Medical Centre
  - "L81046" # Leap Valley Medical Centre, also covers Abbotswood Surgery
  - "L81120" # Birchwood Medical Practice
  - "L81055" # Orchard Medical Centre

# Models to include
models:
  random_forest:
    text: "random forest"
    title: "Random Forest"
    abbr: "RF"
    pipe_fn: "pyhbr.analysis.model.make_random_forest_cv"
    config:
      n_estimates: "scipy.stats.randint(50, 500)"
      max_depth: "scipy.stats.randint(1, 20)"
    description: >
      The risk estimate is an average over decision trees, each of which
      estimates the risk by recursively dividing the input predictors into
      ranges and estimating the prevalence within each range of inputs. 100
      trees are used, and each tree is capped at a depth of 10 to avoid overfitting.
    roc_conclusion: >
      Test
  ada_boost:
    text: "AdaBoost"
    title: "AdaBoost"
    abbr: "AB"
    pipe_fn: "pyhbr.analysis.model.make_abc"
    config:
      "n_estimators": "50"
      "learning_rate": "1.0"
      "algorithm": "'SAMME.R'"
    description: >
      An ensemble method fitting simple trees to the data, where later iterations of the
      fit focus on more difficult cases.
    roc_conclusion: >
      Test
  logistic_regression:
    text: "logistic regression"
    title: "Logistic Regression"
    abbr: "LR"
    pipe_fn: "pyhbr.analysis.model.make_logistic_regression"
    description: >
       The risk is obtained by fitting the logistic regression model to the events,
       using the input features are variables. A penalty term is added to handle
       collinearity of the predictors and to penalize overfitting.
    roc_conclusion: >
      Test
  xgboost:
    text: "XGBoost"
    title: "XGBoost"
    abbr: "XGB"
    pipe_fn: "pyhbr.analysis.model.make_xgboost"
    description: >
      An alternative tree-based method, similar to random forest, that uses a
      different algorithm to obtained train the model.
    roc_conclusion: >
      Test
  nearest_neighbours:
    text: "K-nearest neighbours"
    title: "K-Nearest Neighbours"
    abbr: "KNN"
    pipe_fn: "pyhbr.analysis.model.make_nearest_neighbours_cv"
    config:
      n_neighbors: "scipy.stats.randint(1, 15)"
      weights: "['uniform', 'distance']"
    description: >
      A model that predicts the value for an input based on the corresponding
      values for known similar inputs (the nearest neighbours).
    roc_conclusion: >
      Test
  support_vector_machine:
    text: "support vector machine"
    title: "Support Vector Machine"
    abbr: "SVM"
    pipe_fn: "pyhbr.analysis.model.make_svm"
    config: {}
    description: >
      A model that attempts to collect the training data into groups based on the
      maximum margin between the groups, and makes predictions based on these groups.
    roc_conclusion: >
      Test
  multilayer_perceptron:
    text: "multi-layer perceptron"
    title: "Multi-layer Perceptron"
    abbr: "MLP"
    pipe_fn: "pyhbr.analysis.model.make_mlp"
    config:
      alpha: "1e-5"
      hidden_layer_sizes: "(20, 2)"
      solver: "'lbfgs'"
      max_iter: "1000"
    description: >
      A type of neural network with a fixed number of layers, which can learn non-linear
      relationships among the input data and the output classes.
    roc_conclusion: >
      Test
  complement_naive_bayes:
    text: "Complement naive bayes"
    title: "Complement Naive Bayes"
    abbr: "CNB"
    pipe_fn: "pyhbr.analysis.model.make_cnb"
    config: {}
    description: >
      An algorithm that calculates the most likely class for each data point using Bayesian
      methods, on the assumption that the features act independently (the "naive" assumption).
      The "complement" version of the algorithm is better suited to unbalanced datasets.
    roc_conclusion: >
      Test


# Outcome names
outcomes:
  bleeding:
    text: "bleeding"
    title: "Bleeding" 
    abbr: "B"
    fatal_group: "bleeding_adaptt"
    non_fatal_group: "bleeding_adaptt"
  ischaemia:
    text: "ischaemia"
    title: "Ischaemia"
    abbr: "I"
    fatal_group: "cv_death_ohm"
    non_fatal_group: "ami_stroke_ohm"


# Names of code groups
code_groups:
  "bleeding_adaptt": "Bleeding (ADAPTT)"
  "bavm": "bAVM"
  "ich": "ICH"
  "portal_hypertension": "Portal Hyp."
  "packed_red_cells_transfusion": "Transfusion"
  "liver_cirrhosis": "Liver cirrhosis"
  "ischaemic_stroke": "Ischaemic stroke"
  "diabetes_type1": "Diabetes (Type 1)"
  "mi_stemi_schnier": "MI (STEMI)"
  "bleeding_al_ani": "Bleeding (al Ani)"
  "diabetes": "Diabetes (Any)"
  "diabetes_type2": "Diabetes (Type 2)"
  "mi_schnier": "MI (Any)"
  "ihd_bezin": "IHD"
  #"hussain_ami_stroke": "AMI/stroke"
  "ami_stroke_ohm": "AMI/Stroke"
  "ckd_before": "CKD"
  "cancer": "Cancer (Any)"
  "all_pci_pathak": "PCI"
  "bleeding_cadth": "Bleeding (Cadth)"
  "acs_bezin": "ACS"
  "mi_nstemi_schnier": "MI (NSTEMI)"

