# Introduction

Patients with acute coronary syndromes who undergo an intervention that requires the prescription of blood thinning medication are subject to an assessment of relative bleeding/ischaemia risk by the clinician, in order to make an appropriate choice of therapy [@capodanno2023dual].

Blood thinning therapies, such as dual antiplatelet therapy (DAPT) or triple therapy, are prescribed to patients having a stent implant for three months to a year. While the risk of further acute ischaemia, such as stent thrombosis, is reduced in this timeframe, the risk of bleeding is increased. However, the cardiologist may not be alerted to patients with severe bleeding complications, and other health care professionals may be reticent to adjust DAPT medication [@pufulete2019comprehensive].

Ideally, patients at high bleeding risk could be flagged at the point of prescription, so that cardiologists could account for the increased risk of bleeding, and potentially prescribe a less-potent therapy option.

Assessing bleeding risk is complex because it depends on many factors which are not always directly available to the clinician. As a result, consideration of bleeding risk when prescribing blood thinning therapy would likely increase if a tool could automatically assign each patient a reliable bleeding risk on presentation, without requiring prompting by the clinician.

Bleeding risk alone is not useful; it must be considered along with ischaemia risk, because the overarching goal is to simultaneously optimise both bleeding and ischaemic outcomes [@urban2021assessing].

Bleeding risk is a time-varying quantity; it depends on many factors such as comorbidities, and dynamic physiological factors. An ideal tool would track these factors over the course of therapy, and alert the clinician to changes in risk level.

The basis for any such tool is a robust estimate of bleeding and ischaemia risk. Several models for both outcomes exist, including consensus-based scores and statistical models. Often these are developed with highly cultivated datasets, specifically targetting known risk factors. We investigate models based on machine learning approaches using administrative datasets, motivated by the desire to utilise features from large regional databases of primary and secondary care patient data.

We apply a methodology based on the stability and calibration of the models to assess how well they estimate the one-year probability of an adverse outcome (either bleeding or further ischaemia).

{{ summary_table }}
: Summary of model stability and accuracy metrics. Bleeding models are named -B and ischaemia models -I. The spread of metrics derived using bootstrapping are expressed using the 2.5% and 97.5% quantiles, using the notation Q [x,y]. Uncertainty of the risk estimate is assessed using a 95% confidence interval written CI [x,y]. The instability column (reflecting the consistency of the estimated risk) and the uncertainty column (an estimate of risk accuracy) are both absolute errors in percentage points, meaning they are added directly to the estimated risk to obtain the expected variability. A full discussion of the interpretation of each column is contained in Section III below. {{ '{#' }}tbl-summary}

# Background

Models or scores for assessing the risk of bleeding or ischaemia can have two kinds of outputs: discrete categorisation of patients as "high risk" or "low risk"; or estimation of the probability that an outcome will occur. 

A simple ascertainment of bleeding risk is available by using the ARC high bleeding risk (HBR) criteria [@urban2019defining]. The score classifies a patient as either high or low risk depending on a set of factors including comorbidities and physiological measurements.

An alternative approach is the bleeding and ischaemia risk trade-off model [@urban2021assessing] which develops a pair of Cox proportional hazards models to estimate the probability of bleeding and ischaemic events. This model is intended for application only to patients that are qualify as ARC HBR.

Another score is DAPT [@yeh2016development], which ranks patients on a scale -2 to 10 according to whether they have more bleeding or more ischaemia risk. However, this model is intended for deciding continuation of therapy at 12 months, rather than initial prescription.

## Other Risk Prediction Tools

Bleeding and ischaemia in patients with ACS is a well studied problem, and a large number of risk scores exist, some of which are available as web application or mobile phone applications [@chan2020risk]. One such risk score is the DAPT-score, which is designed to assess bleeding and ischaemia risk for the purpose of deciding duration of DAPT therapy [@yeh2016development]. The DAPT bleeding endpoint definition is moderate or severe GUSTO bleeding, meaning intracerebral haemorrhage, or bleeding requiring blood transfusion or resulting in substantial haemodynamic compromise requiring treatment [@gusto1993international]. The ischaemic endpoint was defined as MI or an ARC definition of definite/probable ST [@cutlip2007clinical]. Models developed to predict occurance of these events between 12 and 30 months after the index procedure both achieved a ROC AUC of 0.64 in the validation group. The difference in bleeding and ischaemia prediction depending on the assumption of continuation vs. discontinuation of therapy (an input to the model) was used to assess the trade-off between bleeding and ischaemia risk. The DAPT score is available as a web calculator [@dapt2023website].

Another bleeding risk score is PreciseDAPT [@costa2017derivation], which predicts the risk of out-of-hospital TIMI (thrombolysis in myocardial infarction) major/minor bleeding [@mehran2011standardized].The bleeding model achieved ROC AUCs of 0.70 and 0.66 in two external validation cohorts, and is available as a web calculator [@precisedapt2023website], which presents the risk of bleeding and ischaemia depending on DAPT duration (3-6 months vs. 12-24 months).

The ARC-HBR trade-off model is designed specifically for use in patients who statisfy the ARC-HBR criteria [@urban2021assessing]. The ischaemia endpoint definition was a composite of ARC definite or probable ST, and MI occurring >48 hours after the index event. The MI definition differed according to the underlying study data used: 5 studies used the third universal definition [@thygesen2012task], one used the ARC MI definition [@cutlip2007clinical], and one used an ad-hoc definition [@valgimigli2015zotarolimus]. Bleeding is defined as BARC 3 or 5 [@mehran2011standardized]. DAPT therapy is not an input to the model, and it is explicitly stated that no conclusions can be drawn about associations between DAPT duration or type and the bleeding and ischaemia risks presented [@urban2021assessing]. The ARC-HBR risk trade-off calculation is available as a mobile application, which is used first to assess if a patient is at high bleeding risk, and then if so, provides their relative bleeding and ischaemia risk levels [@archbrtradeoff2023website].

Machine learning algorithms have been shown to perform very well in predicting MI diagnosis during index hospitalisation (ROC AUC of 0.96 [@than2019machine]). Similarly, prediction of in-hospital BARC 3-5 bleeding in PCI patients has achieved a ROC AUC of 0.837 [@zhao2023machine]. This provides some evidence that machine learning methods could form the basis for risk prediction tools.

# Methods

We assess multiple different machine learning models, trained on multiple different datasets, for the estimation of one-year probability of bleeding and further ischaemia outcome, using the approach described below.

## Inclusion Criteria

We obtain a cohort of ACS patients from Hospital Episode Statistics (HES), which is a national administrative dataset available across the UK. This dataset defines both the inclusion criteria and the outcomes. Patients are considered for inclusion based on the primary and secondary diagnoses and procedures in HES data. For a hospital spell to be considered an index event:

* The primary diagnosis of the first episode must contain an ACS ICD-10 code; or
* There must be a PCI procedure OPCS-4 code in any primary or secondary position in the first episode of the spell.

For the ACS diagnosis code inclusion, only the primary position is allowed to avoid the possibility of incorrectly including unrelated spells with historical ACS events coded in secondary positions. For PCI, it is assumed that all procedures (both primary and secondary) occur in the episode, so this will not produce false positive inclusions.

A great many choices exist when defining code groups to capture patient groups in administrative databases [@bosco2021major]. Due to the use of clinical coding for financial and administrative purposes, ICD-10 code definitions do not necessarily line up with clinically relevant patient groups, or contain other errors [@gavrielov2014use]. It is therfore important that the accuracy of the code group in identifying patients of interest is quantified and incorporated into the uncertainty of the modelling results. We have selected code groups from the literature that have been validated in administrative databases.

A UK Biobank report identifies a validated group of codes for identification of MI (both STEMI and NSTEMI) based on HES data, with PPV greater than 70% for each group [@biobankdefinitions]. However, the codes contain I25.2 (old myocardial infarction), which would capture patients in index events who do not necessarily have ACS at that time. This issue was addressed in a study validating ACS code groups in a French administrative database [@bezin2015choice]. Of the different code groups they present, the I20.0, I21.* and I24.* was identified as a good compromise between validated ACS and PPV (84%).

Hospital spells were only included if they had a full year of information prior to the index data, for features, and a full year following the index for outcomes. Through this process, {{ num_index_spells }} index events were identified for inclusion between the dates {{ index_start }} and {{ index_end}}. 

The list of bleeding and ischaemia ICD-10 and OPCS-4 codes are included in supplementary information.

## Endpoints

The basis for the index event and outcome definition is the diagnosis (ICD-10) and procedure (OPCS-4) codes in Hospital Episode Statistics (HES). Unlike in the ARC-HBR risk trade-off model [@urban2021assessing], modelling is not restricted to patients who are at high bleeding risk. This is because identifying patients at high bleeding risk itself requires a model, or a valid calculation of the ARC-HBR score, which is not possible because the information is not present in source datasets. As discussed above, a model that applies uniformly across all patients would also be more helpful because there would be less need to ensure that a patient is eligible before applying the model. 

Bleeding and ischaemia endpoints are based on the presence of an ICD-10 diagnosis code in a subsequent spell occurring up to one year after the index spell. The index spell was excluded because multiple episodes in the index spell often encode the same ACS, which would contribute false-positive outcomes. A disadvantage of this approach is that further ischaemia that occurs soon after the index event is excluded in our analysis.

Identifying significant bleeding using ICD-10 codes is a complicated problem due to the heterogeneous nature of bleeding conditions. Ideally, a group of bleeding codes should be selected which aligns with the definitions generally agreed upon by other bleeding risk prediction tools [@yeh2016development; @costa2017derivation; @urban2021assessing]; for example, BARC 3 or 5 bleeding, moderate or severe GUSTO bleeding, or TIMI major/minor bleeding. We selected the code group of a UK study for identifying BARC 2-5 bleeding [@pufulete2019comprehensive].

Identification of ischaemic outcomes was based on a definition of major adverse cardiac event (MACE), a type of composite endpoint typically utilised in clinical trials. Several ischaemia outcome definitions are commonly used when deriving outcomes from administrative databases [@bosco2021major]. From the options reviewed there, we selected the MACE definition of [@ohm2018socioeconomic] but excluded ischaemic stroke in order to make our model results more closely comparable with the ARC HBR bleeding/ischaemia trade-off model [@urban2021assessing].

Here, code groups are used to identify both endpoints (used as outcomes to train the models) and features (inputs to the models), and innacuracies in these two categories have different effects. Inaccuracies in the outcome definition affect the accuracy of the model, because it may be trained to predict the wrong thing, and is not observable by the model validation process. On the other hand, errors in the features simply cause the model to perform less well, which is observable by the model validation process. As a result, it is more important to be sure of the outcome code definitons than the many other code groups used as features.

Confirming the accuracy of the outcome code groups is outside the scope of this report, especially including the nuances that can arise from differences in locality, coding standards, and use of diagnosis positions. The resulting models should be considered a proof-of-principle of the approach, rather than a fully-validated prediction of bleeding and ischaemia risk. In particular, chart review should be performed on code groups intended for models deployed in a clinical setting.

![Distribution of primary/secondary code positions]({{ codes_hist_image }}){{ '{#' }}fig-code-dist}

The distribution of primary/secondary ICD-10 code positions for non-fatal bleeding and ischaemia outcome groups is shown in @fig-code-dist, where the primary position is shown on the x-axis. We chose a cut-off for the purposes of defining outcomes so that 95% of instances of codes in the relevant code group are included in the outcomes. This represents a trade-off between the desire to include as many relevant bleeding and ischaemia events as possible, while not introducing noise into the outcome definition.

For non-fatal bleeding, the primary code and codes up to secondary {{ bleeding_secondary_cutoff }} are included. For non-fatal ischaemia, up to secondary {{ ischaemia_secondary_cutoff }} is included.

Fatal bleeding and ischaemia outcomes, based on a bleeding or ischaemia primary cause of death ICD-10 code in a Civil Registration Mortality database, are included in the outcome count. A summary of the total number of each outcome is shown in @tbl-outcome-prevalence.

{{ outcome_prevalences }}
: Total number of outcomes that occurred within 1 year of the index ACS. The dataset contains {{ num_index_spells }} ACS events between the dates {{ index_start }} and {{ index_end }} {{ '{#' }}tbl-outcome-prevalence}

![Kaplan-Meier curves for bleeding and ischaemia outcomes over one year]({{ outcome_survival_image }}){{ '{#' }}fig-outcome-survival-curves}

@fig-outcome-survival-curves shows Kaplan-Meier curves for the bleeding and ischaemia outcomes over the course of one year, broken down by age. Both outcomes have low prevalence, with adverse bleeding outcomes occurring more infrequently than ischaemia outcomes.

![Kaplan-Meier curves for bleeding outcome by ARC HBR score]({{ arc_survival_image }}){{ '{#' }}fig-arc-survival-curves}

@fig-arc-survival-curves shows Kaplan-Meier curves for bleeding over the course of one year, broken down by ARC HBR score.

## Model Input Features

Three data sources are used to provide features for the models: HES data for historical coding; a regional dataset containing primary care information (the System Wide Dataset, SWD); and a hospital-specific dataset of laboratory data and prescription information, derived from the Hospital Information Collaborate (HIC) data at UHBW.

### HES Features

In addition to defining the index event and outcomes, HES data is also used for features. In the 12 months prior to the index event, the number of occurences of ICD-10 and OPCS-4 codes in "feature" code groups are counted, and these variables are used as predictors in the models. These feature groups are designed to correspond to known risk factors for bleeding and ischaemia outcomes [@urban2021assessing], where suitable information is present in clinical coding. All clinical code positions (primary and all secondaries) are counted. These features are listed in @tbl-features as HES ICD-10 codes.

The 1 months prior to the index event is excluded, and does not contribute to the count of prior clinical codes. This is because clinical coding takes time to manually complete, and models deployed in real time would not have access to this information. 

In addition to historical code features, HES data is used to generate some index features, including the presentation type (STEMI or NSTEMI) (defined by code groups), and patient age and gender at index. Although this is based on coded information which would not be present in real time, it is assumed that the information could be sourced using other means in a deployed model. These features are labelled in @tbl-features as HES index features.

### SWD Features

The system wide dataset (SWD) is a Bristol, North Somerset, and South Gloucestershire-specific repository of primary care patient information, which includes flags for comorbidities and risk factors, and some physiological measurements. Patient attribute information is recorded on a monthly basis. We linked this information to each index spell by taking the most recent patient attribute that occurred before the index spell, and not more than two months before the index spell. The choice of a two month cut-off represents a trade-off between the temporal relevance of the data and missingness.  

Attributes are excluded from any further analysis if more than 75% of the data is missing, or if more than 95% of the column is a single constant value, in order to avoid swamping models with a large number of non-informative information. (This process is in addition to automatic feature selection/removal by the model fitting process itself.)

SWD data also includes primary care prescriptions. We focus on features relevant for bleeding risk (the harder modelling task), and count the number of prescriptions of oral anticoagulants (OAC), defined as either warfarin, apixaban, dabigatran etexilate, edoxaban, and rivaroxaban, and non-steroidal anti-inflamatory drugs (NSAIDs), defined as either ibuprofen, naproxen, diclofenac, celecoxib, mefenamic acid, etoricoxib, or indomethacin. These represent two medicine-related parts of the ARC HBR criteria [@urban2019defining]. The number of prescriptions in the year before the index spell were counted and used as predictors (labelled in @tbl-features as SWD prescriptions).

The SWD primary care physiological measurements were restricted to those which are most available in the data. To maximise the time, we selected HbA1c and blood pressure, as these data go back to 2019 (the start of our common-data period). Blood pressure was split into diastolic and systolic numerical components for use as features. We used the most recent measurement up to 2 months prior to the index event (but not after the index) to be used as a feature. These columns are summarised in @tbl-features as SWD measurements.

### HIC Data

HIC data includes laboratory tests performed at the index, including the full blood count and urea and electrolytes measurements. We included haemoglobin (Hb), platelet count, and estimated glomerular filtration rate (eGFR) are numerical features in the models. These were taken from the first test performed after admission in the index spell. Anaemia (low Hb), thrombocytopenia (low platelet count), and low eGFR (reduced liver function) are all ARC HBR criteria (bleeding risk factors) [@urban2019defining].

Secondary care prescriptions were searched for OAC and NSAID prescriptions present on admission. HIC-derived features are listed in @tbl-features as HIC lab results and HIC prescriptions.

{{ features_table }}
: Features used to train the models. The feature name and description are listed, along with the data source for the feature. Three data sources are used: HES data, containing ICD-10 and OPCS-4 codes, are used for prior clinical code history and patient index attributes; SWD data is used for primary care general patient attributes, primary care prescriptions, and primary care measurements; and HIC data is used for index secondary care lab results and prior prescriptions at index. {{ '{#' }}tbl-features}

## Models

We randomly split the data into a training set ({{ 1 - test_proportion }}) and a testing set ({{ test_proportion }}). The testing set is not used in the model fitting process. We trained multiple models on each training set:

{% for model in models.values() %}
* **{{ model["title"] }} ({{ model["abbr" ]}})**: {{ model["description"] }}
{% endfor %}

## Verification

We apply classification machine learning models in this paper, which we train as if we are predicting whether a bleeding event or ischaemic event will occur. It is essential to understand that this is conceptually different from the way classification models are normally used.

In a typical application of machine learning, the ``ground truth'' of the testing set is immediately verifiable (i.e. it is possible to check its classes are labelled correctly), which is why direct agreement of the model class with the entry in the testing set is suitable as a benchmark for the model's performance.

In our case, both bleeding and ischaemia are relatively rare events, and clinically relevant ``high risk'' is not necessarily a large quantitative increase in absolute risk. For example, for bleeding, ARC define high bleeding risk as >4%, whereas a patient with a 1% bleeding risk would be at low risk.

The testing set, containing actual occurrences of bleeding and ischaemic events, is therefore not the ``ground truth'' of interest, because an individual patient outcome neither confirms nor denies that patient was at high risk.

Instead, the fundamental tools we employ to assess the accuracy of the model risk estimates are the model stability and calibration~[@riley2022stability] described below.

### Stability: Are patient risk estimates consistent?

We require the model to make consistent estimates of patient risk, not subject to large random fluctuations. The resilience of a model's risk estimates to the randomness involved in the input data and the training process is called stability.

A proxy to assess stability is to compare the estimates from the model with the estimates from bootstrap models, which are trained on versions of the training set that are resampled with replacement. These resampled datasets should be similar to the primary training dataset, and should lead to similar models.

The risk estimates from the primary model are compared with the estimates from all the bootstrap models, for each patient. Large discrepancies are considered to indicate the expected variability in risk estimates from the primary model.

### Calibration: Is the estimated risk accurate?

If a table of "true risks" were available, it would be possible to assess the accuracy of the estimated risks using a simple measure such as mean-square-error, similarly to how a linear regression model is checked. Since the true risk is unknown, a method is required to estimate the true risk.

Estimating true risk requires an estimate of prevalence in groups of similar patients. We group patients by the risks that the model produces (thereby making an assumption that the model is reasonable), and estimate the prevalence within each group. These prevalences are compared with the average risk estimated by the model, to assess the model accuracy. If the model risk estimates agree with the prevalences, it is a well calibrated model.

Good agreement between the estimated prevalence and the model risk estimate is evidence that the risk estimates may be accurate.

In order to estimate the prevalence, we collect patients into groups of equal size and similar risk. Estimates of the prevalence within these groups is subject to an uncertainty which is a function of the sample size, which is critical to the conclusions we draw in this paper.

### Summary of Metric Interpretations

We conclude this section with a detailed interpretation of all the columns in Table~\ref{tab:summary}.

* **Instability**: This column shows the expected fluctuations between estimates from models trained on similar data. For example, for a patient with a risk estimate of 1\%, an instability of 0.5\% implies the model could well have output 0.5\% or 1.5\%. It does not relate to the accuracy of the estimate, but instead can be interpreted as showing a measure of the precision. Results are empirically gathered from bootstrapping, and are presented as the median, 2.5\% and 97.5\% quantiles of the observed absolute risk differences.
* $H\to L$: If a high-risk threshold is used to classify patients based on the risk estimate, there is a chance that the instability described above will lead to a patient reclassification, if the risk estimate is near the threshold. The column is the proportion of high risk patients who are more than 50\% likely to be reclassed as low risk by a similar model.

    For bleeding, a threshold of 4\% is used to indicate high risk. For ischaemia models, the threshold used is 20\%.
    
* $L\to H$: Similarly, this column is the proportion of low risk patients who have more than 50\% chance to be reclassified as high risk by a bootstrap model.
* Estimated Risk Uncertainty: This column shows an estimate of the absolute error in the risk estimates, based on the assumption of a normally distributed estimate of prevalence in the calibration plot. For a patient with risk 3\%, an estimated risk uncertainty of 5\% indicates that the patient's true risk could be from 0\% to 8\%. The trustworthiness of this parameter is a strong function of the sample size of the testing set.
* **ROC AUC**: The area under the receiver operating characteristic curve is based on the assumption that the model is predicting deterministic events. For two patients $A$ and $B$ in the testing set where $A$ has an event and $B$ does not, the AUC is the probability that $A$ will receive a higher risk estimate than $B$.

# Model Results

This section describes the model results in detail.

The stability of each model is presented by comparing the predictions of the model-under-test with bootstrap models trained on resamples of the training set. 

The accuracy of models is assessed by looking at the calibration curve, and putting error bars on the estimated prevalence in each calibration bin.

{% raw %}
{{< pagebreak >}}
{% endraw %}