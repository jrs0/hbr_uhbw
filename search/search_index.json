{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the PyHBR Docs","text":"<p>This python package contains tools relating to estimating the risk of bleeding/ischaemia risk in cardiology patients.</p>"},{"location":"arc_hbr/","title":"ARC HBR Calculation","text":"<p>The ARC HBR score is a concensus-based risk score to identify patients at high bleeding risk. It is formed from a weighted sum of 13 criteria, involving patient characteristics, patient history, laboratory test results, prescriptions information, and planned healthcare activity. This page describes how the ARC HBR score is calculated and assessed in PyHBR.</p>"},{"location":"arc_hbr/#steps-to-calculate-the-arc-hbr-score","title":"Steps to Calculate the ARC HBR Score","text":"<p>Using source data that includes diagnosis/procedure codes, laboratory tests, and prescriptions information, the ARC HBR score can be calculated as follows:</p>"},{"location":"arc_hbr/#preprocessing-steps","title":"Preprocessing Steps","text":"<p>Before identifying patients/index events, or calculating the score, some preprocessing is required. This is the section that is likely to require modification if a new data source is used.</p> <ol> <li>For diagnosis and procedure codes, add the group they belong to, and drop codes not in any group. Retain the code, the group, and the code position in long format in a table. The following groups are required:<ul> <li><code>acs</code>: Diagnosis codes corresponding to acute coronary syndromes (includes myocardial infarction/unstable angina). Used to identify index events.</li> <li><code>pci</code>: Percutaneous coronary intervention codes (e.g. stent implantation). Used to identify index events.</li> <li><code>ckd</code>: Chronic kidney disease: includes N18.5, N18.4, N18.3, N18.2, N18.1. Used as a fall-back in case eGFR is missing.</li> <li><code>anaemia</code>: Used as a fall-back in case Hb measurement is missing.</li> <li><code>bleeding</code>: Used to identify prior bleeding.</li> <li><code>cbd</code>: Chronic bleeding diatheses.</li> <li><code>cph</code>: Cirrhosis with portal hypertension.</li> <li><code>cancer_diag</code>: Used to identify cancer diagnoses.</li> <li><code>cancer_proc</code>: Used to identify cancer therapy.</li> <li><code>bavm</code>: Brain arteriovenous malformation diagnoses.</li> <li><code>istroke</code>: Ischaemic stroke.</li> <li><code>ich</code>: Intracranial haemorrhage.</li> <li><code>csurgery</code>: Cardiac surgery. Used to exclude cardiac surgery for one criterion.</li> <li><code>surgery</code>: All surgery. Used as a proxy for \"major surgery\" criteria</li> </ul> </li> <li> <p>For laboratory results, narrow to the subset of results shown below. Convert all tests to the standard unit used in the ARC definition, and drop the unit from the table. Keep the date/time at which the laboratory sample was collected, and the patient ID (in this data, associated episode is not linked, and must be inferred from the date).</p> <ul> <li><code>egfr</code>: Used to assess kidney function (unit: mL/min)</li> <li><code>hb</code>: Haemoglobin, used to assess anaemia (unit: g/dL)</li> <li><code>platelets</code>: Platelet count, used to assess thrombocytopenia (unit: <code>x10^9/L</code>)</li> </ul> </li> <li> <p>For prescriptions, narrow to the set of medicines shown below. Keep the medicine name, flag for present-on-admission, patient ID, and prescription order date (used to infer link to episode, as above).</p> <ul> <li><code>oac</code>: any of warfarin, apixaban, edoxaban, dabigatran, rivaroxaban</li> <li><code>nsaid</code>: any of ibuprofen, naproxen, diclofenac, celecoxib, mefenamic acid, etoricoxib, indomethacin (high-does aspirin excluded for now).</li> </ul> </li> <li> <p>For demographics, retain age and gender. This calculation may be postoned until after index events are calculated (for example, if the demographics table contains year of birth instead of age).</p> </li> </ol> <p>NOTE: The <code>episodes</code>, <code>prescriptions</code>, and <code>lab_results</code> tables have <code>episode_id</code> as Pandas index. The <code>demographics</code> table uses <code>patient_id</code> as index. The <code>episodes</code> table contains <code>patient_id</code> as a column for linking to <code>demographics</code>.</p>"},{"location":"arc_hbr/#link-laboratory-results-and-prescriptions-to-episodes","title":"Link Laboratory Results and Prescriptions to Episodes","text":"<p>In the HIC data, laboratory results and prescriptions do not contain an episode_id; instead, they contain a date/time (either a sample date for laboratory tests or an order date for prescriptions).</p> <p>To link each test/prescription to an episode, use the episode start and end times. If the sample date/order date falls within the episode start and end time, then it should be associated with that episode.</p> <p>A complication with this process is that episodes sometimes overlap (i.e. the start time of the next is before the end time of the previous one). This will be solved by associating a test/prescription with the earliest episode containing the time.</p>"},{"location":"arc_hbr/#identify-index-events","title":"Identify Index Events","text":"<p>Inclusion criteria for calculation of the ARC HBR score is having a hospital visit (spell) where the first episode of the spell contains an ACS diagnosis in the primary position, or a PCI procedure in any position.</p> <p>The table is indexed by the episode ID, and contains flag columns <code>acs_index</code> for <code>pci_index</code> for which inclusion condition is satisfied.</p> <p>NOTE: The <code>index_event</code> table is indexed by <code>episode_id</code>, and also contains the <code>patient_id</code> as a column.</p>"},{"location":"arc_hbr/#calculating-the-score","title":"Calculating the Score","text":"<p>The score is calculated differently for each different class of critera:</p>"},{"location":"design/","title":"PyHBR Design","text":"<p>This section describes the design of PyHBR and how the code is structured.</p>"},{"location":"design/#data-sources-and-analysis","title":"Data Sources and Analysis","text":"<p>The package contains routines for performing data analysis and fitting models. The source data for this analysis are tables stored in Microsoft SQL Server.</p> <p>In order to make the models reusable, the analysis/model code expects the tables in a particular format, which is documented for each analysis/model script. The code for analysis is in the <code>pyhbr.analysis</code> module.</p> <p>The database query and data fetch is performed by separate code, which is expected to be modified to port this package to a new data source. These data collection scripts are stored in the <code>pyhbr.data_source</code> module.</p> <p>A middle preprocessing layer <code>pyhbr.middle</code> is used to converted raw data from the data sources into the form expected by analysis. This helps keep the raw data sources clean (there is no need for extensive transformations in the SQL layer).</p>"},{"location":"design/#sql-queries","title":"SQL Queries","text":"<p>The approach taken to prepare SQL statements is to use SQLAlchemy to prepare a query, and then pass it to Pandas read_sql for execution. The advantage of using SQLAlchemy statements instead of raw strings in <code>read_sql</code> is the ability to construct statements using a declarative syntax (including binding parameters), and increased opportunity for error checking (which may be useful for porting the scripts to new databases).</p> <p>An example of how SQL statements are built is shown below:</p> <pre><code>from sqlalchemy import select\nfrom pyhbr.common import make_engine, CheckedTable\nimport pandas as pd\nimport datetime as dt\n\n# All interactions with the database (including building queries,\n# which queries the server to check columns) needs an sqlalchemy \n# engine\nengine = make_engine()\n\n# The CheckedTable is a simple wrapper around sqlalchemy.Table,\n# for the purpose of checking for missing columns. It replaces\n# sqlalchemy syntax table.c.column_name with table.col(\"column_name\")\ntable = CheckedTable(\"cv1_episodes\", engine)\n\n# The SQL statement is built up using the sqlalchemy select function.\n# The declarative syntax reduces the chance of errors typing a raw\n# SQL string. This line will throw an error if any of the columns are\n# wrong.\nstmt = select(\n    table.col(\"subject\").label(\"patient_id\"),\n    table.col(\"episode_identifier\").label(\"episode_id\"),\n    table.col(\"spell_identifier\").label(\"spell_id\"),\n    table.col(\"episode_start_time\").label(\"episode_start\"),\n    table.col(\"episode_end_time\").label(\"episode_end\"),\n).where(\n    table.col(\"episode_start_time\") &gt;= dt.date(2000, 1, 1),\n    table.col(\"episode_end_time\") &lt;= dt.date(2005, 1, 1)\n)\n\n# The SQL query can be printed for inspection if required,\n# or for using directly in a SQL script\nprint(stmt)\n\n# Otherwise, execute the query using pandas to get a dataframe\ndf = pd.read_sql(stmt, engine)\n</code></pre> <p>See the <code>pyhbr.data_source</code> module for more examples of functions that return the <code>stmt</code> variable for different tables.</p> <p>The following are some tips for building statements using the <code>CheckedTable</code> object:</p> <ul> <li><code>CheckedTable</code> contains the SQLAlchemy <code>Table</code> as the <code>table</code> member. This means you can use <code>select(table.table)</code> to initially fetch all the columns (useful for seeing what the table contains)</li> <li>If you need to rename a column (using <code>AS</code> in SQL), use <code>label</code>; e.g. <code>select(table.col(\"old_name\").label(\"new_name\"))</code>.</li> <li>Sometimes (particularly with ID columns which are typed incorrectly), it is useful to be able to cast to a different type. You can do this using <code>select(table.col(\"col_to_cast\").cast(String))</code>. The list of generic types is provided here; import the one you need using a line like <code>from sqlalchemy import String</code>.</li> </ul>"},{"location":"design/#middle-layer","title":"Middle Layer","text":"<p>To account for differences in data sources and the analysis, the module <code>pyhbr.middle</code> contains modules like <code>from_hic</code> which contain function that return transformed versions of the data sources more suitable for analysis.</p> <p>The outputs from this layer are documented here so that it is possible to take a new data source and write a new module in <code>pyhbr.middle</code> which exposes the new data source for analysis. These tables are grouped together into classes and (where the table name is used as the attribute name) used as the argument to analysis functions. Analysis functions may not use all the columns of each table, but when a column is present it should have the name and meaning given below.</p> <p>All tables are Pandas DataFrames.</p>"},{"location":"design/#episodes","title":"Episodes","text":"<p>Most analysis is performed in terms of episodes, which correspond to individual consultant interactions within a hospital visit (called a spell). Episode information is stored in a table called <code>episodes</code>, which has the following columns:</p> <ul> <li><code>episode_id</code> (<code>str</code>, Pandas index): uniquely identifies the episode.</li> <li><code>patient_id</code> (<code>str</code>): the unique identifier for the patient.</li> <li><code>spell_id</code> (<code>str</code>): identifies the spell containing the episode.</li> <li><code>episode_start</code> (<code>datetime.date</code>): the episode start time.</li> </ul> <p>Note</p> <p>Consider filtering the episodes table based on a date range of interest when it is fetched from the data source. This will speed up subsequent processing.</p>"},{"location":"design/#codes","title":"Codes","text":"<p>Episodes contain clinical code data, which lists the diagnoses made and the procedures performed in an episode. This is stored in a table called <code>codes</code>, with the following columns:</p> <ul> <li><code>episode_id</code> (<code>str</code>): which episode contains this clinical code.</li> <li><code>code</code> (<code>str</code>): the clinical code, all lowercase with no whitespace or dot, e.g. <code>i212</code></li> <li><code>position</code> (<code>int</code>): the position of the code in the episode. 1 means the primary position (e.g. for a primary diagnosis), and &gt;1 means a secondary code. Often episodes contain 5-10 clinical codes, and the maximum number depends on the data source.</li> <li><code>type</code> (<code>category</code>): either \"diagnosis\" (for ICD-10 diagnosis codes) or \"procedure\" (for OPCS-4 codes)</li> <li><code>group</code> (<code>str</code>): which group contains this clinical code.</li> </ul> <p>The Pandas index is a unique integer (note that <code>episode_id</code> is not unique, since a single episode can contain many codes).</p> <p>Note</p> <p>This table only contains codes that are in a code group (i.e. the function making <code>codes</code> should filter out codes not in any group; the <code>group</code> column is not NaN). If all codes are required, make a code group \"all\" which contains every code. Note that codes occupy multiple rows in the <code>codes</code> table if they are in more than one group (take care when counting rows). In these cases, a duplicate code is identified by having the same <code>code</code>, <code>position</code> and <code>type</code> values, but a different group.</p>"},{"location":"design/#demographics","title":"Demographics","text":"<p>Demographic information is stored in a table called <code>demographics</code>, which has the following columns:</p> <ul> <li><code>patient_id</code> (<code>str</code>, Pandas index): the unique patient identifier</li> <li><code>gender</code> (<code>category</code>): One of \"male\", \"female\", or \"unknown\". </li> </ul>"},{"location":"design/#laboratory-tests","title":"Laboratory Tests","text":"<p>Write me please</p>"},{"location":"design/#prescriptions","title":"Prescriptions","text":"<p>Write me please</p>"},{"location":"design/#datamodelanalysis-save-points","title":"Data/Model/Analysis Save Points","text":"<p>To support saving intermediate results of calculations, <code>pyhbr.common</code> includes two functions <code>save_item</code> and <code>load_item</code>, which save a Python object to a directory (by default <code>save_data/</code> in your working directory).</p> <p>The scripts in hbr_uhbw use these functions to create these checkpoints:</p> <ul> <li>Data: After fetching data from databases or data sources and converting it into the raw format suitable for modelling or analysis. These files have <code>_data</code> in the file name. This data is then loaded again for modelling or analysis</li> <li>Model: After training models using the data stored in the <code>_data</code> files. These files have <code>_model</code> in the file name. This data is loaded for analysis.</li> <li>Analysis: After performing analysis using the <code>_data</code> or <code>_model</code> files. These files have <code>_analysis</code> in the file name. This data can be loaded and used to generate reports/outputs.</li> </ul> <p>Splitting up the scripts in this way makes them easier to develop, because each of the three parts above can take quite long to run.</p> <p>Multiple objects can be saved under one file by including them in a dictionary. It is up to the script to determine the format of the items being saved and loaded.</p> <p>Warning</p> <p>By default, <code>save_item</code> puts the saved files into a directory called <code>save_data/</code> relative to your current working directory. Ensure that this is added to the .gitignore if the files contain sensitive data, to avoid committing them to your repository.</p>"},{"location":"design/#saving-data","title":"Saving Data","text":"<p>In addition to saving the item, <code>pyhbr.common.save_item</code> also includes in the file name:</p> <ul> <li>The commit hash of the git repository at the time <code>save_item</code> is called. This is intended to make it easier to reproduce the state of the repository that generated the file. By default, <code>save_item</code> requires you to commit any changes before saving a file. (The cleanest/most reproducible thing to do is commit changes, and then run a script non-interactively from the top.) If you are not using a git repository, then \"nogit\" is used in place of the commit hash.</li> <li>The timestamp when <code>save_item</code> was called, which is more granular than the commit hash (or useful in case you do not have a git repository).</li> </ul> <p>Note</p> <p>You can save multiple items with the same name, because the file names will use different timestamps. By default, <code>load_item</code> will load the most recently saved file with a given name.</p> <p>The <code>save_item</code> function is shown below. The simplest way to call it is <code>save_item(df, \"my_df\")</code>, which will save the DataFrame <code>df</code> to the directory <code>save_data/</code> using the name \"my_df\".</p> Use this function to save a Python object (e.g. a DataFrame) <code></code> <code>save_item(item, name, save_dir='save_data/', enforce_clean_branch=True)</code> <p>Save an item to a pickle file</p> <p>Saves a python object (e.g. a pandas DataFrame) dataframe in the save_dir folder, using a filename that includes the current timestamp and the current commit hash. Use load_item to retrieve the file.</p> <p>Important</p> <p>Ensure that <code>save_data/</code> (or your chosen <code>save_dir</code>) is added to the .gitignore of your repository to ensure sensitive data is not committed.</p> <p>By storing the commit hash and timestamp, it is possible to identify when items were created and what code created them. To make most effective use of the commit hash, ensure that you commit, and do not make any further code edits, before running a script that calls save_item (otherwise the commit hash will not quite reflect the state of the running code).</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Any</code> <p>The python object to saave (e.g. pandas DataFrame)</p> required <code>name</code> <code>str</code> <p>The name of the item. The filename will be created by adding a suffix for the current commit and the timestamp to show when the data was saved (format: <code>name_commit_timestamp.pkl</code>)</p> required <code>save_dir</code> <code>str</code> <p>Where to save the data, relative to the current working directory. The directory will be created if it does not exist.</p> <code>'save_data/'</code> <code>enforce_clean_branch</code> <p>If True, the function will raise an exception if an attempt is made to save an item when the repository has uncommitted changes.</p> <code>True</code> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def save_item(\n    item: Any, name: str, save_dir: str = \"save_data/\", enforce_clean_branch=True\n) -&gt; None:\n    \"\"\"Save an item to a pickle file\n\n    Saves a python object (e.g. a pandas DataFrame) dataframe in the save_dir\n    folder, using a filename that includes the current timestamp and the current\n    commit hash. Use load_item to retrieve the file.\n\n    !!! important\n        Ensure that `save_data/` (or your chosen `save_dir`) is added to the\n        .gitignore of your repository to ensure sensitive data is not committed.\n\n    By storing the commit hash and timestamp, it is possible to identify when items\n    were created and what code created them. To make most effective use of the\n    commit hash, ensure that you commit, and do not make any further code edits,\n    before running a script that calls save_item (otherwise the commit hash will\n    not quite reflect the state of the running code).\n\n    Args:\n        item: The python object to saave (e.g. pandas DataFrame)\n        name: The name of the item. The filename will be created by adding\n            a suffix for the current commit and the timestamp to show when the\n            data was saved (format: `name_commit_timestamp.pkl`)\n        save_dir: Where to save the data, relative to the current working directory.\n            The directory will be created if it does not exist.\n        enforce_clean_branch: If True, the function will raise an exception if an attempt\n            is made to save an item when the repository has uncommitted changes.\n    \"\"\"\n\n    if enforce_clean_branch and requires_commit():\n        raise RuntimeError(\n            \"Aborting save_item() because branch is not clean. Commit your changes before saving item to increase the chance of reproducing the item based on the filename commit hash.\"\n        )\n\n    if not os.path.isdir(save_dir):\n        print(f\"Creating missing folder '{save_dir}' for storing item\")\n        os.mkdir(save_dir)\n\n    # Make the file suffix out of the current git\n    # commit hash and the current time\n    filename = f\"{name}_{current_commit()}_{current_timestamp()}.pkl\"\n    path = os.path.join(save_dir, filename)\n\n    with open(path, \"wb\") as file:\n        pickle.dump(item, file)\n</code></pre>"},{"location":"design/#loading-data","title":"Loading Data","text":"<p>To load a previously saved item, using <code>pyhbr.common.load_item</code>. It can be called most simply using <code>load_item(\"my_df\")</code>, assuming you previously saved an object in the default directory (<code>save_data</code>) with the name \"my_df\". By default, the most recent item is loaded, but using <code>load_item(\"my_df\", True)</code> will let you pick which file you want to load.</p> <p>The function <code>load_item</code> is shown below:</p> Use this function to load a previously saved Python object <code></code> <code>load_item(name, interactive=False, save_dir='save_data')</code> <p>Load a previously saved item from file</p> <p>Use this function to load a file that was previously saved using save_item(). By default, the latest version of the item will be returned (the one with the most recent timestamp).</p> <p>None is returned if an interactive load is cancelled by the user.</p> <p>To load an item that is an object from a library (e.g. a pandas DataFrame), the library must be installed (otherwise you will get a ModuleNotFound exception). However, you do not have to import the library before calling this function.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the item to load</p> required <code>interactive</code> <code>bool</code> <p>If True, let the user pick which item version to load interactively. If False, non-interactively load the most recent item (i.e. with the most recent timestamp). The commit hash is not considered when loading the item.</p> <code>False</code> <code>save_fir</code> <p>Which folder to load the item from.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The python object loaded from file, or None for an interactive load that is cancelled by the user.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def load_item(name: str, interactive: bool = False, save_dir: str = \"save_data\") -&gt; Any:\n    \"\"\"Load a previously saved item from file\n\n    Use this function to load a file that was previously saved using\n    save_item(). By default, the latest version of the item will be returned\n    (the one with the most recent timestamp).\n\n    None is returned if an interactive load is cancelled by the user.\n\n    To load an item that is an object from a library (e.g. a pandas DataFrame),\n    the library must be installed (otherwise you will get a ModuleNotFound\n    exception). However, you do not have to import the library before calling this\n    function.\n\n    Args:\n        name: The name of the item to load\n        interactive: If True, let the user pick which item version to load interactively.\n            If False, non-interactively load the most recent item (i.e. with the most\n            recent timestamp). The commit hash is not considered when loading the item.\n        save_fir: Which folder to load the item from.\n\n    Returns:\n        The python object loaded from file, or None for an interactive load that\n            is cancelled by the user.\n\n    \"\"\"\n    if interactive:\n        item_path = pick_saved_file_interactive(name, save_dir)\n    else:\n        item_path = pick_most_recent_saved_file(name, save_dir)\n\n    if item_path is None:\n        print(\"Aborted (interactive) load item\")\n        return\n\n    print(f\"Loading {item_path}\")\n\n    # Load a generic pickle. Note that if this is a pandas dataframe,\n    # pandas must be installed (otherwise you will get module not found).\n    # The same goes for a pickle storing an object from any other library.\n    with open(item_path, \"rb\") as file:\n        return pickle.load(file)\n</code></pre>"},{"location":"design/#clinical-codes","title":"Clinical Codes","text":"<p>PyHBR has functions for creating and using lists of ICD-10 and OPCS-4 codes. A prototype version of the graphical program to create the code lists was written in Tauri here. However, it is simpler and more portable to have the codes editor bundled in this python package, and written in python.</p> <p>Users should be able to do the following things with the codes editor GUI:</p> <ul> <li>Open the GUI program, and select a blank ICD-10 or OPCS-4 codes tree to begin creating code groups.</li> <li>Create new code groups starting from the blank template.</li> <li>Search for strings within the ICD-10/OPCS-4 descriptions to make creation of groups easier.</li> <li>Save the resulting codes file to a working directory.</li> <li>Open and edit a previously saved codes file from a working directory.</li> </ul> <p>Once the groups have been defined, the user should be able to perform the following actions with the code groups files:</p> <ul> <li>Import codes files from the package (i.e. predefined code groups).</li> <li>Import codes files (containing custom groups) from a working directory.</li> <li>Extract the code groups, and show which codes are in which groups.</li> <li>Use the code groups in analysis (i.e. get a Pandas DataFrame showing which codes are in which groups)</li> </ul> <p>Multiple code groups are stored in a single file, which means that only two codes files are necessary: <code>icd10-yaml</code> and <code>opcs4.yaml</code>. There is no limit to the number of code groups.</p> <p>Previously implemented functionality to check whether a clinical code is valid will not be implemented here, because sufficiently performant code cannot be written in pure python (and this package is intended to contain only pure Python to maximise portability).</p> <p>Instead, all codes are converted to a standard \"normal form\" where upper-case letters are replaced with lower-case, and dots/whitespace is removed. Codes can then be compared, and most codes will match under this condition. (Codes that will not match include those with suffixes, such as dagger or asterix, or codes that contain further qualifying suffixes that are not present in the codes tree.).</p>"},{"location":"design/#counting-codes","title":"Counting Codes","text":"<p>Diagnosis and procedure codes can be grouped together and used as features for building models. One way to do this is to count the codes in a particular time window (for example, one year before an index event), and use that as a predictor for subsequent outcomes.</p> <p>This sections describes how raw episode data is converted into this counted form in PyHBR.</p>"},{"location":"design/#getting-clinical-code-data","title":"Getting Clinical Code Data","text":"<p>Hospital episodes contain multiple diagnosis and procedure codes. The starting point for counting codes is using the <code>pyhbr.middle.*.get_clinical_codes</code> function, which returns a data frame with the following columns:</p> <ul> <li><code>episode_id</code>: Which episode the code was in</li> <li><code>code</code>: The name of the clinical code in normal form (lowercase, no whitespace/dots), e.g. \"n183\"</li> <li><code>group</code>: The group containing the code. The table only contains codes that are defined in a code group, which is based on the codes files from the previous section</li> <li><code>position</code>: The priority of the clinical code, where 1 means the primary diagnosis/procedure, and &gt; 1 means a secondary code.</li> <li><code>type</code>: Either \"diagnosis\" or \"procedure\" depending on the type of code.</li> </ul> <p>This table does not use <code>episode_id</code> as the index because a single episode ID often has many rows.</p> <p>An example of this function in <code>pyhbr.middle.from_hic</code> is:</p> Example function which fetches clinical codes <code></code> <code>get_clinical_codes(engine, diagnoses_file, procedures_file)</code> <p>Main diagnoses/procedures fetch for the HIC data</p> <p>This function wraps the diagnoses/procedures queries and a filtering operation to reduce the tables to only those rows which contain a code in a group. One table is returned which contains both the diagnoses and procedures in long format, along with the associated episode ID and the primary/secondary position of the code in the episode.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>The connection to the database</p> required <code>diagnoses_file</code> <code>str</code> <p>The diagnoses codes file name (loaded from the package)</p> required <code>procedures_file</code> <code>str</code> <p>The procedures codes file name (loaded from the package)</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A table containing diagnoses/procedures, normalised codes, code groups, diagnosis positions, and associated episode ID.</p> Source code in <code>src\\pyhbr\\middle\\from_hic.py</code> <pre><code>def get_clinical_codes(\n    engine: Engine, diagnoses_file: str, procedures_file: str\n) -&gt; pd.DataFrame:\n    \"\"\"Main diagnoses/procedures fetch for the HIC data\n\n    This function wraps the diagnoses/procedures queries and a filtering\n    operation to reduce the tables to only those rows which contain a code\n    in a group. One table is returned which contains both the diagnoses and\n    procedures in long format, along with the associated episode ID and the\n    primary/secondary position of the code in the episode.\n\n    Args:\n        engine: The connection to the database\n        diagnoses_file: The diagnoses codes file name (loaded from the package)\n        procedures_file: The procedures codes file name (loaded from the package)\n\n    Returns:\n        A table containing diagnoses/procedures, normalised codes, code groups,\n            diagnosis positions, and associated episode ID.\n    \"\"\"\n\n    diagnosis_codes = load_from_package(diagnoses_file)\n    procedures_codes = load_from_package(procedures_file)\n\n    # Fetch the data from the server\n    diagnoses = get_data(engine, hic.diagnoses_query)\n    procedures = get_data(engine, hic.procedures_query)\n\n    # Reduce data to only code groups, and combine diagnoses/procedures\n    filtered_diagnoses = filter_to_groups(diagnoses, diagnosis_codes)\n    filtered_procedures = filter_to_groups(procedures, procedures_codes)\n\n    # Tag the diagnoses/procedures, and combine the tables\n    filtered_diagnoses[\"type\"] = \"diagnosis\"\n    filtered_procedures[\"type\"] = \"procedure\"\n\n    codes = pd.concat([filtered_diagnoses, filtered_procedures])\n    codes[\"type\"] = codes[\"type\"].astype(\"category\")\n\n    return codes\n</code></pre>"},{"location":"design/#codes-in-other-episodes-relative-to-a-base-episode","title":"Codes in Other Episodes Relative to a Base Episode","text":"<p>To count up codes that occur in a time window before or after a particular base episode, it is necessary to join together each base episode with all the other episodes for the same patient.</p> <p>To do this, three tables are needed:</p> <ul> <li><code>base_episodes</code>: A table of the base episodes of interest, containing <code>episode_id</code> as an index.</li> <li><code>episodes</code>: A table of episode information (all episodes), which is indexed by <code>episode_id</code> and contains <code>patient_id</code> and <code>episode_start</code> as columns.</li> <li><code>codes</code>: The table of diagnosis/procedure codes from the previous section, containing a column <code>episode_id</code> and other code data columns.</li> </ul> <p>A function which combines these into a table containing all codes for other episodes relative to a base episode is <code>pyhbr.clinical_codes.counting.get_all_other_codes</code>:</p> Function that gets data from other episodes relative to a base episode <code></code> <code>get_all_other_codes(base_episodes, data)</code> <p>For each patient, get clinical codes in other episodes before/after the base</p> <p>This makes a table of base episodes along with all other episodes for a patient. Two columns <code>base_episode_id</code> and <code>other_episode_id</code> identify the two episodes for each row (they may be equal), and other information is stored such as the time of the base episode, the time to the other episode, and clinical code information for the other episode.</p> <p>This table is used as the basis for all processing involving counting codes before and after an episode.</p> <p>Parameters:</p> Name Type Description Default <code>base_episodes</code> <code>DataFrame</code> <p>Contains <code>episode_id</code> as an index.</p> required <code>data</code> <code>HicData</code> <p>A class containing two DataFrame attributes: * episodes: Contains <code>episode_id</code> as an index, and <code>patient_id</code> and <code>episode_start</code> as columns * codes: Contains <code>episode_id</code> and other code data as columns</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A table containing columns <code>base_episode_id</code>, <code>other_episode_id</code>, <code>base_episode_start</code>, <code>time_to_other_episode</code>, and code data columns for the other episode. Note that the base episode itself is included as an other episode.</p> Source code in <code>src\\pyhbr\\clinical_codes\\counting.py</code> <pre><code>def get_all_other_codes(base_episodes: DataFrame, data: HicData) -&gt; DataFrame:\n    \"\"\"For each patient, get clinical codes in other episodes before/after the base\n\n    This makes a table of base episodes along with all other episodes for a patient.\n    Two columns `base_episode_id` and `other_episode_id` identify the two episodes\n    for each row (they may be equal), and other information is stored such as the\n    time of the base episode, the time to the other episode, and clinical code information\n    for the other episode.\n\n    This table is used as the basis for all processing involving counting codes before\n    and after an episode.\n\n    Args:\n        base_episodes: Contains `episode_id` as an index.\n        data: A class containing two DataFrame attributes:\n            * episodes: Contains `episode_id` as an index, and `patient_id` and `episode_start` as columns\n            * codes: Contains `episode_id` and other code data as columns\n\n    Returns:\n        A table containing columns `base_episode_id`, `other_episode_id`,\n            `base_episode_start`, `time_to_other_episode`, and code data columns\n            for the other episode. Note that the base episode itself is included\n            as an other episode.\n    \"\"\"\n\n    # Remove everything but the index episode_id (in case base_episodes\n    # already has the columns)\n    df = base_episodes[[]]\n\n    base_episode_info = df.merge(\n        data.episodes[[\"patient_id\", \"episode_start\"]], how=\"left\", on=\"episode_id\"\n    ).rename(columns={\"episode_start\": \"base_episode_start\"})\n\n    other_episodes = base_episode_info.reset_index(names=\"base_episode_id\").merge(\n        data.episodes[[\"episode_start\", \"patient_id\"]].reset_index(names=\"other_episode_id\"),\n        how=\"left\",\n        on=\"patient_id\",\n    )\n\n    other_episodes[\"time_to_other_episode\"] = (\n        other_episodes[\"episode_start\"] - other_episodes[\"base_episode_start\"]\n    )\n\n    with_codes = other_episodes.merge(\n        data.codes, how=\"left\", left_on=\"other_episode_id\", right_on=\"episode_id\"\n    ).drop(columns=[\"patient_id\", \"episode_start\", \"episode_id\"])\n\n    return with_codes\n</code></pre>"},{"location":"design/#counting-codes-group-occurrences","title":"Counting Codes Group Occurrences","text":"<p>In any table that has ``</p>"},{"location":"modelling/","title":"Bleeding/Ischaemia Risk Modelling","text":"<p>Bleeding and ischaemia risk prediction models contained in PyHBR are trained to predict a bleeding and ischaemia outcome defined by clinical codes (ICD-10 and OPCS-4), and are trained on a number of different datasets. The methodology used to develop and test the models is explained below.</p>"},{"location":"modelling/#index-events","title":"Index Events","text":"<p>The target cohort for risk prediction is patients who present in hospital with an acute coronary syndrome (ACS), or receive a percutaneous coronary intervention (PCI), such as a stent implant, and are to be placed on a blood thinning medication such as dual antiplatelet therapy. </p> <p>These patients are identified using diagnosis (ICD-10) and procedure (OPCS-4) code data, which is generated by clinical coders who transcribe written patient notes. Hospital visits are structured into groups of episodes called spells, where each episode contains a list of diagnoses and procedures. Of these, one diagnosis and one procedure is marked as primary.</p> <p>To capture acute presentation for ACS, patients are included if the ACS diagnosis is listed as the primary diagnosis in any episode of the spell. This is to rule out episodes where a historical ACS is coded. A PCI is allowed in any primary or secondary position, on the assumption that inclusion of the procedure means that the procedure was performed.</p>"},{"location":"modelling/#outcome-definition","title":"Outcome Definition","text":"<p>Bleeding and ischaemia outcomes are defined by looking for ICD-10 codes in the spells that occur after the index presentation, up to one year.</p>"},{"location":"modelling/#bleeding-outcome","title":"Bleeding Outcome","text":"<p>The bleeding outcome definition should map to a clinically relevant definition of bleeding for patients on anticoagulant therapy. One such definition is bleeding academic research consortium (BARC) 3 or 5 bleeding<sup>1</sup>, which is the basis of the high bleeding risk definition by the Academic Research Consortium (ARC)<sup>2</sup>.</p> <p>According to BARC criteria, many ICD-10-coded bleeding events may qualify for BARC-2, because being written explicitly in the patient notes (a criterion for coding) could imply that an \"overt\" bleed is present, or that the bleed is \"more than would be expected for the clinical circumstance\". </p> <p>Determining that a bleed qualifies for BARC-3 requires a haemoglobin drop due to the bleed of greater than 3 g/dL. This cannot be determined from an analysis of ICD-10 codes alone. </p> <p>Several attempts to capture \"severe\" bleeding from administrative codes exist in the literature. For example, one study identifies a group of ICD-10 codes with a particularly high positive predictive value (PPV; chance that a ICD-10-coded bleed is in fact a clinically relevant bleed), of 88%<sup>3</sup>. The list of ICD-10 codes is shown below:</p> List of major bleeding codes<sup>3</sup> Description ICD-10CM Codes Subarachnoid hemorrhage I60 Intracranial hemorrhage I61 Subdural hemorrhage I62 Upper gastrointestinal bleeding I85.0, K22.1, K22.6, K25.0, K25.2, K25.4, K25.6,K26.0, K26.2, K26.4, K26.6, K27.0, K27.2, K27.4,K27.6, K28.0, K28.2, K28.4, K28.6, K29.0, K31.80,K63.80, K92.0, K92.1, K92.2 Lower gastrointestinal bleeding K55.2, K51, K57, K62.5, K92.0, K92.1, K92.2 <p>This groups has been interpreted as the following set of (UK) ICD-10 codes:</p> ICD-10 Description I60 Subarachnoid haemorrhage I61 Intracerebral haemorrhage I62 Other nontraumatic intracranial haemorrhage I85.0 Oesophageal varices with bleeding K22.1 Ulcer of oesophagus K22.6 Gastro-oesophageal laceration-haemorrhage syndrome K25.0 Gastric ulcer : acute with haemorrhage K25.2 Gastric ulcer : acute with both haemorrhage and perforation K25.4 Gastric ulcer : chronic or unspecified with haemorrhage K25.6 Gastric ulcer : chronic or unspecified with both haemorrhage and perforation K26.0 Duodenal ulcer : acute with haemorrhage K26.2 Duodenal ulcer : acute with both haemorrhage and perforation K26.4 Duodenal ulcer : chronic or unspecified with haemorrhage K26.6 Duodenal ulcer : chronic or unspecified with both haemorrhage and perforation K27.0 Peptic ulcer, site unspecified : acute with haemorrhage K27.2 Peptic ulcer, site unspecified : acute with both haemorrhage and perforation K27.4 Peptic ulcer, site unspecified : chronic or unspecified with haemorrhage K27.6 Peptic ulcer, site unspecified : chronic or unspecified with both haemorrhage and perforation K28.0 Gastrojejunal ulcer : acute with haemorrhage K28.2 Gastrojejunal ulcer : acute with both haemorrhage and perforation K28.4 Gastrojejunal ulcer : chronic or unspecified with haemorrhage K28.6 Gastrojejunal ulcer : chronic or unspecified with both haemorrhage and perforation K29.0 Acute haemorrhagic gastritis K92.0 Haematemesis K92.1 Melaena K92.2 Gastrointestinal haemorrhage, unspecified K55.2 Angiodysplasia of colon K51 Ulcerative colitis K57 Diverticular disease of intestine K62.5 Haemorrhage of anus and rectum <p>The primary rationale for adopting such a code group is that</p> <ul> <li>It originates from a study where the PPV of the code group was measured (offsetting the risk that coded bleeding definitions do not correspond to real bleeds)</li> <li>The study qualifies the group as \"major\" bleeds (so that it might be taken as a reasonable proxy for BARC 3 or 5 bleeding).</li> </ul> <p>Disadvantages, however, include differences in coding between the UK and Canada (the location of the study), particularly the difference between ICD-10 (UK) and ICD-10CM (Canada). In addition, the presence of unqualified diverticulosis within the bleeding group is not directly a bleeding condition, and may significantly reduce the PPV in older patients.</p> <p>As a result, we define a bleeding outcome based on the BARC 2-5 criteria<sup>4</sup>.</p> Bleeding codes corresponding to BARC 2-5 Category ICD-10 Description Gastrointestinal I85.0 Oesophageal varices with bleeding K25.0 Gastric ulcer, acute with haemorrhage K25.2 Gastric ulcer, acute with both haemorrhage and perforation K25.4 Gastric ulcer, chronic or unspecified with haemorrhage K25.6 Chronic or unspecified with both haemorrhage and perforation K26.0 Duodenal ulcer, acute with haemorrhage K26.2 Duodenal ulcer, acute with both haemorrhage and perforation K26.4 Duodenal ulcer, chronic or unspecified with haemorrhage K26.6 Chronic or unspecified with both haemorrhage and perforation K27.0 Peptic ulcer, acute with haemorrhage K27.2 Peptic ulcer, acute with both haemorrhage and perforation K27.4 Peptic ulcer, chronic or unspecified with haemorrhage K27.6 Chronic or unspecified with both haemorrhage and perforation K28.0 Gastrojejunal ulcer, acute with haemorrhage K28.2 Acute with both haemorrhage and perforation K28.4 Gastrojejunal ulcer, chronic or unspecified with haemorrhage K28.6 Chronic or unspecified with both haemorrhage and perforation K29.0 Acute haemorrhagic gastritis K62.5 Haemorrhage of anus and rectum K66.1 Haemoperitoneum K92.0 Haematemesis K92.1 Melaena K92.2 Gastrointestinal haemorrhage, unspecified Intracerebral I60.* Subarachnoid haemorrhage I61.* Intracerebral haemorrhage I62.* Other nontraumatic intracranial haemorrhage I69.0 Sequelae of subarachnoid haemorrhage I69.1 Sequelae of intracerebral haemorrhage I69.2 Sequelae of other nontraumatic intracranial haemorrhage S06.4 Epidural haemorrhage Genitourinary N93.8 Other specified abnormal uterine and vaginal bleeding N93.9 Abnormal uterine and vaginal bleeding, unspecified Other R04.* Haemorrhage from respiratory passages I23.0 Haemopericardium as current complication following acute myocardial infarction <p>The group is generated based on UK ICD-10 data, which is likely to reduce coding discrepancies, and does not contain the generic diverticulosis category. </p> <p>Further, the lack of attempt to distinguish \"major\" bleeding (i.e. more bleeding codes are included) inherently makes the group more likely to be robust. Coded events are likely to correspond to a bleed written on the chart, with the only mechanism to reduce to positive predictive being coding errors.</p> <p>The primary disadvantage is the use of an all-hospital-bleeding definition. However, this is justified as a simple starting point for modelling, that can be refined later by the inclusion of factors to distinguish major bleeds.</p> <p>No PPV is available for this code group. A basic chart review should be performed on the patients identified by this bleeding group to increase confidence that they match relevant bleeding events.</p>"},{"location":"modelling/#predictors","title":"Predictors","text":"<p>Predictors are defined based on data that is available in the index presentation, if that data would be available to the clinician.</p> <p>Clinical codes are used to define predictors, but an exclusion period of one month is applied to avoid using ICD-10 and OPCS-4 codes that would not have been coded yet before the index (clinical coding happens monthly, and clinical codes are not available until this processing has occurred). </p>"},{"location":"modelling/#models","title":"Models","text":""},{"location":"modelling/#model-testing","title":"Model Testing","text":"<ol> <li> <p>Mehran R, Rao SV, Bhatt DL, Gibson CM, Caixeta A, Eikelboom J, et al. Standardized bleeding definitions for cardiovascular clinical trials: A consensus report from the bleeding academic research consortium. Circulation [Internet]. 2011;123(23):2736\u201347. Available from: https://www.ahajournals.org/doi/10.1161/circulationaha.110.009449 \u21a9</p> </li> <li> <p>Urban P, Mehran R, Colleran R, Angiolillo DJ, Byrne RA, Capodanno D, et al. Defining high bleeding risk in patients undergoing percutaneous coronary intervention: A consensus document from the academic research consortium for high bleeding risk. Circulation [Internet]. 2019;140(3):240\u201361. Available from: https://www.ahajournals.org/doi/10.1161/CIRCULATIONAHA.119.040167 \u21a9</p> </li> <li> <p>Al-Ani F, Shariff S, Siqueira L, Seyam A, Lazo-Langner A. Identifying venous thromboembolism and major bleeding in emergency room discharges using administrative data. Thrombosis research [Internet]. 2015;136(6):1195\u20138. Available from: https://www.thrombosisresearch.com/article/S0049-3848(15)30167-5/abstract \u21a9\u21a9</p> </li> <li> <p>Pufulete M, Harris J, Sterne JA, Johnson TW, Lasserson D, Mumford A, et al. Comprehensive ascertainment of bleeding in patients prescribed different combinations of dual antiplatelet therapy (DAPT) and triple therapy (TT) in the UK: Study protocol for three population-based cohort studies emulating \u201ctarget trials\u201d(the ADAPTT study). BMJ open [Internet]. 2019;9(6):e029388. Available from: https://bmjopen.bmj.com/content/9/6/e029388 \u21a9</p> </li> </ol>"},{"location":"reference/","title":"PyHBR Function Reference","text":"<p>This page contains the documentation for all objects in PyHBR.</p>"},{"location":"reference/#data-sources","title":"Data Sources","text":""},{"location":"reference/#pyhbr.analysis","title":"<code>analysis</code>","text":"<p>Routines for performing statistics, analysis, or fitting models</p>"},{"location":"reference/#pyhbr.analysis.acs","title":"<code>acs</code>","text":""},{"location":"reference/#pyhbr.analysis.acs.index_episodes","title":"<code>index_episodes(data)</code>","text":"<p>Get the index episodes for ACS/PCI patients</p> <p>Index episodes are defined by the contents of the first episode of the spell (i.e. the cause of admission to hospital). Episodes are considered an index event if:</p> <ul> <li>The primary diagnosis contains an ACS ICD-10 code; or</li> <li>There is a PCI procedure in any primary or secondary position</li> </ul> <p>A prerequisite for an episode to be an index episode is that it is present in both the episodes and codes table. The episodes table contains start-time/spell information, and the codes table contains information about what diagnoses/procedures occurred in the episode.</p> <p>The table returned contains only the episodes that match, along with all the information about that episode present in the episodes and codes tables.</p> <p>If you need to modify this function, note that the group names used to identify ACS/PCI groups are called <code>acs</code> and <code>pci</code> (present in the codes table). Also note that the position column in the codes table is indexed from 1.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>HicData</code> <p>A class containing two DataFrame attributes: * episodes: All patient episodes. Must contain <code>episode_id</code>, <code>spell_id</code>     and <code>episode_start</code>. * codes: All diagnosis/procedure codes by episode. Must contain     <code>episode_id</code>, <code>position</code> (indexed from 1 which is the primary     code, &gt;1 are secondary codes), and <code>group</code> (containing either <code>acs</code>     or <code>pci</code>).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The index episodes.</p> Source code in <code>src\\pyhbr\\analysis\\acs.py</code> <pre><code>def index_episodes(data: HicData) -&gt; DataFrame:\n    \"\"\"Get the index episodes for ACS/PCI patients\n\n    Index episodes are defined by the contents of the first episode of\n    the spell (i.e. the cause of admission to hospital). Episodes are\n    considered an index event if:\n\n    * The primary diagnosis contains an ACS ICD-10 code; or\n    * There is a PCI procedure in any primary or secondary position\n\n    A prerequisite for an episode to be an index episode is that it\n    is present in both the episodes and codes table. The episodes table\n    contains start-time/spell information, and the codes table contains\n    information about what diagnoses/procedures occurred in the episode.\n\n    The table returned contains only the episodes that match, along\n    with all the information about that episode present in the episodes\n    and codes tables.\n\n    If you need to modify this function, note that the group names used to\n    identify ACS/PCI groups are called `acs` and `pci` (present in the\n    codes table). Also note that the position column in the codes table\n    is indexed from 1.\n\n    Args:\n        data: A class containing two DataFrame attributes:\n            * episodes: All patient episodes. Must contain `episode_id`, `spell_id`\n                and `episode_start`.\n            * codes: All diagnosis/procedure codes by episode. Must contain\n                `episode_id`, `position` (indexed from 1 which is the primary\n                code, &gt;1 are secondary codes), and `group` (containing either `acs`\n                or `pci`).\n\n    Returns:\n        The index episodes.\n    \"\"\"\n\n    # Index episodes are defined by the contents of the first episode in the\n    # spell (to capture to cause of admission to hospital).\n    first_episodes = (\n        data.episodes.sort_values(\"episode_start\").groupby(\"spell_id\").head(1)\n    )\n\n    # Join the diagnosis/procedure codes (inner join reduces to episodes which\n    # have codes in any group, which is a superset of the index episodes)\n    first_episodes_with_codes = first_episodes.merge(\n        data.codes, how=\"inner\", on=\"episode_id\"\n    )\n\n    # ACS matches based on a primary diagnosis of ACS (this is to rule out\n    # cases where patient history may contain ACS recorded as a secondary\n    # diagnosis).\n    acs_match = (first_episodes_with_codes[\"group\"] == \"acs_bezin\") &amp; (\n        first_episodes_with_codes[\"position\"] == 1\n    )\n\n    # A PCI match is allowed anywhere in the procedures list, but must still\n    # be present in the index episode\n    pci_match = first_episodes_with_codes[\"group\"] == \"pci\"\n\n    # Get all the episodes matching the ACS or PCI condition (multiple rows\n    # per episode)\n    matching_episodes = first_episodes_with_codes[acs_match | pci_match]\n\n    matching_episodes.set_index(\"episode_id\", drop=True, inplace=True)\n\n    # Reduce to one row per episode, and store a flag for whether the ACS\n    # or PCI condition was present\n    index_episodes = DataFrame()\n    index_episodes[\"acs_index\"] = (\n        matching_episodes[\"group\"].eq(\"acs_bezin\").groupby(\"episode_id\").any()\n    )\n    index_episodes[\"pci_index\"] = (\n        matching_episodes[\"group\"].eq(\"pci\").groupby(\"episode_id\").any()\n    )\n\n    # Join some useful information about the episode\n    return index_episodes.merge(\n        data.episodes[[\"patient_id\", \"episode_start\"]], how=\"left\", on=\"episode_id\"\n    )\n</code></pre>"},{"location":"reference/#pyhbr.analysis.arc_hbr","title":"<code>arc_hbr</code>","text":"<p>Calculation of the ARC HBR score</p>"},{"location":"reference/#pyhbr.analysis.arc_hbr.all_index_spell_episodes","title":"<code>all_index_spell_episodes(index_episodes, episodes)</code>","text":"<p>Get all the other episodes in the index spell</p> <p>This is a dataframe of index spells (defined as the spell containing  an episode in index_episodes), along with all the episodes in that spell (including the index episode itself). This is useful for  performing operations at index-spell granularity</p> <p>Parameters:</p> Name Type Description Default <code>index_episodes</code> <code>DataFrame</code> <p>Must contain Pandas index <code>episode_id</code></p> required <code>episodes</code> <code>DataFrame</code> <p>Must contain Pandas index <code>episode_id</code> and have a columne <code>spell_id</code>.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe with a column <code>spell_id</code> for index spells, and <code>episode_id</code> for all episodes in that spell. A column <code>index_episode</code> shows which of the episodes is the first episode in the spell.</p> Source code in <code>src\\pyhbr\\analysis\\arc_hbr.py</code> <pre><code>def all_index_spell_episodes(index_episodes: DataFrame, episodes: DataFrame) -&gt; DataFrame:\n    \"\"\"Get all the other episodes in the index spell\n\n    This is a dataframe of index spells (defined as the spell containing \n    an episode in index_episodes), along with all the episodes in that\n    spell (including the index episode itself). This is useful for \n    performing operations at index-spell granularity\n\n    Args:\n        index_episodes: Must contain Pandas index `episode_id`\n        episodes: Must contain Pandas index `episode_id` and have a columne\n            `spell_id`.\n\n    Returns:\n        A dataframe with a column `spell_id` for index spells, and `episode_id`\n            for all episodes in that spell. A column `index_episode` shows which\n            of the episodes is the first episode in the spell.\n    \"\"\"\n    index_spells = (\n        index_episodes[[]]\n        .merge(episodes[\"spell_id\"], how=\"left\", on=\"episode_id\")\n        .set_index(\"spell_id\")\n    )\n    return index_spells.merge(\n        episodes.reset_index(), how=\"left\", on=\"spell_id\"\n    )[[\"episode_id\", \"spell_id\"]]\n</code></pre>"},{"location":"reference/#pyhbr.analysis.arc_hbr.arc_hbr_age","title":"<code>arc_hbr_age(has_age)</code>","text":"<p>Calculate the age ARC-HBR criterion</p> <p>Calculate the age ARC HBR criterion (0.5 points if &gt; 75 at index, 0 otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>has_age</code> <code>DataFrame</code> <p>Dataframe indexed by episode_id which has a column <code>age</code></p> required <p>Returns:</p> Type Description <code>Series</code> <p>A series of values 0.5 (if age &gt; 75 at index) or 0 otherwise, indexed by <code>episode_id</code>.</p> Source code in <code>src\\pyhbr\\analysis\\arc_hbr.py</code> <pre><code>def arc_hbr_age(has_age: DataFrame) -&gt; Series:\n    \"\"\"Calculate the age ARC-HBR criterion\n\n    Calculate the age ARC HBR criterion (0.5 points if &gt; 75 at index, 0 otherwise.\n\n    Args:\n        has_age: Dataframe indexed by episode_id which has a column `age`\n\n    Returns:\n        A series of values 0.5 (if age &gt; 75 at index) or 0 otherwise, indexed\n            by `episode_id`.\n    \"\"\"\n    return Series(np.where(has_age[\"age\"] &gt; 75, 0.5, 0), index=has_age.index)\n</code></pre>"},{"location":"reference/#pyhbr.analysis.arc_hbr.arc_hbr_anaemia","title":"<code>arc_hbr_anaemia(has_index_hb_and_gender)</code>","text":"<p>Calculate the ARC HBR anaemia (low Hb) criterion</p> <p>Calculates anaemia based on the worst (lowest) index Hb measurement and gender currently. Should be modified to take most recent Hb value or clinical code.</p> <p>Parameters:</p> Name Type Description Default <code>has_index_hb_and_gender</code> <code>DataFrame</code> <p>Dataframe having the column <code>index_hb</code> containing the Hb measurement (g/dL) at the index event, or NaN if no Hb measurement was made. Also contains <code>gender</code> (categorical with categories \"male\", \"female\", and \"unknown\").</p> required <p>Returns:</p> Type Description <code>Series</code> <p>A series containing the HBR score for the index episode.</p> Source code in <code>src\\pyhbr\\analysis\\arc_hbr.py</code> <pre><code>def arc_hbr_anaemia(has_index_hb_and_gender: DataFrame) -&gt; Series:\n    \"\"\"Calculate the ARC HBR anaemia (low Hb) criterion\n\n    Calculates anaemia based on the worst (lowest) index Hb measurement\n    and gender currently. Should be modified to take most recent Hb value\n    or clinical code.\n\n    Args:\n        has_index_hb_and_gender: Dataframe having the column `index_hb` containing the\n            Hb measurement (g/dL) at the index event, or NaN if no Hb measurement\n            was made. Also contains `gender` (categorical with categories \"male\",\n            \"female\", and \"unknown\").\n\n    Returns:\n        A series containing the HBR score for the index episode.\n    \"\"\"\n\n    df = has_index_hb_and_gender\n\n    # Evaluated in order\n    arc_score_conditions = [\n        df[\"index_hb\"] &lt; 11.0,  # Major for any gender\n        df[\"index_hb\"] &lt; 11.9,  # Minor for any gender\n        (df[\"index_hb\"] &lt; 12.9) &amp; (df[\"gender\"] == \"male\"),  # Minor for male\n        df[\"index_hb\"] &gt;= 12.9,  # None for any gender\n    ]\n    arc_scores = [1.0, 0.5, 0.5, 0.0]\n\n    # Default is used to fill missing Hb score with 0.0 for now. TODO: replace with\n    # fall-back to recent Hb, or codes.\n    return Series(\n        np.select(arc_score_conditions, arc_scores, default=0.0),\n        index=df.index,\n    )\n</code></pre>"},{"location":"reference/#pyhbr.analysis.arc_hbr.arc_hbr_cancer","title":"<code>arc_hbr_cancer(has_prior_cancer)</code>","text":"<p>Calculate the cancer ARC HBR criterion</p> <p>This function takes a dataframe with a column prior_cancer with a count of the cancer diagnoses in the previous year.</p> <p>Parameters:</p> Name Type Description Default <code>has_prior_cancer</code> <code>DataFrame</code> <p>Has a column <code>prior_cancer</code> with a count of the number of cancer diagnoses occurring in the year before the index event.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>The ARC HBR cancer criterion (0.0, 1.0)</p> Source code in <code>src\\pyhbr\\analysis\\arc_hbr.py</code> <pre><code>def arc_hbr_cancer(has_prior_cancer: DataFrame) -&gt; Series:\n    \"\"\"Calculate the cancer ARC HBR criterion\n\n    This function takes a dataframe with a column prior_cancer\n    with a count of the cancer diagnoses in the previous year.\n\n    Args:\n        has_prior_cancer: Has a column `prior_cancer` with a count\n            of the number of cancer diagnoses occurring in the\n            year before the index event.\n\n    Returns:\n        The ARC HBR cancer criterion (0.0, 1.0)\n    \"\"\"\n    return Series(\n        np.where(has_prior_cancer[\"prior_cancer\"] &gt; 0, 1.0, 0),\n        index=has_prior_cancer.index,\n    )\n</code></pre>"},{"location":"reference/#pyhbr.analysis.arc_hbr.arc_hbr_cirrhosis_ptl_hyp","title":"<code>arc_hbr_cirrhosis_ptl_hyp(has_prior_cirrhosis)</code>","text":"<p>Calculate the liver cirrhosis with portal hypertension ARC HBR criterion</p> <p>This function takes a dataframe with two columns prior_cirrhosis and prior_portal_hyp, which count the number of diagnosis of liver cirrhosis and portal hypertension seen in the previous year.</p> <p>Parameters:</p> Name Type Description Default <code>has_prior_cirrhosis</code> <code>DataFrame</code> <p>Has columns <code>prior_cirrhosis</code> and <code>prior_portal_hyp</code>.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>The ARC HBR criterion (0.0, 1.0)</p> Source code in <code>src\\pyhbr\\analysis\\arc_hbr.py</code> <pre><code>def arc_hbr_cirrhosis_ptl_hyp(has_prior_cirrhosis: DataFrame) -&gt; Series:\n    \"\"\"Calculate the liver cirrhosis with portal hypertension ARC HBR criterion\n\n    This function takes a dataframe with two columns prior_cirrhosis\n    and prior_portal_hyp, which count the number of diagnosis of\n    liver cirrhosis and portal hypertension seen in the previous\n    year.\n\n    Args:\n        has_prior_cirrhosis: Has columns `prior_cirrhosis` and\n            `prior_portal_hyp`.\n\n    Returns:\n        The ARC HBR criterion (0.0, 1.0)\n    \"\"\"\n    cirrhosis = has_prior_cirrhosis[\"prior_cirrhosis\"] &gt; 0\n    portal_hyp = has_prior_cirrhosis[\"prior_portal_hyp\"] &gt; 0\n\n    return Series(\n        np.where(cirrhosis &amp; portal_hyp, 1.0, 0),\n        index=has_prior_cirrhosis.index,\n    )\n</code></pre>"},{"location":"reference/#pyhbr.analysis.arc_hbr.arc_hbr_ckd","title":"<code>arc_hbr_ckd(has_index_egfr)</code>","text":"<p>Calculate the ARC HBR chronic kidney disease (CKD) criterion</p> <p>The ARC HBR CKD criterion is calculated based on the eGFR as follows:</p> eGFR Score eGFR &lt; 30 mL/min 1.0 30 mL/min \\&lt;= eGFR &lt; 60 mL/min 0.5 eGFR &gt;= 60 mL/min 0.0 <p>If the eGFR is NaN, set score to zero (TODO: fall back to ICD-10 codes in this case)</p> <p>Parameters:</p> Name Type Description Default <code>has_index_egfr</code> <code>DataFrame</code> <p>Dataframe having the column <code>index_egfr</code> (in units of mL/min) with the eGFR measurement at index, or NaN which means no eGFR measurement was found at the index.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>A series containing the CKD ARC criterion, based on the eGFR at index.</p> Source code in <code>src\\pyhbr\\analysis\\arc_hbr.py</code> <pre><code>def arc_hbr_ckd(has_index_egfr: DataFrame) -&gt; Series:\n    \"\"\"Calculate the ARC HBR chronic kidney disease (CKD) criterion\n\n    The ARC HBR CKD criterion is calculated based on the eGFR as\n    follows:\n\n    | eGFR                           | Score |\n    |--------------------------------|-------|\n    | eGFR &lt; 30 mL/min               | 1.0   |\n    | 30 mL/min \\&lt;= eGFR &lt; 60 mL/min | 0.5   |\n    | eGFR &gt;= 60 mL/min              | 0.0   |\n\n    If the eGFR is NaN, set score to zero (TODO: fall back to ICD-10\n    codes in this case)\n\n    Args:\n        has_index_egfr: Dataframe having the column `index_egfr` (in units of mL/min)\n            with the eGFR measurement at index, or NaN which means no eGFR\n            measurement was found at the index.\n\n    Returns:\n        A series containing the CKD ARC criterion, based on the eGFR at\n            index.\n    \"\"\"\n\n    # Replace NaN values for now with 100 (meaning score 0.0)\n    df = has_index_egfr[\"index_egfr\"].fillna(90)\n\n    # Using a high upper limit to catch any high eGFR values. In practice,\n    # the highest value is 90 (which comes from the string \"&gt;90\" in the database).\n    return cut(df, [0, 30, 60, 10000], right=False, labels=[1.0, 0.5, 0.0])\n</code></pre>"},{"location":"reference/#pyhbr.analysis.arc_hbr.arc_hbr_ischaemic_stroke_ich","title":"<code>arc_hbr_ischaemic_stroke_ich(has_prior_ischaemic_stroke)</code>","text":"<p>Calculate the ischaemic stroke/intracranial haemorrhage ARC HBR criterion</p> <p>This function takes a dataframe with two columns prior_bavm_ich and prior_portal_hyp, which count the number of diagnosis of liver cirrhosis and portal hypertension seen in the previous year.</p> <p>If bAVM/ICH is present, 1.0 is added to the score. Else, if ischaemic stroke is present, add 0.5. Otherwise add 0.</p> <p>Parameters:</p> Name Type Description Default <code>has_prior_ischaemic_stroke</code> <code>DataFrame</code> <p>Has a column <code>prior_ischaemic_stroke</code> containing the number of any-severity ischaemic strokes in the previous year, and a column <code>prior_bavm_ich</code> containing a count of any diagnosis of brain arteriovenous malformation or intracranial haemorrhage.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>The ARC HBR criterion (0.0, 1.0)</p> Source code in <code>src\\pyhbr\\analysis\\arc_hbr.py</code> <pre><code>def arc_hbr_ischaemic_stroke_ich(has_prior_ischaemic_stroke: DataFrame) -&gt; Series:\n    \"\"\"Calculate the ischaemic stroke/intracranial haemorrhage ARC HBR criterion\n\n    This function takes a dataframe with two columns prior_bavm_ich\n    and prior_portal_hyp, which count the number of diagnosis of\n    liver cirrhosis and portal hypertension seen in the previous\n    year.\n\n    If bAVM/ICH is present, 1.0 is added to the score. Else, if\n    ischaemic stroke is present, add 0.5. Otherwise add 0.\n\n    Args:\n        has_prior_ischaemic_stroke: Has a column `prior_ischaemic_stroke` containing\n            the number of any-severity ischaemic strokes in the previous\n            year, and a column `prior_bavm_ich` containing a count of\n            any diagnosis of brain arteriovenous malformation or\n            intracranial haemorrhage.\n\n\n    Returns:\n        The ARC HBR criterion (0.0, 1.0)\n    \"\"\"\n    ischaemic_stroke = has_prior_ischaemic_stroke[\"prior_ischaemic_stroke\"] &gt; 0\n    bavm_ich = has_prior_ischaemic_stroke[\"prior_bavm_ich\"] &gt; 0\n\n    score_one = np.where(bavm_ich, 1, 0)\n    score_half = np.where(ischaemic_stroke, 0.5, 0)\n    score_zero = np.zeros(len(has_prior_ischaemic_stroke))\n\n    return Series(\n        np.maximum(score_one, score_half, score_zero),\n        index=has_prior_ischaemic_stroke.index,\n    )\n</code></pre>"},{"location":"reference/#pyhbr.analysis.arc_hbr.arc_hbr_nsaid","title":"<code>arc_hbr_nsaid(index_episodes, prescriptions)</code>","text":"<p>Calculate the non-steroidal anti-inflamatory drug (NSAID) ARC HBR criterion</p> <p>1.0 point is added if an one of the following NSAIDs is present on admission:</p> <ul> <li>Ibuprofen</li> <li>Naproxen</li> <li>Diclofenac</li> <li>Celecoxib</li> <li>Mefenamic acid</li> <li>Etoricoxib</li> <li>Indomethacin</li> </ul> <p>Parameters:</p> Name Type Description Default <code>index_episodes</code> <code>DataFrame</code> <p>Index <code>episode_id</code> is used to narrow prescriptions.</p> required <code>prescriptions</code> <code>DataFrame</code> <p>Contains <code>name</code> (of medicine).</p> required <p>Returns:</p> Type Description <code>Series</code> <p>The OAC ARC score for each index event.</p> Source code in <code>src\\pyhbr\\analysis\\arc_hbr.py</code> <pre><code>def arc_hbr_nsaid(index_episodes: DataFrame, prescriptions: DataFrame) -&gt; Series:\n    \"\"\"Calculate the non-steroidal anti-inflamatory drug (NSAID) ARC HBR criterion\n\n    1.0 point is added if an one of the following NSAIDs is present\n    on admission:\n\n    * Ibuprofen\n    * Naproxen\n    * Diclofenac\n    * Celecoxib\n    * Mefenamic acid\n    * Etoricoxib\n    * Indomethacin\n\n    Args:\n        index_episodes: Index `episode_id` is used to narrow prescriptions.\n        prescriptions: Contains `name` (of medicine).\n\n    Returns:\n        The OAC ARC score for each index event.\n    \"\"\"\n    df = index_episodes.merge(prescriptions, how=\"left\", on=\"episode_id\")\n    nsaid_criterion = ((df[\"group\"] == \"nsaid\") &amp; (df[\"on_admission\"] == True)).astype(\n        \"float\"\n    )\n    return nsaid_criterion.set_axis(index_episodes.index)\n</code></pre>"},{"location":"reference/#pyhbr.analysis.arc_hbr.arc_hbr_oac","title":"<code>arc_hbr_oac(index_episodes, data)</code>","text":"<p>Calculate the oral-anticoagulant ARC HBR criterion</p> <p>1.0 point if an one of the OACs \"warfarin\", \"apixaban\", \"rivaroxaban\", \"edoxaban\", \"dabigatran\", is present in the index spell (meaning the index episode, or any other episode in the spell).</p> <p>Note</p> <p>The on admission flag could be used to imply expected chronic/extended use, but this is not included as it filters out all OAC prescriptions in the HIC data.</p> <p>Parameters:</p> Name Type Description Default <code>index_episodes</code> <code>DataFrame</code> <p>Index <code>episode_id</code> is used to narrow prescriptions.</p> required <code>prescriptions</code> <p>Contains <code>name</code> (of medicine).</p> required <p>Returns:</p> Type Description <code>Series</code> <p>The OAC ARC score for each index event.</p> Source code in <code>src\\pyhbr\\analysis\\arc_hbr.py</code> <pre><code>def arc_hbr_oac(index_episodes: DataFrame, data: HicData) -&gt; Series:\n    \"\"\"Calculate the oral-anticoagulant ARC HBR criterion\n\n    1.0 point if an one of the OACs \"warfarin\", \"apixaban\",\n    \"rivaroxaban\", \"edoxaban\", \"dabigatran\", is present\n    in the index spell (meaning the index episode, or any\n    other episode in the spell).\n\n    !!! note\n        The on admission flag could be used to imply expected\n        chronic/extended use, but this is not included as it filters\n        out all OAC prescriptions in the HIC data.\n\n    Args:\n        index_episodes: Index `episode_id` is used to narrow prescriptions.\n        prescriptions: Contains `name` (of medicine).\n\n    Returns:\n        The OAC ARC score for each index event.\n    \"\"\"\n\n    # Get all the episodes in the index spell (not just the index\n    # episode), and then get a map from spell_id back to to the index\n    # episode_id\n    all_spell_episodes = all_index_spell_episodes(index_episodes, data.episodes)\n    spell_id_to_index_episode = index_episodes.merge(\n        all_spell_episodes, how=\"left\", on=\"episode_id\"\n    )[[\"spell_id\", \"episode_id\"]]\n\n    # Get all the prescriptions that happened in the index spell, and keep\n    # track of which index episode is linked to the spell\n    df = all_spell_episodes.merge(data.prescriptions, how=\"left\", on=\"episode_id\")\n\n    # Find which index spells have an OAC prescription anywhere\n    oac_list = [\"warfarin\", \"apixaban\", \"rivaroxaban\", \"edoxaban\", \"dabigatran\"]\n    df[\"oac\"] = df[\"name\"].isin(oac_list)\n    index_spells_with_oac = DataFrame(df.groupby(\"spell_id\")[\"oac\"].any())\n    oac_criterion = index_spells_with_oac.merge(\n        spell_id_to_index_episode, how=\"left\", on=\"spell_id\"\n    ).set_index(\"episode_id\")[\"oac\"]\n\n    return oac_criterion.astype(\"float\")\n</code></pre>"},{"location":"reference/#pyhbr.analysis.arc_hbr.arc_hbr_prior_bleeding","title":"<code>arc_hbr_prior_bleeding(has_prior_bleeding)</code>","text":"<p>Calculate the prior bleeding/transfusion ARC HBR criterion</p> <p>This function takes a dataframe with a column prior_bleeding_12 with a count of the prior bleeding events in the previous year.</p> <p>TODO: Input needs a separate column for bleeding in 6 months and bleeding in a year, so distinguish 0.5 from 1. Also need to add transfusion.</p> <p>Parameters:</p> Name Type Description Default <code>has_prior_bleeding</code> <code>DataFrame</code> <p>Has a column <code>prior_bleeding_12</code> with a count of the number of bleeds occurring one year before the index. Has <code>episode_id</code> as the index.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>The ARC HBR bleeding/transfusion criterion (0.0, 0.5, or 1.0)</p> Source code in <code>src\\pyhbr\\analysis\\arc_hbr.py</code> <pre><code>def arc_hbr_prior_bleeding(has_prior_bleeding: DataFrame) -&gt; Series:\n    \"\"\"Calculate the prior bleeding/transfusion ARC HBR criterion\n\n    This function takes a dataframe with a column prior_bleeding_12\n    with a count of the prior bleeding events in the previous year.\n\n    TODO: Input needs a separate column for bleeding in 6 months and\n    bleeding in a year, so distinguish 0.5 from 1. Also need to add\n    transfusion.\n\n    Args:\n        has_prior_bleeding: Has a column `prior_bleeding_12` with a count\n            of the number of bleeds occurring one year before the index.\n            Has `episode_id` as the index.\n\n    Returns:\n        The ARC HBR bleeding/transfusion criterion (0.0, 0.5, or 1.0)\n    \"\"\"\n    return Series(\n        np.where(has_prior_bleeding[\"prior_bleeding_12\"] &gt; 0, 0.5, 0),\n        index=has_prior_bleeding.index,\n    )\n</code></pre>"},{"location":"reference/#pyhbr.analysis.arc_hbr.arc_hbr_tcp","title":"<code>arc_hbr_tcp(has_index_platelets)</code>","text":"<p>Calculate the ARC HBR thrombocytopenia (low platelet count) criterion</p> <p>The score is 1.0 if platelet count &lt; 100e9/L, otherwise it is 0.0.</p> <p>Parameters:</p> Name Type Description Default <code>has_index_platelets</code> <code>DataFrame</code> <p>Has column <code>index_platelets</code>, which is the platelet count measurement in the index.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Series containing the ARC score</p> Source code in <code>src\\pyhbr\\analysis\\arc_hbr.py</code> <pre><code>def arc_hbr_tcp(has_index_platelets: DataFrame) -&gt; Series:\n    \"\"\"Calculate the ARC HBR thrombocytopenia (low platelet count) criterion\n\n    The score is 1.0 if platelet count &lt; 100e9/L, otherwise it is 0.0.\n\n    Args:\n        has_index_platelets: Has column `index_platelets`, which is the\n            platelet count measurement in the index.\n\n    Returns:\n        Series containing the ARC score\n    \"\"\"\n    return Series(\n        np.where(has_index_platelets[\"index_platelets\"] &lt; 100, 1.0, 0),\n        index=has_index_platelets.index,\n    )\n</code></pre>"},{"location":"reference/#pyhbr.analysis.arc_hbr.calculate_age","title":"<code>calculate_age(index_episodes, demographics)</code>","text":"<p>Calculate the patient age at index</p> <p>The HIC data contains only year_of_birth, which is used here. In order to make an unbiased estimate of the age, birthday is assumed to be 2nd july (halfway through the year).</p> <p>Parameters:</p> Name Type Description Default <code>index_episodes</code> <code>DataFrame</code> <p>Contains <code>episode_start</code> date and column <code>patient_id</code>, indexed by <code>episode_id</code>.</p> required <code>demographics</code> <code>DataFrame</code> <p>Contains <code>year_of_birth</code> date and index <code>patient_id</code>.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>A series containing age, indexed by <code>episode_id</code>.</p> Source code in <code>src\\pyhbr\\analysis\\arc_hbr.py</code> <pre><code>def calculate_age(index_episodes: DataFrame, demographics: DataFrame) -&gt; Series:\n    \"\"\"Calculate the patient age at index\n\n    The HIC data contains only year_of_birth, which is used here. In order\n    to make an unbiased estimate of the age, birthday is assumed to be\n    2nd july (halfway through the year).\n\n    Args:\n        index_episodes: Contains `episode_start` date and column `patient_id`,\n            indexed by `episode_id`.\n        demographics: Contains `year_of_birth` date and index `patient_id`.\n\n    Returns:\n        A series containing age, indexed by `episode_id`.\n    \"\"\"\n    df = index_episodes.merge(demographics, how=\"left\", on=\"patient_id\")\n    age_offset = np.where(\n        (df[\"episode_start\"].dt.month &lt; 7) &amp; (df[\"episode_start\"].dt.day &lt; 2), 1, 0\n    )\n    age_at_index = df[\"episode_start\"].dt.year - df[\"year_of_birth\"] - age_offset\n    age_at_index.index = index_episodes.index\n    return age_at_index\n</code></pre>"},{"location":"reference/#pyhbr.analysis.arc_hbr.first_index_spell_result","title":"<code>first_index_spell_result(test_name, index_episodes, data)</code>","text":"<p>Get the (first) lab result associated to the index spell</p> <p>Compared to min_index_result, this function accounts for the possibility that a lab result was not associated with the first episode of the spell, and is therefore missed. The minimum value is not taken, because of the possibility that a value in a later episode is smaller due to some other cause.</p> <p>Parameters:</p> Name Type Description Default <code>index_episodes</code> <code>DataFrame</code> <p>Has an <code>episode_id</code> index.</p> required <code>lab_results</code> <p>Has a <code>test_name</code> column matching the <code>test_name</code> argument, and a <code>result</code> column for the numerical test result.</p> required <code>episodes</code> <p>Required to obtain the other episodes in the same spell as the index episode. Has a <code>spell_id</code> column and an <code>episode_id</code> index.</p> required <code>test_name</code> <code>str</code> <p>Which lab measurement to get.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>A series containing the minimum value of test_name in the index episode. Contains NaN if test_name was not recorded in the index episode.</p> Source code in <code>src\\pyhbr\\analysis\\arc_hbr.py</code> <pre><code>def first_index_spell_result(\n    test_name: str,\n    index_episodes: DataFrame,\n    data: HicData,\n) -&gt; Series:\n    \"\"\"Get the (first) lab result associated to the index spell\n\n    Compared to min_index_result, this function accounts for the\n    possibility that a lab result was not associated with the first\n    episode of the spell, and is therefore missed. The minimum value\n    is not taken, because of the possibility that a value in a later\n    episode is smaller due to some other cause.\n\n    Args:\n        index_episodes: Has an `episode_id` index.\n        lab_results: Has a `test_name` column matching the `test_name` argument,\n            and a `result` column for the numerical test result.\n        episodes: Required to obtain the other episodes in the same spell as\n            the index episode. Has a `spell_id` column and an `episode_id` index.\n        test_name: Which lab measurement to get.\n\n    Returns:\n        A series containing the minimum value of test_name in the index\n            episode. Contains NaN if test_name was not recorded in the\n            index episode.\n    \"\"\"\n\n    # Find the spells that contain the index episodes. This is\n    # used to get a list of all episodes in the index spells.\n    all_spell_episodes = all_index_spell_episodes(index_episodes, data.episodes)\n\n    # Get the lab tests specified by test_name for all the episodes\n    # which occur in the index spell.\n    df = all_spell_episodes.merge(data.lab_results, how=\"left\", on=\"episode_id\")\n    index_lab_result = df[df[\"test_name\"] == test_name]\n\n    # Pick the first result. For measurements such as platelet count,\n    # eGFR, and Hb, lower is more severe.\n    first_lab_result = (\n        index_lab_result.sort_values(\"sample_date\").groupby(\"spell_id\").head(1)\n    )\n\n    # Some index episodes do not have an measurement, so join\n    # to get all index episodes (NaN means no index measurement)\n    first_result_or_nan = index_episodes.merge(\n        first_lab_result[[\"result\", \"episode_id\"]], how=\"left\", on=\"episode_id\"\n    ).set_index(\"episode_id\")\n\n    return first_result_or_nan[\"result\"].rename(f\"index_{test_name}\")\n</code></pre>"},{"location":"reference/#pyhbr.analysis.arc_hbr.get_arc_hbr_score","title":"<code>get_arc_hbr_score(features, data)</code>","text":"<p>Calculate the ARC HBR score</p> <p>The <code>features</code> table has one row per index event, and must have the following data:</p> <ul> <li><code>episode_id</code> as Pandas index.</li> <li><code>age</code> column for age at index.</li> <li><code>gender</code> column for patient gender (category with values \"male\", \"female\"     or \"unknown\")</li> <li><code>min_index_egfr</code> column containing the minimum eGFR measurement at     the index episode, in mL/min</li> <li><code>min_index_hb</code> column containing the minimum Hb measurement at the     index episode, in g/dL.</li> <li><code>min_index_platelets</code> column containing the minimum platelet count     measurement at the index episode, in units 100e9/L.</li> <li><code>prior_bleeding_12</code> column containing the total number of qualifying     prior bleeding events that occurred in the previous 12 months before     the index event.</li> <li><code>prior_cancer</code> column containing the total number of cancer diagnosis     codes seen in the previous 12 months before the index event.</li> </ul> <p>The data.prescriptions table must be indexed by <code>episode_id</code>, and needs a <code>name</code> column (str, for medicine name), and an <code>on_admission</code> column (bool) for whether the medicine was present on hospital admission.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>DataFrame</code> <p>Table of index-episode data for calculating the ARC HBR score</p> required <code>prescriptions</code> <p>A class containing a DataFrame attribute prescriptions.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A table with one column per ARC HBR criterion, containing the score (0.0, 0.5, or 1.0)</p> Source code in <code>src\\pyhbr\\analysis\\arc_hbr.py</code> <pre><code>def get_arc_hbr_score(features: DataFrame, data: HicData) -&gt; DataFrame:\n    \"\"\"Calculate the ARC HBR score\n\n    The `features` table has one row per index event, and must have the\n    following data:\n\n    * `episode_id` as Pandas index.\n    * `age` column for age at index.\n    * `gender` column for patient gender (category with values \"male\", \"female\"\n        or \"unknown\")\n    * `min_index_egfr` column containing the minimum eGFR measurement at\n        the index episode, in mL/min\n    * `min_index_hb` column containing the minimum Hb measurement at the\n        index episode, in g/dL.\n    * `min_index_platelets` column containing the minimum platelet count\n        measurement at the index episode, in units 100e9/L.\n    * `prior_bleeding_12` column containing the total number of qualifying\n        prior bleeding events that occurred in the previous 12 months before\n        the index event.\n    * `prior_cancer` column containing the total number of cancer diagnosis\n        codes seen in the previous 12 months before the index event.\n\n    The data.prescriptions table must be indexed by `episode_id`, and needs a `name`\n    column (str, for medicine name), and an `on_admission` column (bool) for\n    whether the medicine was present on hospital admission.\n\n    Args:\n        features: Table of index-episode data for calculating the ARC HBR score\n        prescriptions: A class containing a DataFrame attribute prescriptions.\n\n    Returns:\n        A table with one column per ARC HBR criterion, containing the score (0.0,\n            0.5, or 1.0)\n    \"\"\"\n\n    # Calculate the ARC HBR score\n    arc_score_data = {\n        \"arc_hbr_age\": arc_hbr_age(features),\n        \"arc_hbr_oac\": arc_hbr_oac(features, data),\n        \"arc_hbr_ckd\": arc_hbr_ckd(features),\n        \"arc_hbr_anaemia\": arc_hbr_anaemia(features),\n        \"arc_hbr_tcp\": arc_hbr_tcp(features),\n        \"arc_hbr_prior_bleeding\": arc_hbr_prior_bleeding(features),\n        \"arc_hbr_cirrhosis_portal_hyp\": arc_hbr_cirrhosis_ptl_hyp(features),\n        \"arc_hbr_ischaemic_stroke_ich\": arc_hbr_ischaemic_stroke_ich(features),\n        \"arc_hbr_cancer\": arc_hbr_cancer(features),\n        \"arc_hbr_nsaid\": arc_hbr_nsaid(features, data.prescriptions),\n    }\n    return DataFrame(arc_score_data)\n</code></pre>"},{"location":"reference/#pyhbr.analysis.arc_hbr.get_features","title":"<code>get_features(index_episodes, previous_year, data, index_result_fn)</code>","text":"<p>Index/prior history features for calculating ARC HBR</p> <p>Make table of general features (more granular than the ARC-HBR criteria, from which the ARC score will be computed)</p> <p>Parameters:</p> Name Type Description Default <code>index_episodes</code> <code>DataFrame</code> <p>Contains <code>acs_index</code> and <code>pci_index</code> columns (indexed by <code>episode_id</code>)</p> required <code>previous_year</code> <code>DataFrame</code> <p>Contains the all_other_codes table narrowed to the previous year (for the purposes of counting code groups).</p> required <code>data</code> <code>HicData</code> <p>Contains DataFrame attributes <code>demographics</code> and <code>lab_results</code>.</p> required <code>index_result_fn</code> <code>Callable[[str, DataFrame, HicData], Series]</code> <p>The function to calculate the value of an index laboratory measurement. Can be any function which takes the measurement name as the first argument, the index_episodes as the second, and the HicData as the third argument. You can pass min_index_result to get the lowest test result from the index episode, or first_index_spell_result to get the first measurement value from any episode in the spell.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The table of features (used for calculating the ARC HBR score).</p> Source code in <code>src\\pyhbr\\analysis\\arc_hbr.py</code> <pre><code>def get_features(\n    index_episodes: DataFrame,\n    previous_year: DataFrame,\n    data: HicData,\n    index_result_fn: Callable[[str, DataFrame, HicData], Series]\n) -&gt; DataFrame:\n    \"\"\"Index/prior history features for calculating ARC HBR\n\n    Make table of general features (more granular than the ARC-HBR criteria,\n    from which the ARC score will be computed)\n\n    Args:\n        index_episodes: Contains `acs_index` and `pci_index` columns (indexed\n            by `episode_id`)\n        previous_year: Contains the all_other_codes table narrowed to the previous\n            year (for the purposes of counting code groups).\n        data: Contains DataFrame attributes `demographics` and\n            `lab_results`.\n        index_result_fn: The function to calculate the value of an index\n            laboratory measurement. Can be any function which takes the\n            measurement name as the first argument, the index_episodes as\n            the second, and the HicData as the third argument. You can\n            pass min_index_result to get the lowest test result from the\n            index episode, or first_index_spell_result to get the first\n            measurement value from any episode in the spell.\n\n    Returns:\n        The table of features (used for calculating the ARC HBR score).\n    \"\"\"\n\n    cirrhosis_groups = [\"cirrhosis\"]\n    portal_hyp_groups = [\"portal_hypertension\"]\n    bleeding_groups = [\"bleeding_al_ani\"]\n    cancer_groups = [\"cancer\"]\n    bavm_ich_groups = [\"bavm\", \"ich\"]\n    ischaemic_stroke_groups = [\"ischaemic_stroke\"]\n    feature_data = {\n        \"age\": calculate_age(index_episodes, data.demographics),\n        \"gender\": get_gender(index_episodes, data.demographics),\n        \"index_egfr\": index_result_fn(\"egfr\", index_episodes, data),\n        \"index_hb\": index_result_fn(\"hb\", index_episodes, data),\n        \"index_platelets\": index_result_fn(\"platelets\", index_episodes, data),\n        # Only use primary position, as proxy for \"requiring hospitalisation\".\n        # Note: logic will need to account for \"first episode\" when multiple\n        # episodes are used.\n        \"prior_bleeding_12\": counting.count_code_groups(\n            index_episodes, previous_year, bleeding_groups, False\n        ),\n        \"prior_cirrhosis\": counting.count_code_groups(\n            index_episodes, previous_year, cirrhosis_groups, True\n        ),\n        \"prior_portal_hyp\": counting.count_code_groups(\n            index_episodes, previous_year, portal_hyp_groups, True\n        ),\n        \"prior_bavm_ich\": counting.count_code_groups(\n            index_episodes, previous_year, bavm_ich_groups, True\n        ),\n        \"prior_ischaemic_stroke\": counting.count_code_groups(\n            index_episodes, previous_year, ischaemic_stroke_groups, True\n        ),\n        # TODO: transfusion\n        \"prior_cancer\": counting.count_code_groups(\n            index_episodes, previous_year, cancer_groups, True\n        ),\n        # TODO: add cancer therapy\n    }\n    features = DataFrame(feature_data)\n    features[[\"acs_index\", \"pci_index\"]] = index_episodes[[\"acs_index\", \"pci_index\"]]\n    return features\n</code></pre>"},{"location":"reference/#pyhbr.analysis.arc_hbr.get_gender","title":"<code>get_gender(index_episodes, demographics)</code>","text":"<p>Get gender from the demographics table for each index event</p> <p>Parameters:</p> Name Type Description Default <code>index_episodes</code> <code>DataFrame</code> <p>Indexed by <code>episode_id</code> and having column <code>patient_id</code></p> required <code>demographics</code> <code>DataFrame</code> <p>Having columns <code>patient_id</code> and <code>gender</code>.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>A series containing gender indexed by <code>episode_id</code></p> Source code in <code>src\\pyhbr\\analysis\\arc_hbr.py</code> <pre><code>def get_gender(index_episodes: DataFrame, demographics: DataFrame) -&gt; Series:\n    \"\"\"Get gender from the demographics table for each index event\n\n    Args:\n        index_episodes: Indexed by `episode_id` and having column `patient_id`\n        demographics: Having columns `patient_id` and `gender`.\n\n    Returns:\n        A series containing gender indexed by `episode_id`\n    \"\"\"\n    gender = index_episodes.merge(demographics, how=\"left\", on=\"patient_id\")[\"gender\"]\n    gender.index = index_episodes.index\n    return gender\n</code></pre>"},{"location":"reference/#pyhbr.analysis.arc_hbr.min_index_result","title":"<code>min_index_result(test_name, index_episodes, data)</code>","text":"<p>Get the lowest lab result associated to the index episode</p> <p>Getting the lowest value corresponds to the worst severity for measurements such as eGFR, Hb and platelet count.</p> <p>Parameters:</p> Name Type Description Default <code>index_episodes</code> <code>DataFrame</code> <p>Has an <code>episode_id</code> for filtering lab_results</p> required <code>lab_results</code> <p>Has a <code>test_name</code> column matching the <code>test_name</code> argument, and a <code>result</code> column for the numerical test result</p> required <code>test_name</code> <code>str</code> <p>Which lab measurement to get.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>A series containing the minimum value of test_name in the index episode. Contains NaN if test_name was not recorded in the index episode.</p> Source code in <code>src\\pyhbr\\analysis\\arc_hbr.py</code> <pre><code>def min_index_result(\n    test_name: str, index_episodes: DataFrame, data: HicData\n) -&gt; Series:\n    \"\"\"Get the lowest lab result associated to the index episode\n\n    Getting the lowest value corresponds to the worst severity for\n    measurements such as eGFR, Hb and platelet count.\n\n    Args:\n        index_episodes: Has an `episode_id` for filtering lab_results\n        lab_results: Has a `test_name` column matching the `test_name` argument,\n            and a `result` column for the numerical test result\n        test_name: Which lab measurement to get.\n\n    Returns:\n        A series containing the minimum value of test_name in the index\n            episode. Contains NaN if test_name was not recorded in the\n            index episode.\n    \"\"\"\n    df = index_episodes.merge(data.lab_results, how=\"left\", on=\"episode_id\")\n    index_lab_result = df[df[\"test_name\"] == test_name]\n\n    # Pick the lowest result. For measurements such as platelet count,\n    # eGFR, and Hb, lower is more severe.\n    min_index_lab_result = index_lab_result.groupby(\"episode_id\").min(\"result\")\n\n    # Some index episodes do not have an measurement, so join\n    # to get all index episodes (NaN means no index measurement)\n    min_result_or_nan = index_episodes.merge(\n        min_index_lab_result[\"result\"], how=\"left\", on=\"episode_id\"\n    )\n\n    return min_result_or_nan[\"result\"].rename(f\"index_{test_name}\")\n</code></pre>"},{"location":"reference/#pyhbr.analysis.arc_hbr.plot_index_measurement_distribution","title":"<code>plot_index_measurement_distribution(features)</code>","text":"<p>Plot a histogram of measurement results at the index</p> <p>Parameters:</p> Name Type Description Default <code>index_episodes</code> <p>Must contain <code>index_hb</code>, <code>index_egfr</code>,</p> required Source code in <code>src\\pyhbr\\analysis\\arc_hbr.py</code> <pre><code>def plot_index_measurement_distribution(features: DataFrame):\n    \"\"\"Plot a histogram of measurement results at the index\n\n    Args:\n        index_episodes: Must contain `index_hb`, `index_egfr`,\n        and `index_platelets`. The index_hb column is multiplied\n        by 10 to get units g/L.\n    \"\"\"\n\n    # Make a plot showing the three lab results as histograms\n    df = features.copy()\n    df[\"index_hb\"] = 10 * df[\"index_hb\"]  # Convert from g/dL to g/L\n    df = (\n        df.filter(regex=\"^index_(egfr|hb|platelets)\")\n        .rename(\n            columns={\n                \"index_egfr\": \"eGFR (mL/min)\",\n                \"index_hb\": \"Hb (g/L)\",\n                \"index_platelets\": \"Plt (x10^9/L)\",\n            }\n        )\n        .melt(value_name=\"Test result at index episode\", var_name=\"Test\")\n    )\n    g = sns.displot(\n        df,\n        x=\"Test result at index episode\",\n        hue=\"Test\",\n    )\n    g.figure.subplots_adjust(top=0.95)\n    g.ax.set_title(\"Distribution of Laboratory Test Results in ACS/PCI index events\")\n</code></pre>"},{"location":"reference/#pyhbr.analysis.calibration","title":"<code>calibration</code>","text":"<p>Calibration plots</p> <p>A calibration plot is a comparison of the proportion p of events that occur in the subset of those with predicted probability p'. Ideally, p = p' meaning that of the cases predicted to occur with probability p', p of them do occur. Calibration is presented as a plot of p against 'p'.</p> <p>The stability of the calibration can be investigated, by plotting p against p' for multiple bootstrapped models (see stability.py).</p>"},{"location":"reference/#pyhbr.analysis.calibration.get_average_calibration_error","title":"<code>get_average_calibration_error(probs, y_test, n_bins)</code>","text":"<p>This is the weighted average discrepancy between the predicted risk and the observed proportions on the calibration curve.</p> <p>See \"https://towardsdatascience.com/expected-calibration-error-ece-a-step- by-step-visual-explanation-with-python-code-c3e9aa12937d\" for a good explanation.</p> <p>The formula for estimated calibration error (ece) is:</p> <p>ece = Sum over bins [samples_in_bin / N] * | P_observed - P_pred |,</p> <p>where P_observed is the empirical proportion of positive samples in the bin, and P_pred is the predicted probability for that bin. The results are weighted by the number of samples in the bin (because some probabilities are predicted more frequently than others).</p> <p>The result is interpreted as an absolute error: i.e. a value of 0.1 means that the calibration is out on average by 10%. It may be better to modify the formula to compute an average relative error.</p> <p>Testing: not yet tested.</p> Source code in <code>src\\pyhbr\\analysis\\calibration.py</code> <pre><code>def get_average_calibration_error(probs, y_test, n_bins):\n    \"\"\"\n    This is the weighted average discrepancy between the predicted risk and the\n    observed proportions on the calibration curve.\n\n    See \"https://towardsdatascience.com/expected-calibration-error-ece-a-step-\n    by-step-visual-explanation-with-python-code-c3e9aa12937d\" for a good\n    explanation.\n\n    The formula for estimated calibration error (ece) is:\n\n       ece = Sum over bins [samples_in_bin / N] * | P_observed - P_pred |,\n\n    where P_observed is the empirical proportion of positive samples in the\n    bin, and P_pred is the predicted probability for that bin. The results are\n    weighted by the number of samples in the bin (because some probabilities are\n    predicted more frequently than others).\n\n    The result is interpreted as an absolute error: i.e. a value of 0.1 means\n    that the calibration is out on average by 10%. It may be better to modify the\n    formula to compute an average relative error.\n\n    Testing: not yet tested.\n    \"\"\"\n\n    # There is one estimated calibration error for each model (the model under\n    # test and all the bootstrap models). These will be averaged at the end\n    estimated_calibration_errors = []\n\n    # The total number of samples is the number of rows in the probs array. This\n    # is used with the number of samples in the bins to weight the probability\n    # error\n    N = probs.shape[0]\n\n    bin_edges = np.linspace(0, 1, n_bins + 1)\n    for n in range(probs.shape[1]):\n\n        prob_true, prob_pred = calibration_curve(y_test, probs[:, n], n_bins=n_bins)\n\n        # For each prob_pred, need to count the number of samples in that lie in\n        # the bin centered at prob_pred.\n        bin_width = 1 / n_bins\n        count_in_bins = []\n        for prob in prob_pred:\n            bin_start = prob - bin_width / 2\n            bin_end = prob + bin_width / 2\n            count = ((bin_start &lt;= probs[:, n]) &amp; (probs[:, n] &lt; bin_end)).sum()\n            count_in_bins.append(count)\n        count_in_bins = np.array(count_in_bins)\n\n        error = np.sum(count_in_bins * np.abs(prob_true - prob_pred)) / N\n        estimated_calibration_errors.append(error)\n\n    return np.mean(estimated_calibration_errors)\n</code></pre>"},{"location":"reference/#pyhbr.analysis.calibration.get_calibration","title":"<code>get_calibration(probs, y_test, n_bins)</code>","text":"<p>Calculate the calibration of the fitted models</p> <p>Get the calibration curves for all models (whose probability predictions for the positive class are columns of probs) based on the outcomes in y_test. Rows of y_test correspond to rows of probs. The result is a list of pairs, one for each model (column of probs). Each pair contains the vector of x- and y-coordinates of the calibration curve.</p> <p>Parameters:</p> Name Type Description Default <code>probs</code> <code>DataFrame</code> <p>The dataframe of probabilities predicted by the model. The first column is the model-under-test (fitted on the training data) and the other columns are from the fits on the training data resamples.</p> required <code>y_test</code> <code>Series</code> <p>The outcomes corresponding to the predicted probabilities.</p> required <code>n_bins</code> <code>int</code> <p>The number of bins to group probability predictions into, for the purpose of averaging the observed frequency of outcome in the test set.</p> required <p>Returns:</p> Type Description <code>list[DataFrame]</code> <p>A list of DataFrames containing the calibration curves. Each DataFrame contains the columns <code>predicted</code> and <code>observed</code>.</p> Source code in <code>src\\pyhbr\\analysis\\calibration.py</code> <pre><code>def get_calibration(probs: DataFrame, y_test: Series, n_bins: int) -&gt; list[DataFrame]:\n    \"\"\"Calculate the calibration of the fitted models\n\n    Get the calibration curves for all models (whose probability\n    predictions for the positive class are columns of probs) based\n    on the outcomes in y_test. Rows of y_test correspond to rows of\n    probs. The result is a list of pairs, one for each model (column\n    of probs). Each pair contains the vector of x- and y-coordinates\n    of the calibration curve.\n\n    Args:\n        probs: The dataframe of probabilities predicted by the model.\n            The first column is the model-under-test (fitted on the training\n            data) and the other columns are from the fits on the training\n            data resamples.\n        y_test: The outcomes corresponding to the predicted probabilities.\n        n_bins: The number of bins to group probability predictions into, for\n            the purpose of averaging the observed frequency of outcome in the\n            test set.\n\n    Returns:\n        A list of DataFrames containing the calibration curves. Each DataFrame\n            contains the columns `predicted` and `observed`.\n\n    \"\"\"\n    curves = []\n    for column in probs.columns:\n        prob_true, prob_pred = calibration_curve(y_test, probs[column], n_bins=n_bins)\n        df = DataFrame({\"predicted\": prob_pred, \"observed\": prob_true})\n        curves.append(df)\n    return curves\n</code></pre>"},{"location":"reference/#pyhbr.analysis.calibration.plot_calibration_curves","title":"<code>plot_calibration_curves(ax, curves, colour, name, title='Calibration-stability curves')</code>","text":"<p>Plot calibration curves for the model under test and resampled models</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>The axes on which to plot the calibration curves</p> required <code>curves</code> <code>list[DataFrame]</code> <p>A list of DataFrames containing <code>predicted</code> and <code>observed</code> columns. The first DataFrame corresponds to the model under test</p> required <code>title</code> <p>Title to add to the plot.</p> <code>'Calibration-stability curves'</code> Source code in <code>src\\pyhbr\\analysis\\calibration.py</code> <pre><code>def plot_calibration_curves(\n    ax: Axes,\n    curves: list[DataFrame],\n    colour: str,\n    name: str,\n    title=\"Calibration-stability curves\",\n):\n    \"\"\"Plot calibration curves for the model under test and resampled models\n\n    Args:\n        ax: The axes on which to plot the calibration curves\n        curves: A list of DataFrames containing `predicted` and `observed`\n            columns. The first DataFrame corresponds to the model under test\n        title: Title to add to the plot.\n    \"\"\"\n    mut_curve = curves[0]  # model-under-test\n    ax.plot(mut_curve[\"predicted\"], mut_curve[\"observed\"], label=name, c=colour)\n    for curve in curves[1:]:\n        ax.plot(\n            curve[\"predicted\"],\n            curve[\"observed\"],\n            label=\"Resample\",\n            c=colour,\n            linewidth=0.3,\n            alpha=0.4,\n        )\n    # ax.axline([0, 0], [1, 1], color=\"k\", linestyle=\"--\")\n    # ax.legend([\"Model-under-test\", \"Bootstrapped models\"])\n    ax.set_title(title)\n    ax.set_xlabel(\"Predicted probability of bleeding from model\")\n    ax.set_ylabel(\"Observed bleeding rate corresponding to risk prediction\")\n</code></pre>"},{"location":"reference/#pyhbr.analysis.calibration.plot_prediction_distribution","title":"<code>plot_prediction_distribution(ax, probs, n_bins)</code>","text":"<p>Plot the distribution of predicted probabilities over the models as a bar chart, with error bars showing the standard deviation of each model height. All model predictions (columns of probs) are given equal weight in the average; column 0 (the model under test) is not singled out in any way.</p> <p>The function plots vertical error bars that are one standard deviation up and down (so 2*sd in total)</p> Source code in <code>src\\pyhbr\\analysis\\calibration.py</code> <pre><code>def plot_prediction_distribution(ax, probs, n_bins):\n    \"\"\"\n    Plot the distribution of predicted probabilities over the models as\n    a bar chart, with error bars showing the standard deviation of each\n    model height. All model predictions (columns of probs) are given equal\n    weight in the average; column 0 (the model under test) is not singled\n    out in any way.\n\n    The function plots vertical error bars that are one standard deviation\n    up and down (so 2*sd in total)\n    \"\"\"\n    bin_edges = np.linspace(0, 1, n_bins + 1)\n    freqs = []\n    for j in range(probs.shape[1]):\n        f, _ = np.histogram(probs[:, j], bins=bin_edges)\n        freqs.append(f)\n    means = np.mean(freqs, axis=0)\n    sds = np.std(freqs, axis=0)\n\n    bin_centers = (bin_edges[1:] + bin_edges[:-1]) / 2\n\n    # Compute the bin width to leave a gap between bars\n    # of 20%\n    bin_width = 0.80 / n_bins\n\n    ax.bar(bin_centers, height=means, width=bin_width, yerr=2 * sds)\n    # ax.set_title(\"Distribution of predicted probabilities\")\n    ax.set_xlabel(\"Mean predicted probability\")\n    ax.set_ylabel(\"Count\")\n</code></pre>"},{"location":"reference/#pyhbr.analysis.dim_reduce","title":"<code>dim_reduce</code>","text":"<p>Functions for dimension-reduction of clinical codes</p>"},{"location":"reference/#pyhbr.analysis.dim_reduce.Dataset","title":"<code>Dataset</code>  <code>dataclass</code>","text":"<p>Stores either the train or test set</p> Source code in <code>src\\pyhbr\\analysis\\dim_reduce.py</code> <pre><code>@dataclass\nclass Dataset:\n    \"\"\"Stores either the train or test set\"\"\"\n\n    y: DataFrame\n    X_manual: DataFrame\n    X_reduce: DataFrame\n</code></pre>"},{"location":"reference/#pyhbr.analysis.dim_reduce.make_full_pipeline","title":"<code>make_full_pipeline(model, reducer=None)</code>","text":"<p>Make a model pipeline from the model part and dimension reduction</p> <p>This pipeline has one or two steps:</p> <ul> <li>If no reduction is performed, the only step is \"model\"</li> <li>If dimension reduction is performed, the steps are \"reducer\", \"model\"</li> </ul> <p>This function can be used to make the pipeline with no dimension (pass None to reducer). Otherwise, pass the reducer which will reduce a subset of the columns before fitting the model (use make_column_transformer to create this argument).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Pipeline</code> <p>A list of model fitting steps that should be applied after the (optional) dimension reduction.</p> required <code>reducer</code> <code>Pipeline</code> <p>If non-None, this reduction pipeline is applied before the model to reduce a subset of the columns.</p> <code>None</code> <p>Returns:</p> Type Description <code>Pipeline</code> <p>A scikit-learn pipeline that can be fitted to training data.</p> Source code in <code>src\\pyhbr\\analysis\\dim_reduce.py</code> <pre><code>def make_full_pipeline(model: Pipeline, reducer: Pipeline = None) -&gt; Pipeline:\n    \"\"\"Make a model pipeline from the model part and dimension reduction\n\n    This pipeline has one or two steps:\n\n    * If no reduction is performed, the only step is \"model\"\n    * If dimension reduction is performed, the steps are \"reducer\", \"model\"\n\n    This function can be used to make the pipeline with no dimension\n    (pass None to reducer). Otherwise, pass the reducer which will reduce\n    a subset of the columns before fitting the model (use make_column_transformer\n    to create this argument).\n\n    Args:\n        model: A list of model fitting steps that should be applied\n            after the (optional) dimension reduction.\n        reducer: If non-None, this reduction pipeline is applied before\n            the model to reduce a subset of the columns.\n\n    Returns:\n        A scikit-learn pipeline that can be fitted to training data.\n    \"\"\"\n    if reducer is not None:\n        return Pipeline([(\"reducer\", reducer), (\"model\", model)])\n    else:\n        return Pipeline([(\"model\", model)])\n</code></pre>"},{"location":"reference/#pyhbr.analysis.dim_reduce.make_grad_boost","title":"<code>make_grad_boost(random_state)</code>","text":"<p>Make a new gradient boosting classifier</p> <p>Returns:</p> Type Description <code>Pipeline</code> <p>The unfitted pipeline for the gradient boosting classifier</p> Source code in <code>src\\pyhbr\\analysis\\dim_reduce.py</code> <pre><code>def make_grad_boost(random_state: RandomState) -&gt; Pipeline:\n    \"\"\"Make a new gradient boosting classifier\n\n    Returns:\n        The unfitted pipeline for the gradient boosting classifier\n    \"\"\"\n    random_forest = GradientBoostingClassifier(\n        n_estimators=100, max_depth=10, random_state=random_state\n    )\n    return Pipeline([(\"model\", random_forest)])\n</code></pre>"},{"location":"reference/#pyhbr.analysis.dim_reduce.make_logistic_regression","title":"<code>make_logistic_regression(random_state)</code>","text":"<p>Make a new logistic regression model</p> <p>The model involves scaling all predictors and then applying a logistic regression model.</p> <p>Returns:</p> Type Description <code>Pipeline</code> <p>The unfitted pipeline for the logistic regression model</p> Source code in <code>src\\pyhbr\\analysis\\dim_reduce.py</code> <pre><code>def make_logistic_regression(random_state: RandomState) -&gt; Pipeline:\n    \"\"\"Make a new logistic regression model\n\n    The model involves scaling all predictors and then\n    applying a logistic regression model.\n\n    Returns:\n        The unfitted pipeline for the logistic regression model\n    \"\"\"\n\n    scaler = StandardScaler()\n    logreg = LogisticRegression(random_state=random_state)\n    return Pipeline([(\"scaler\", scaler), (\"model\", logreg)])\n</code></pre>"},{"location":"reference/#pyhbr.analysis.dim_reduce.make_random_forest","title":"<code>make_random_forest(random_state)</code>","text":"<p>Make a new random forest model</p> <p>Returns:</p> Type Description <code>Pipeline</code> <p>The unfitted pipeline for the random forest model</p> Source code in <code>src\\pyhbr\\analysis\\dim_reduce.py</code> <pre><code>def make_random_forest(random_state: RandomState) -&gt; Pipeline:\n    \"\"\"Make a new random forest model\n\n    Returns:\n        The unfitted pipeline for the random forest model\n    \"\"\"\n    random_forest = RandomForestClassifier(\n        n_estimators=100, max_depth=10, random_state=random_state\n    )\n    return Pipeline([(\"model\", random_forest)])\n</code></pre>"},{"location":"reference/#pyhbr.analysis.dim_reduce.make_reducer_pipeline","title":"<code>make_reducer_pipeline(reducer, cols_to_reduce)</code>","text":"<p>Make a wrapper that applies dimension reduction to a subset of columns.</p> <p>A column transformer is necessary if only some of the columns should be dimension-reduced, and others should be preserved. The resulting pipeline is intended for use in a scikit-learn pipeline taking a pandas DataFrame as input (where a subset of the columns are cols_to_reduce).</p> <p>Parameters:</p> Name Type Description Default <code>reducer</code> <p>The dimension reduction model to use for reduction</p> required <code>cols_to_reduce</code> <code>list[str]</code> <p>The list of column names to reduce</p> required <p>Returns:</p> Type Description <code>Pipeline</code> <p>A pipeline which contains the column_transformer that applies the reducer to cols_to_reduce. This can be included as a step in a larger pipeline.</p> Source code in <code>src\\pyhbr\\analysis\\dim_reduce.py</code> <pre><code>def make_reducer_pipeline(reducer, cols_to_reduce: list[str]) -&gt; Pipeline:\n    \"\"\"Make a wrapper that applies dimension reduction to a subset of columns.\n\n    A column transformer is necessary if only some of the columns should be\n    dimension-reduced, and others should be preserved. The resulting pipeline\n    is intended for use in a scikit-learn pipeline taking a pandas DataFrame as\n    input (where a subset of the columns are cols_to_reduce).\n\n    Args:\n        reducer: The dimension reduction model to use for reduction\n        cols_to_reduce: The list of column names to reduce\n\n    Returns:\n        A pipeline which contains the column_transformer that applies the\n            reducer to cols_to_reduce. This can be included as a step in a\n            larger pipeline.\n    \"\"\"\n    column_transformer = ColumnTransformer(\n        [(\"reducer\", reducer, cols_to_reduce)],\n        remainder=\"passthrough\",\n        verbose_feature_names_out=True,\n    )\n    return Pipeline([(\"column_transformer\", column_transformer)])\n</code></pre>"},{"location":"reference/#pyhbr.analysis.dim_reduce.prepare_train_test","title":"<code>prepare_train_test(data_manual, data_reduce, random_state)</code>","text":"<p>Make the test/train datasets for manually-chosen groups and high-dimensional data</p> <p>Parameters:</p> Name Type Description Default <code>data_manual</code> <code>DataFrame</code> <p>The dataset with manually-chosen code groups</p> required <code>data_reduce</code> <code>DataFrame</code> <p>The high-dimensional dataset</p> required <code>random_state</code> <code>RandomState</code> <p>The random state to pick the test/train split</p> required <p>Returns:</p> Type Description <code>(Dataset, Dataset)</code> <p>A tuple (train, test) containing the datasets to be used for training and testing the models. Both contain the outcome y along with the features for both the manually-chosen code groups and the data for dimension reduction.</p> Source code in <code>src\\pyhbr\\analysis\\dim_reduce.py</code> <pre><code>def prepare_train_test(\n    data_manual: DataFrame, data_reduce: DataFrame, random_state: RandomState\n) -&gt; (Dataset, Dataset):\n    \"\"\"Make the test/train datasets for manually-chosen groups and high-dimensional data\n\n    Args:\n        data_manual: The dataset with manually-chosen code groups\n        data_reduce: The high-dimensional dataset\n        random_state: The random state to pick the test/train split\n\n    Returns:\n        A tuple (train, test) containing the datasets to be used for training and\n            testing the models. Both contain the outcome y along with the features\n            for both the manually-chosen code groups and the data for dimension\n            reduction.\n    \"\"\"\n\n    # Check number of rows match\n    if data_manual.shape[0] != data_reduce.shape[0]:\n        raise RuntimeError(\n            \"The number of rows in data_manual and data_reduce do not match.\"\n        )\n\n    test_set_proportion = 0.25\n\n    # First, get the outcomes (y) from the dataframe. This is the\n    # source of test/train outcome data, and is used for both the\n    # manual and UMAP models. Just interested in whether bleeding\n    # occurred (not number of occurrences) for this experiment\n    outcome_name = \"bleeding_al_ani_outcome\"\n    y = data_manual[outcome_name]\n\n    # Get the set of manual code predictors (X0) to use for the\n    # first logistic regression model (all the columns with names\n    # ending in \"_before\").\n    X_manual = data_manual.drop(columns=[outcome_name])\n\n    # Make a random test/train split.=\n    X_train_manual, X_test_manual, y_train, y_test = train_test_split(\n        X_manual, y, test_size=test_set_proportion, random_state=random_state\n    )\n\n    # Extract the test/train sets from the UMAP data based on\n    # the index of the training set for the manual codes\n    X_reduce = data_reduce.drop(columns=[outcome_name])\n    X_train_reduce = X_reduce.loc[X_train_manual.index]\n    X_test_reduce = X_reduce.loc[X_test_manual.index]\n\n    # Store the test/train data together\n    train = Dataset(y_train, X_train_manual, X_train_reduce)\n    test = Dataset(y_test, X_test_manual, X_test_reduce)\n\n    return train, test\n</code></pre>"},{"location":"reference/#pyhbr.analysis.patient_viewer","title":"<code>patient_viewer</code>","text":""},{"location":"reference/#pyhbr.analysis.patient_viewer.get_patient_history","title":"<code>get_patient_history(patient_id, hic_data)</code>","text":"<p>Get a list of all this patient's episode data</p> <p>Parameters:</p> Name Type Description Default <code>patient_id</code> <code>str</code> <p>Which patient to fetch</p> required <code>hic_data</code> <code>HicData</code> <p>Contains <code>episodes</code> and <code>codes</code> tables</p> required <p>Returns:</p> Type Description <p>A table indexed by spell_id, episode_id, type (of clinical code) and clinical code position.</p> Source code in <code>src\\pyhbr\\analysis\\patient_viewer.py</code> <pre><code>def get_patient_history(patient_id: str, hic_data: HicData):\n    \"\"\"Get a list of all this patient's episode data\n\n    Args:\n        patient_id: Which patient to fetch\n        hic_data: Contains `episodes` and `codes` tables\n\n    Returns:\n        A table indexed by spell_id, episode_id, type (of clinical code)\n            and clinical code position.\n    \"\"\"\n    df = hic_data.codes.merge(\n        hic_data.episodes[[\"patient_id\", \"spell_id\", \"episode_start\"]],\n        how=\"left\",\n        on=\"episode_id\",\n    )\n    this_patient = (\n        df[df[\"patient_id\"] == patient_id]\n        .sort_values([\"episode_start\", \"type\",\"position\"])\n        .drop(columns=\"group\")\n        .set_index([\"spell_id\", \"episode_id\", \"type\", \"position\"])\n    ).drop_duplicates()\n    return this_patient\n</code></pre>"},{"location":"reference/#pyhbr.analysis.roc","title":"<code>roc</code>","text":"<p>ROC Curves</p> <p>The file calculates the ROC curves of the bootstrapped models (for assessing ROC curve stability; see stability.py).</p>"},{"location":"reference/#pyhbr.analysis.roc.AucData","title":"<code>AucData</code>  <code>dataclass</code>","text":"Source code in <code>src\\pyhbr\\analysis\\roc.py</code> <pre><code>@dataclass\nclass AucData:\n    model_under_test_auc: float\n    resample_auc: list[float]\n\n    def mean_resample_auc(self) -&gt; float:\n        \"\"\"Get the mean of the resampled AUCs\n        \"\"\"\n        return np.mean(self.resample_auc)\n\n    def std_dev_resample_auc(self) -&gt; float:\n        \"\"\"Get the standard deviation of the resampled AUCs\n        \"\"\"\n        return np.mean(self.resample_auc)\n</code></pre>"},{"location":"reference/#pyhbr.analysis.roc.AucData.mean_resample_auc","title":"<code>mean_resample_auc()</code>","text":"<p>Get the mean of the resampled AUCs</p> Source code in <code>src\\pyhbr\\analysis\\roc.py</code> <pre><code>def mean_resample_auc(self) -&gt; float:\n    \"\"\"Get the mean of the resampled AUCs\n    \"\"\"\n    return np.mean(self.resample_auc)\n</code></pre>"},{"location":"reference/#pyhbr.analysis.roc.AucData.std_dev_resample_auc","title":"<code>std_dev_resample_auc()</code>","text":"<p>Get the standard deviation of the resampled AUCs</p> Source code in <code>src\\pyhbr\\analysis\\roc.py</code> <pre><code>def std_dev_resample_auc(self) -&gt; float:\n    \"\"\"Get the standard deviation of the resampled AUCs\n    \"\"\"\n    return np.mean(self.resample_auc)\n</code></pre>"},{"location":"reference/#pyhbr.analysis.roc.get_auc","title":"<code>get_auc(probs, y_test)</code>","text":"<p>Get the area under the ROC curves for the fitted models</p> <p>Compute area under the ROC curve (AUC) for the model-under-test (the first column of probs), and the other bootstrapped models (other columns of probs).</p> Source code in <code>src\\pyhbr\\analysis\\roc.py</code> <pre><code>def get_auc(probs: DataFrame, y_test: Series) -&gt; AucData:\n    \"\"\"Get the area under the ROC curves for the fitted models\n\n    Compute area under the ROC curve (AUC) for the model-under-test\n    (the first column of probs), and the other bootstrapped models\n    (other columns of probs).\n\n    \"\"\"\n    model_under_test_auc = roc_auc_score(y_test, probs.iloc[:,0]) # Model-under test\n    resample_auc = []\n    for column in probs:\n        resample_auc.append(roc_auc_score(y_test, probs[column]))\n    return AucData(model_under_test_auc, resample_auc)\n</code></pre>"},{"location":"reference/#pyhbr.analysis.roc.get_roc_curves","title":"<code>get_roc_curves(probs, y_test)</code>","text":"<p>Get the ROC curves for the fitted models</p> <p>Get the ROC curves for all models (whose probability predictions for the positive class are columns of probs) based on the outcomes in y_test. Rows of y_test correspond to rows of probs. The result is a list of pairs, one for each model (column of probs). Each pair contains the vector of x- and y-coordinates of the ROC curve.</p> <p>Parameters:</p> Name Type Description Default <code>probs</code> <code>DataFrame</code> <p>The probabilities predicted by all the fitted models. The first column is the model-under-test (the training set), and the other columns are resamples of the training set.</p> required <code>y_test</code> <code>Series</code> <p>The outcome data corresponding to each row of probs.</p> required <p>Returns:</p> Type Description <code>list[DataFrame]</code> <p>A list of DataFrames, each of which contains one ROC curve, corresponding to the columns in probs. The columns of the DataFrames are <code>fpr</code> (false positive rate) and <code>tpr</code> (true positive rate)</p> Source code in <code>src\\pyhbr\\analysis\\roc.py</code> <pre><code>def get_roc_curves(probs: DataFrame, y_test: Series) -&gt; list[DataFrame]:\n    \"\"\"Get the ROC curves for the fitted models\n\n    Get the ROC curves for all models (whose probability\n    predictions for the positive class are columns of probs) based\n    on the outcomes in y_test. Rows of y_test correspond to rows of\n    probs. The result is a list of pairs, one for each model (column\n    of probs). Each pair contains the vector of x- and y-coordinates\n    of the ROC curve.\n\n    Args:\n        probs: The probabilities predicted by all the fitted models.\n            The first column is the model-under-test (the training set),\n            and the other columns are resamples of the training set.\n        y_test: The outcome data corresponding to each row of probs.\n\n    Returns:\n        A list of DataFrames, each of which contains one ROC curve,\n            corresponding to the columns in probs. The columns of the\n            DataFrames are `fpr` (false positive rate) and `tpr` (true\n            positive rate)\n    \"\"\"\n    curves = []\n    for n in range(probs.shape[1]):\n        fpr, tpr, _ = roc_curve(y_test, probs.iloc[:, n])\n        curves.append((fpr, tpr))\n    return curves\n</code></pre>"},{"location":"reference/#pyhbr.analysis.roc.plot_roc_curves","title":"<code>plot_roc_curves(ax, curves, auc, title='ROC-stability Curves')</code>","text":"<p>Plot ROC curves of the model-under-test and resampled models</p> <p>Plot the set of bootstrapped ROC curves (an instability plot), using the data in curves (a list of curves to plot). Assume that the first curve is the model-under-test (which is coloured differently).</p> <p>The auc argument is an array where the first element is the AUC of the model under test, and the second element is the mean AUC of the bootstrapped models, and the third element is the standard deviation of the AUC of the bootstrapped models (these latter two measure stability). This argument is the output from get_bootstrapped_auc.</p> Source code in <code>src\\pyhbr\\analysis\\roc.py</code> <pre><code>def plot_roc_curves(ax, curves, auc, title = \"ROC-stability Curves\"):\n    \"\"\"Plot ROC curves of the model-under-test and resampled models\n\n    Plot the set of bootstrapped ROC curves (an instability plot),\n    using the data in curves (a list of curves to plot). Assume that the\n    first curve is the model-under-test (which is coloured differently).\n\n    The auc argument is an array where the first element is the AUC of the\n    model under test, and the second element is the mean AUC of the\n    bootstrapped models, and the third element is the standard deviation\n    of the AUC of the bootstrapped models (these latter two measure\n    stability). This argument is the output from get_bootstrapped_auc.\n    \"\"\"\n    mut_curve = curves[0]  # model-under-test\n    ax.plot(mut_curve[0], mut_curve[1], color=\"r\")\n    for curve in curves[1:]:\n        ax.plot(curve[0], curve[1], color=\"b\", linewidth=0.3, alpha=0.4)\n    ax.axline([0, 0], [1, 1], color=\"k\", linestyle=\"--\")\n    ax.legend(\n        [\n            f\"Model (AUC = {auc.model_under_test_auc:.2f})\",\n            f\"Bootstrapped models\",\n        ]\n    )\n    ax.set_title(title)\n    ax.set_xlabel(\"False positive rate\")\n    ax.set_ylabel(\"True positive rate\")\n</code></pre>"},{"location":"reference/#pyhbr.analysis.stability","title":"<code>stability</code>","text":"<p>Assessing model stability</p> <p>Model stability of an internally-validated model refers to how well models developed on a similar internal population agree with each other. The methodology for assessing model stability follows Riley and Collins, 2022 (https://arxiv.org/abs/2211.01061)</p> <p>Assessing model stability is an end-to-end test of the entire model development process. Riley and Collins do not refer to a test/train split, but their method will be interpreted as applying to the training set (with instability measures assessed by applying models to the test set). As a result, the first step in the process is to split the internal dataset into a training set P0 and a test set T.</p> <p>Assuming that a training set P0 is used to develop a model M0 using a model development  process D (involving steps such cross-validation and hyperparameter tuning in the training set, and validation of accuracy of model prediction in the test set), the following steps are required to assess the stability of M0:</p> <ol> <li>Bootstrap resample P0 with replacement M &gt;= 200 times, creating    M new datasets Pm that are all the same size as P0</li> <li>Apply D to each Pm, to obtain M new models Mn which are all    comparable with M0.</li> <li>Collect together the predictions from all Mn and compare them    to the predictions from M0 for each sample in the test set T.</li> <li>From the data in 3, plot instability plots such as a scatter    plot of M0 predictions on the x-axis and all the Mn predictions    on the y-axis, for each sample of T. In addition, plot graphs    of how all the model validation metrics vary as a function of    the bootstrapped models Mn.</li> </ol> <p>Implementation</p> <p>A function is required that takes the original training set P0 and generates N bootstrapped resamples Pn that are the same size as P.</p> <p>A function is required that wraps the entire model into one call, taking as input the bootstrapped resample Pn and providing as output the bootstrapped model Mn. This function can then be called M times to generate the bootstrapped models. This function is not defined in this file (see the fit.py file)</p> <p>An aggregating function will then take all the models Mn, the model-under-test M0, and the test set T, and make predictions using all the models for each sample in the test set. It should return all these predictions (probabilities) in a 2D array, where each row corresponds to a test-set sample, column 0 is the probability from M0, and columns 1 through M are the probabilities from each Mn.</p> <p>This 2D array may be used as the basis of instability plots. Paired with information about the true outcomes y_test, this can also be used to plot ROC-curve variability (i.e. plotting the ROC curve for all model M0 and Mn on one graph). Any other accuracy metric of interest can be calculated from this information (i.e. for step 4 above).</p>"},{"location":"reference/#pyhbr.analysis.stability.FittedModel","title":"<code>FittedModel</code>  <code>dataclass</code>","text":"<p>Stores a model fitted to a training set and resamples of the training set.</p> Source code in <code>src\\pyhbr\\analysis\\stability.py</code> <pre><code>@dataclass\nclass FittedModel:\n    \"\"\"Stores a model fitted to a training set and resamples of the training set.\n    \"\"\"\n    M0: Pipeline\n    Mm: list[Pipeline]\n\n    def flatten(self) -&gt; list[Pipeline]:\n        \"\"\"Get a flat list of all the models\n\n        Returns:\n            The list of fitted models, with M0 at the front\n        \"\"\"\n        return [self.M0] + self.Mm    \n</code></pre>"},{"location":"reference/#pyhbr.analysis.stability.FittedModel.flatten","title":"<code>flatten()</code>","text":"<p>Get a flat list of all the models</p> <p>Returns:</p> Type Description <code>list[Pipeline]</code> <p>The list of fitted models, with M0 at the front</p> Source code in <code>src\\pyhbr\\analysis\\stability.py</code> <pre><code>def flatten(self) -&gt; list[Pipeline]:\n    \"\"\"Get a flat list of all the models\n\n    Returns:\n        The list of fitted models, with M0 at the front\n    \"\"\"\n    return [self.M0] + self.Mm    \n</code></pre>"},{"location":"reference/#pyhbr.analysis.stability.Resamples","title":"<code>Resamples</code>  <code>dataclass</code>","text":"<p>Store a training set along with M resamples of it</p> Source code in <code>src\\pyhbr\\analysis\\stability.py</code> <pre><code>@dataclass\nclass Resamples:\n    \"\"\"Store a training set along with M resamples of it\n    \"\"\"\n    X0: DataFrame\n    y0: Series\n    Xm: list[DataFrame]\n    ym: list[Series]\n</code></pre>"},{"location":"reference/#pyhbr.analysis.stability.fit_model","title":"<code>fit_model(model, X0, y0, M, random_state)</code>","text":"<p>Fit a model to a training set and resamples of the training set.</p> <p>Use the unfitted model pipeline returned by model_factory to:</p> <ul> <li>Fit a model to the training set (X0, y0)</li> <li>Fit a model to M resamples (Xm, ym) of the training set</li> </ul> <p>The model is an unfitted scikit-learn Pipeline. Note that if RandomState is used when specifying the model, then the models used to fit the resamples here will be statstical clones (i.e. they might not necessarily produce the same result on the same data). clone() is called on model before fitting, so each fit gets a  new clean object.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Pipeline</code> <p>An unfitted scikit-learn pipeline, which is used as the basis for all the fits. Each fit calls clone() on this object before fitting, to get a new model with clean parameters. The cloned fitted models are then stored in the returned fitted model.</p> required <code>X0</code> <code>DataFrame</code> <p>The training set features</p> required <code>y0</code> <code>Series</code> <p>The training set outcome</p> required <code>M</code> <code>int</code> <p>How many resamples to take from the training set (ideally &gt;= 200)</p> required <code>random_state</code> <code>RandomState</code> <p>The source of randomness for model fitting</p> required <p>Returns:</p> Type Description <code>FittedModel</code> <p>An object containing the model fitted on (X0,y0) and all (Xm,ym)</p> Source code in <code>src\\pyhbr\\analysis\\stability.py</code> <pre><code>def fit_model(model: Pipeline, X0: DataFrame, y0: Series, M: int, random_state: RandomState) -&gt; FittedModel:\n    \"\"\"Fit a model to a training set and resamples of the training set.\n\n    Use the unfitted model pipeline returned by model_factory to:\n\n    * Fit a model to the training set (X0, y0)\n    * Fit a model to M resamples (Xm, ym) of the training set\n\n    The model is an unfitted scikit-learn Pipeline. Note that if RandomState is used\n    when specifying the model, then the models used to fit the resamples here will\n    be _statstical clones_ (i.e. they might not necessarily produce the same result\n    on the same data). clone() is called on model before fitting, so each fit gets a \n    new clean object.\n\n    Args:\n        model: An unfitted scikit-learn pipeline, which is used as the basis for\n            all the fits. Each fit calls clone() on this object before fitting, to\n            get a new model with clean parameters. The cloned fitted models are then\n            stored in the returned fitted model.\n        X0: The training set features\n        y0: The training set outcome\n        M (int): How many resamples to take from the training set (ideally &gt;= 200)\n        random_state: The source of randomness for model fitting\n\n    Returns:\n        An object containing the model fitted on (X0,y0) and all (Xm,ym)\n    \"\"\"\n\n    # Develop a single model from the training set (X0_train, y0_train),\n    # using any method (e.g. including cross validation and hyperparameter\n    # tuning) using training set data. This is referred to as D in\n    # stability.py.\n    print(\"Fitting model-under-test\")\n    pipe = clone(model)\n    M0 = pipe.fit(X0, y0)\n\n    # Resample the training set to obtain the new datasets (Xm, ym)\n    print(f\"Creating {M} bootstrap resamples of training set\")\n    resamples = make_bootstrapped_resamples(X0, y0, M, random_state)\n\n    # Develop all the bootstrap models to compare with the model-under-test M0\n    print(\"Fitting bootstrapped models\")\n    Mm = []\n    for m in range(M):\n        pipe = clone(model)\n        ym = resamples.ym[m]\n        Xm = resamples.Xm[m]\n        Mm.append(pipe.fit(Xm, ym))\n\n    return FittedModel(M0, Mm)\n</code></pre>"},{"location":"reference/#pyhbr.analysis.stability.get_average_instability","title":"<code>get_average_instability(probs)</code>","text":"<p>Instability is the extend to which the bootstrapped models give a different prediction from the model under test. The  average instability is an average of the SMAPE between the prediction of the model-under-test and the predictions of each of the other bootstrap models (i.e. pairing the model-under-test) with a single bootstrapped model gives one SMAPE value, and  these are averaged over all the bootstrap models).</p> <p>SMAPE is preferable to mean relative error, because the latter diverges when the prediction from the model-under-test is very small. It may however be better still to use the log of the accuracy ratio; see https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error, since the probabilities are all positive (or maybe there is a better  thing for comparing probabilities specifically)</p> <p>Testing: not yet tested</p> Source code in <code>src\\pyhbr\\analysis\\stability.py</code> <pre><code>def get_average_instability(probs):\n    \"\"\"\n    Instability is the extend to which the bootstrapped models\n    give a different prediction from the model under test. The \n    average instability is an average of the SMAPE between\n    the prediction of the model-under-test and the predictions of\n    each of the other bootstrap models (i.e. pairing the model-under-test)\n    with a single bootstrapped model gives one SMAPE value, and \n    these are averaged over all the bootstrap models).\n\n    SMAPE is preferable to mean relative error, because the latter\n    diverges when the prediction from the model-under-test is very small.\n    It may however be better still to use the log of the accuracy ratio;\n    see https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error,\n    since the probabilities are all positive (or maybe there is a better \n    thing for comparing probabilities specifically)\n\n    Testing: not yet tested\n    \"\"\"\n    num_rows = probs.shape[0]\n    num_cols = probs.shape[1]\n\n    smape_over_bootstraps = []\n\n    # Loop over each boostrap model\n    for j in range(1, num_cols):\n\n        # Calculate SMAPE between bootstrap model j and\n        # the model-under-test\n        smape_over_bootstraps.append(smape(probs[:,0], probs[:,j]))\n\n    return np.mean(smape_over_bootstraps)\n</code></pre>"},{"location":"reference/#pyhbr.analysis.stability.make_bootstrapped_resamples","title":"<code>make_bootstrapped_resamples(X0, y0, M, random_state)</code>","text":"<p>Make M resamples of the training data</p> <p>Makes M bootstrapped resamples of a training set (X0,y0). M should be at least 200 (as per recommendation).</p> <p>Parameters:</p> Name Type Description Default <code>X0</code> <code>DataFrame</code> <p>The features in the training set to be resampled</p> required <code>y0</code> <code>Series</code> <p>The outcome in the training set to be resampled</p> required <code>M</code> <code>int</code> <p>How many resamples to take</p> required <code>random_state</code> <code>RandomState</code> <p>Source of randomness for resampling</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of rows in X0 and y0 do not match</p> <p>Returns:</p> Type Description <code>Resamples</code> <p>An object containing the original training set and the resamples.</p> Source code in <code>src\\pyhbr\\analysis\\stability.py</code> <pre><code>def make_bootstrapped_resamples(X0: DataFrame, y0: Series, M: int, random_state: RandomState) -&gt; Resamples:\n    \"\"\"Make M resamples of the training data\n\n    Makes M bootstrapped resamples of a training set (X0,y0).\n    M should be at least 200 (as per recommendation).\n\n    Args:\n        X0: The features in the training set to be resampled\n        y0: The outcome in the training set to be resampled\n        M: How many resamples to take\n        random_state: Source of randomness for resampling\n\n    Raises:\n        ValueError: If the number of rows in X0 and y0 do not match\n\n    Returns:\n        An object containing the original training set and the resamples.\n    \"\"\"\n\n    num_samples = X0.shape[0]\n    if num_samples != len(y0):\n        raise ValueError(\"Number of rows in X0_train and y0_train must match\")\n    if M &lt; 200:\n        warnings.warn(\"M should be at least 200; see Riley and Collins, 2022\")\n\n    Xm = []\n    ym = []\n    for _ in range(M):\n        X, y = resample(X0, y0, random_state=random_state)\n        Xm.append(X)\n        ym.append(y)\n\n    return Resamples(X0, y0, Xm, ym)\n</code></pre>"},{"location":"reference/#pyhbr.analysis.stability.plot_instability","title":"<code>plot_instability(ax, probs, y_test, title='Probability stability')</code>","text":"<p>This function plots a scatter graph of one point per value in the test set (row of probs), where the x-axis is the value of the model under test (the first column of probs), and the y-axis is every other probability predicted from the bootstrapped models Mn (the other columns of probs). The predictions from the model-under-test corresponds to the straight line at 45 degrees through the origin</p> <p>For a stable model M0, the scattered points should be close to the M0 line, indicating that the bootstrapped models Mn broadly agree with the predictions made by M0.</p> <p>Testing: not yet tested</p> Source code in <code>src\\pyhbr\\analysis\\stability.py</code> <pre><code>def plot_instability(ax: Axes, probs: DataFrame, y_test: Series, title=\"Probability stability\"):\n    \"\"\"\n    This function plots a scatter graph of one point\n    per value in the test set (row of probs), where the\n    x-axis is the value of the model under test (the\n    first column of probs), and the y-axis is every other\n    probability predicted from the bootstrapped models Mn\n    (the other columns of probs). The predictions from\n    the model-under-test corresponds to the straight line\n    at 45 degrees through the origin\n\n    For a stable model M0, the scattered points should be\n    close to the M0 line, indicating that the bootstrapped\n    models Mn broadly agree with the predictions made by M0.\n\n    Testing: not yet tested\n    \"\"\"\n\n    num_rows = probs.shape[0]\n    num_cols = probs.shape[1]\n    x = []\n    y = []\n    c = []\n    for i in range(num_rows):\n        for j in range(1, num_cols):\n            x.append(probs.iloc[i, 0])  # Model-under-test\n            y.append(probs.iloc[i, j])  # Other bootstrapped models\n            c.append(y_test.iloc[i]),  # What was the actual outcome\n\n    colour_map = {0: \"g\", 1: \"r\"}\n\n    for outcome_to_plot, colour in colour_map.items():\n       x_to_plot = [x for x, outcome in zip(x, c) if outcome == outcome_to_plot]\n       y_to_plot = [y for y, outcome in zip(y, c) if outcome == outcome_to_plot]\n       ax.scatter(x_to_plot, y_to_plot, c=colour, s=1, marker=\".\")\n\n    ax.axline([0, 0], [1, 1])\n\n    # You can restrict the axes here if you want\n    #ax.set_xlim(0, 0.1)\n    #ax.set_ylim(0,0.1)\n\n    ax.legend(\n        [   \n            \"Did not occur (background)\",\n            \"Event occurred (foreground)\",\n        ],\n        markerscale=15\n    )\n    ax.set_title(title)\n    ax.set_xlabel(\"Prediction from model-under-test\")\n    ax.set_ylabel(\"Bootstrap model predictions\")\n</code></pre>"},{"location":"reference/#pyhbr.analysis.stability.predict_probabilities","title":"<code>predict_probabilities(fitted_model, X_test)</code>","text":"<p>Predict outcome probabilities using the fitted models on the test set</p> <p>Aggregating function which finds the predicted probability from the model-under-test M0 and all the bootstrapped models Mn on each sample of the training set features X_test. The result is a 2D numpy array, where each row corresponds to a test-set sample, the first column is the predicted probabilities from M0, and the following N columns are the predictions from all the other Mn.</p> <p>Note: the numbers in the matrix are the probabilities of 1 in the test set y_test.</p> <p>Parameters:</p> Name Type Description Default <code>fitted_model</code> <code>FittedModel</code> <p>The model fitted on the training set and resamples</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>An table of probabilities of the positive outcome in the class, where each column comes from a different model. Column zero  corresponds to the training set, and the other columns are from the resamples. The index for the DataFrame is the same as X_test</p> Source code in <code>src\\pyhbr\\analysis\\stability.py</code> <pre><code>def predict_probabilities(fitted_model: FittedModel, X_test: DataFrame) -&gt; DataFrame:\n    \"\"\"Predict outcome probabilities using the fitted models on the test set\n\n    Aggregating function which finds the predicted probability\n    from the model-under-test M0 and all the bootstrapped models\n    Mn on each sample of the training set features X_test. The\n    result is a 2D numpy array, where each row corresponds to\n    a test-set sample, the first column is the predicted probabilities\n    from M0, and the following N columns are the predictions from all\n    the other Mn.\n\n    Note: the numbers in the matrix are the probabilities of 1 in the\n    test set y_test.\n\n    Args:\n        fitted_model: The model fitted on the training set and resamples\n\n    Returns:\n        An table of probabilities of the positive outcome in the class,\n            where each column comes from a different model. Column zero \n            corresponds to the training set, and the other columns are\n            from the resamples. The index for the DataFrame is the same\n            as X_test\n    \"\"\"\n    columns = []\n    for m, M in enumerate(fitted_model.flatten()):\n        print(f\"Predicting test-set probabilities {m}\")\n        columns.append(M.predict_proba(X_test)[:, 1])\n\n    raw_probs = np.column_stack(columns)\n\n    df = DataFrame(raw_probs)\n    df.columns = [f\"prob_M{m}\" for m in range(len(fitted_model.Mm) + 1)]\n    df.index = X_test.index\n    return df\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes","title":"<code>clinical_codes</code>","text":"<p>Contains utilities for clinical code groups</p>"},{"location":"reference/#pyhbr.clinical_codes.Category","title":"<code>Category</code>  <code>dataclass</code>","text":"<p>Code/categories struct</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the category (e.g. I20) or clinical code (I20.1)</p> <code>docs</code> <code>str</code> <p>The description of the category or code</p> <code>index</code> <code>str | tuple[str, str]</code> <p>Used to sort a list of Categories</p> <code>categories</code> <code>list[Category] | None</code> <p>For a category, the list of sub-categories contained. None for a code.</p> <code>exclude</code> <code>set[str] | None</code> <p>Contains code groups which do not contain any members from this category or any of its sub-categories.</p> Source code in <code>src\\pyhbr\\clinical_codes\\__init__.py</code> <pre><code>@dataclass\nclass Category:\n    \"\"\"Code/categories struct\n\n    Attributes:\n        name: The name of the category (e.g. I20) or clinical code (I20.1)\n        docs: The description of the category or code\n        index: Used to sort a list of Categories\n        categories: For a category, the list of sub-categories contained.\n            None for a code.\n        exclude: Contains code groups which do not contain any members\n            from this category or any of its sub-categories.\n\n    \"\"\"\n\n    name: str\n    docs: str\n    index: str | tuple[str, str]\n    categories: list[Category] | None\n    exclude: set[str] | None\n\n    def is_leaf(self):\n        \"\"\"Check if the categories is a leaf node\n\n        Returns:\n            True if leaf node (i.e. clinical code), false otherwise\n        \"\"\"\n        return self.categories is None\n\n    def excludes(self, group: str) -&gt; bool:\n        \"\"\"Check if this category excludes a code group\n\n        Args:\n            group: The string name of the group to check\n\n        Returns:\n            True if the group is excluded; False otherwise\n        \"\"\"\n        if self.exclude is not None:\n            return group in self.exclude\n        else:\n            return False\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes.Category.excludes","title":"<code>excludes(group)</code>","text":"<p>Check if this category excludes a code group</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>str</code> <p>The string name of the group to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the group is excluded; False otherwise</p> Source code in <code>src\\pyhbr\\clinical_codes\\__init__.py</code> <pre><code>def excludes(self, group: str) -&gt; bool:\n    \"\"\"Check if this category excludes a code group\n\n    Args:\n        group: The string name of the group to check\n\n    Returns:\n        True if the group is excluded; False otherwise\n    \"\"\"\n    if self.exclude is not None:\n        return group in self.exclude\n    else:\n        return False\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes.Category.is_leaf","title":"<code>is_leaf()</code>","text":"<p>Check if the categories is a leaf node</p> <p>Returns:</p> Type Description <p>True if leaf node (i.e. clinical code), false otherwise</p> Source code in <code>src\\pyhbr\\clinical_codes\\__init__.py</code> <pre><code>def is_leaf(self):\n    \"\"\"Check if the categories is a leaf node\n\n    Returns:\n        True if leaf node (i.e. clinical code), false otherwise\n    \"\"\"\n    return self.categories is None\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes.ClinicalCode","title":"<code>ClinicalCode</code>  <code>dataclass</code>","text":"<p>Store a clinical code together with its description.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The code itself, e.g. \"I21.0\"</p> <code>docs</code> <code>str</code> <p>The code description, e.g. \"Acute transmural myocardial infarction of anterior wall\"</p> Source code in <code>src\\pyhbr\\clinical_codes\\__init__.py</code> <pre><code>@dataclass\nclass ClinicalCode:\n    \"\"\"Store a clinical code together with its description.\n\n    Attributes:\n        name: The code itself, e.g. \"I21.0\"\n        docs: The code description, e.g. \"Acute\n            transmural myocardial infarction of anterior wall\"\n    \"\"\"\n\n    name: str\n    docs: str\n\n    def normalise(self):\n        \"\"\"Return the name without whitespace/dots, as lowercase\n\n        See the documentation for [normalize_code()][pyhbr.clinical_codes.normalise_code].\n\n        Returns:\n            The normalized form of this clinical code\n        \"\"\"\n        return normalise_code(self.name)\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes.ClinicalCode.normalise","title":"<code>normalise()</code>","text":"<p>Return the name without whitespace/dots, as lowercase</p> <p>See the documentation for normalize_code().</p> <p>Returns:</p> Type Description <p>The normalized form of this clinical code</p> Source code in <code>src\\pyhbr\\clinical_codes\\__init__.py</code> <pre><code>def normalise(self):\n    \"\"\"Return the name without whitespace/dots, as lowercase\n\n    See the documentation for [normalize_code()][pyhbr.clinical_codes.normalise_code].\n\n    Returns:\n        The normalized form of this clinical code\n    \"\"\"\n    return normalise_code(self.name)\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes.ClinicalCodeTree","title":"<code>ClinicalCodeTree</code>  <code>dataclass</code>","text":"<p>Code definition file structure</p> Source code in <code>src\\pyhbr\\clinical_codes\\__init__.py</code> <pre><code>@serde\n@dataclass\nclass ClinicalCodeTree:\n    \"\"\"Code definition file structure\"\"\"\n\n    categories: list[Category]\n    groups: set[str]\n\n    def codes_in_group(self, group: str) -&gt; list[ClinicalCode]:\n        \"\"\"Get the clinical codes in a code group\n\n        Args:\n            group: The group to fetch\n\n        Raises:\n            ValueError: Raised if the requested group does not exist\n\n        Returns:\n            The list of code groups\n        \"\"\"\n        if not group in self.groups:\n            raise ValueError(f\"'{group}' is not a valid code group ({self.groups})\")\n\n        return get_codes_in_group(group, self.categories)\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes.ClinicalCodeTree.codes_in_group","title":"<code>codes_in_group(group)</code>","text":"<p>Get the clinical codes in a code group</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>str</code> <p>The group to fetch</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Raised if the requested group does not exist</p> <p>Returns:</p> Type Description <code>list[ClinicalCode]</code> <p>The list of code groups</p> Source code in <code>src\\pyhbr\\clinical_codes\\__init__.py</code> <pre><code>def codes_in_group(self, group: str) -&gt; list[ClinicalCode]:\n    \"\"\"Get the clinical codes in a code group\n\n    Args:\n        group: The group to fetch\n\n    Raises:\n        ValueError: Raised if the requested group does not exist\n\n    Returns:\n        The list of code groups\n    \"\"\"\n    if not group in self.groups:\n        raise ValueError(f\"'{group}' is not a valid code group ({self.groups})\")\n\n    return get_codes_in_group(group, self.categories)\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes.codes_in_any_group","title":"<code>codes_in_any_group(codes)</code>","text":"<p>Get a DataFrame of all the codes in any group in a codes file</p> <p>Returns a table with the normalised code (lowercase/no whitespace/no dots) in column <code>code</code>, and the group containing the code in the column <code>group</code>. </p> <p>All codes which are in any group will be included.</p> <p>Codes will be duplicated if they appear in more than one group.</p> <p>Parameters:</p> Name Type Description Default <code>codes</code> <code>ClinicalCodeTree</code> <p>The tree clinical codes (e.g. ICD-10 or OPCS-4, loaded from a file) to search for codes</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: All codes in any group in the codes file</p> Source code in <code>src\\pyhbr\\clinical_codes\\__init__.py</code> <pre><code>def codes_in_any_group(codes: ClinicalCodeTree) -&gt; pd.DataFrame:\n    \"\"\"Get a DataFrame of all the codes in any group in a codes file\n\n    Returns a table with the normalised code (lowercase/no whitespace/no\n    dots) in column `code`, and the group containing the code in the\n    column `group`. \n\n    All codes which are in any group will be included.\n\n    Codes will be duplicated if they appear in more than one group.\n\n    Args:\n        codes: The tree clinical codes (e.g. ICD-10 or OPCS-4, loaded\n            from a file) to search for codes\n\n    Returns:\n        pd.DataFrame: All codes in any group in the codes file\n    \"\"\"\n    dfs = []\n    for g in codes.groups:\n        clinical_codes = codes.codes_in_group(g)\n        normalised_codes = [c.normalise() for c in clinical_codes]\n        docs = [c.docs for c in clinical_codes]\n        df = pd.DataFrame({\"code\": normalised_codes, \"docs\": docs, \"group\": g})\n        dfs.append(df)\n\n    return pd.concat(dfs).reset_index()\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes.get_codes_in_group","title":"<code>get_codes_in_group(group, categories)</code>","text":"<p>Helper function to get clinical codes in a group</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>str</code> <p>The group to fetch</p> required <code>categories</code> <code>list[Category]</code> <p>The list of categories to search for codes</p> required <p>Returns:</p> Type Description <code>list[ClinicalCode]</code> <p>A list of clinical codes in the group</p> Source code in <code>src\\pyhbr\\clinical_codes\\__init__.py</code> <pre><code>def get_codes_in_group(group: str, categories: list[Category]) -&gt; list[ClinicalCode]:\n    \"\"\"Helper function to get clinical codes in a group\n\n    Args:\n        group: The group to fetch\n        categories: The list of categories to search for codes\n\n    Returns:\n        A list of clinical codes in the group\n    \"\"\"\n\n    # Filter out the categories that exclude the group\n    categories_left = [c for c in categories if not c.excludes(group)]\n\n    codes_in_group = []\n\n    # Loop over the remaining categories. For all the leaf\n    # categories, if there is no exclude for this group,\n    # include it in the results. For non-leaf categories,\n    # call this function again and append the resulting codes\n    for category in categories_left:\n        if category.is_leaf() and not category.excludes(group):\n            code = ClinicalCode(name=category.name, docs=category.docs)\n            codes_in_group.append(code)\n        else:\n            sub_categories = category.categories\n            # Check it is non-empty (or refactor logic)\n            new_codes = get_codes_in_group(group, sub_categories)\n            codes_in_group.extend(new_codes)\n\n    return codes_in_group\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes.load_from_package","title":"<code>load_from_package(name)</code>","text":"<p>Load a clinical codes file from the pyhbr package.</p> <p>The clinical codes are stored in yaml format, and this function returns a dictionary corresponding to the structure of the yaml file.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pyhbr.clinical_codes as codes\n&gt;&gt;&gt; tree = codes.load_from_package(\"icd10_test.yaml\")\n&gt;&gt;&gt; group = tree.codes_in_group(\"group_1\")\n&gt;&gt;&gt; [code.name for code in group]\n['I20.0', 'I20.1', 'I20.8', 'I20.9']\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The file name of the codes file to load</p> required <p>Returns:</p> Type Description <code>ClinicalCodeTree</code> <p>The contents of the file.</p> Source code in <code>src\\pyhbr\\clinical_codes\\__init__.py</code> <pre><code>def load_from_package(name: str) -&gt; ClinicalCodeTree:\n    \"\"\"Load a clinical codes file from the pyhbr package.\n\n    The clinical codes are stored in yaml format, and this\n    function returns a dictionary corresponding to the structure\n    of the yaml file.\n\n    Examples:\n        &gt;&gt;&gt; import pyhbr.clinical_codes as codes\n        &gt;&gt;&gt; tree = codes.load_from_package(\"icd10_test.yaml\")\n        &gt;&gt;&gt; group = tree.codes_in_group(\"group_1\")\n        &gt;&gt;&gt; [code.name for code in group]\n        ['I20.0', 'I20.1', 'I20.8', 'I20.9']\n\n    Args:\n        name: The file name of the codes file to load\n\n    Returns:\n        The contents of the file.\n    \"\"\"\n    contents = res_files(\"pyhbr.clinical_codes.files\").joinpath(name).read_text()\n    return from_yaml(ClinicalCodeTree, contents)\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes.normalise_code","title":"<code>normalise_code(code)</code>","text":"<p>Remove whitespace/dots, and convert to lower-case</p> <p>The format of clinical codes can vary across different data sources. A simple way to compare codes is to convert them into a common format and compare them as strings. The purpose of this function is to define the common format, which uses all lower-case letters, does not contain any dots, and does not include any leading/trailing whitespace.</p> <p>Comparing codes for equality does not immediately allow checking whether one code is a sub-category of another. It also ignores clinical code annotations such as dagger/asterisk.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; normalise_code(\"  I21.0 \")\n'i210'\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>The raw code, e.g.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The normalised form of the clinical code</p> Source code in <code>src\\pyhbr\\clinical_codes\\__init__.py</code> <pre><code>def normalise_code(code: str) -&gt; str:\n    \"\"\"Remove whitespace/dots, and convert to lower-case\n\n    The format of clinical codes can vary across different data\n    sources. A simple way to compare codes is to convert them into\n    a common format and compare them as strings. The purpose of\n    this function is to define the common format, which uses all\n    lower-case letters, does not contain any dots, and does not\n    include any leading/trailing whitespace.\n\n    Comparing codes for equality does not immediately allow checking\n    whether one code is a sub-category of another. It also ignores\n    clinical code annotations such as dagger/asterisk.\n\n    Examples:\n        &gt;&gt;&gt; normalise_code(\"  I21.0 \")\n        'i210'\n\n    Args:\n        code: The raw code, e.g.\n\n    Returns:\n        The normalised form of the clinical code\n    \"\"\"\n    return code.lower().strip().replace(\".\", \"\")\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes.codes_editor","title":"<code>codes_editor</code>","text":"<p>Edit groups of ICD-10 and OPCS-4 codes</p>"},{"location":"reference/#pyhbr.clinical_codes.codes_editor.codes_editor","title":"<code>codes_editor</code>","text":""},{"location":"reference/#pyhbr.clinical_codes.codes_editor.codes_editor.run_app","title":"<code>run_app()</code>","text":"<p>Run the main codes editor application</p> Source code in <code>src\\pyhbr\\clinical_codes\\codes_editor\\codes_editor.py</code> <pre><code>def run_app() -&gt; None:\n    \"\"\"Run the main codes editor application\n    \"\"\"\n\n    # You need one (and only one) QApplication instance per application.\n    # Pass in sys.argv to allow command line arguments for your app.\n    # If you know you won't use command line arguments QApplication([]) works too.\n    app = QApplication(sys.argv)\n\n    # Create a Qt widget, which will be our window.\n    window = MainWindow()\n    window.show()\n\n    # Start the event loop.\n    app.exec()\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes.counting","title":"<code>counting</code>","text":"<p>Utilities for counting clinical codes satisfying conditions</p>"},{"location":"reference/#pyhbr.clinical_codes.counting.count_code_groups","title":"<code>count_code_groups(index_episodes, other_episodes, code_groups, any_position)</code>","text":"<p>Count occurrences of prior code group in the previous year</p> <p>Count the total occurrences of the codes in any of code_groups in the other episodes before the index episode specified in previous_year.</p> <p>Note that the index episode itself will not necessarily be excluded from the count (if other_episodes contains the index episode).</p> <p>Parameters:</p> Name Type Description Default <code>index_episodes</code> <code>DataFrame</code> <p>Any DataFrame with <code>episode_id</code> as index</p> required <code>other_episodes</code> <code>DataFrame</code> <p>Table of other episodes (relative to the index). This can be narrowed to either the previous or subsequent year, or a different time frame. (In particular, exclude the index event if required.)</p> required <code>code_groups</code> <code>list[str]</code> <p>List of code group names containing clinical codes that will count towards the sum.</p> required <code>any_position</code> <code>bool</code> <p>If True, count any code in any diagnosis/procedure position. If False, only count a code if it is the primary diagnosis/procedure.</p> required <p>TODO: add another argument for first_episode (bool), to narrow to first episode of spell. Needs spell_id in the all_other_codes table.</p> <p>BUG: This function probably double-counts codes if they appear in two groups. Can be fixed by dropping code duplicates after narrowing by group, and then counting the result of that. Needs some testing.</p> <p>Returns:</p> Type Description <code>Series</code> <p>A series containing the number of code group occurrences in the other_episodes table.</p> Source code in <code>src\\pyhbr\\clinical_codes\\counting.py</code> <pre><code>def count_code_groups(\n    index_episodes: DataFrame,\n    other_episodes: DataFrame,\n    code_groups: list[str],\n    any_position: bool,\n) -&gt; Series:\n    \"\"\"Count occurrences of prior code group in the previous year\n\n    Count the total occurrences of the codes in any of code_groups in\n    the other episodes before the index episode specified in\n    previous_year.\n\n    Note that the index episode itself will not necessarily be\n    excluded from the count (if other_episodes contains the index\n    episode).\n\n    Args:\n        index_episodes: Any DataFrame with `episode_id` as index\n        other_episodes: Table of other episodes (relative to the index).\n            This can be narrowed to either the previous or subsequent\n            year, or a different time frame. (In particular, exclude the\n            index event if required.)\n        code_groups: List of code group names containing clinical codes\n            that will count towards the sum.\n        any_position: If True, count any code in any diagnosis/procedure\n            position. If False, only count a code if it is the primary\n            diagnosis/procedure.\n\n    TODO: add another argument for first_episode (bool), to narrow to\n    first episode of spell. Needs spell_id in the all_other_codes table.\n\n    BUG: This function probably double-counts codes if they appear in\n    two groups. Can be fixed by dropping code duplicates after narrowing\n    by group, and then counting the result of that. Needs some testing.\n\n    Returns:\n        A series containing the number of code group occurrences in the\n            other_episodes table.\n    \"\"\"\n\n    code_group_count = (\n        other_episodes[\n            (any_position | (other_episodes[\"position\"] == 1))\n            &amp; (other_episodes[\"group\"].isin(code_groups))\n        ]\n        .groupby(\"base_episode_id\")\n        .size()\n        .rename(\"code_group_count\")\n    )\n\n    return (\n        index_episodes[[]]\n        .merge(code_group_count, how=\"left\", left_index=True, right_index=True)\n        .fillna(0.0)[\"code_group_count\"]\n    )\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes.counting.get_all_other_codes","title":"<code>get_all_other_codes(base_episodes, data)</code>","text":"<p>For each patient, get clinical codes in other episodes before/after the base</p> <p>This makes a table of base episodes along with all other episodes for a patient. Two columns <code>base_episode_id</code> and <code>other_episode_id</code> identify the two episodes for each row (they may be equal), and other information is stored such as the time of the base episode, the time to the other episode, and clinical code information for the other episode.</p> <p>This table is used as the basis for all processing involving counting codes before and after an episode.</p> <p>Parameters:</p> Name Type Description Default <code>base_episodes</code> <code>DataFrame</code> <p>Contains <code>episode_id</code> as an index.</p> required <code>data</code> <code>HicData</code> <p>A class containing two DataFrame attributes: * episodes: Contains <code>episode_id</code> as an index, and <code>patient_id</code> and <code>episode_start</code> as columns * codes: Contains <code>episode_id</code> and other code data as columns</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A table containing columns <code>base_episode_id</code>, <code>other_episode_id</code>, <code>base_episode_start</code>, <code>time_to_other_episode</code>, and code data columns for the other episode. Note that the base episode itself is included as an other episode.</p> Source code in <code>src\\pyhbr\\clinical_codes\\counting.py</code> <pre><code>def get_all_other_codes(base_episodes: DataFrame, data: HicData) -&gt; DataFrame:\n    \"\"\"For each patient, get clinical codes in other episodes before/after the base\n\n    This makes a table of base episodes along with all other episodes for a patient.\n    Two columns `base_episode_id` and `other_episode_id` identify the two episodes\n    for each row (they may be equal), and other information is stored such as the\n    time of the base episode, the time to the other episode, and clinical code information\n    for the other episode.\n\n    This table is used as the basis for all processing involving counting codes before\n    and after an episode.\n\n    Args:\n        base_episodes: Contains `episode_id` as an index.\n        data: A class containing two DataFrame attributes:\n            * episodes: Contains `episode_id` as an index, and `patient_id` and `episode_start` as columns\n            * codes: Contains `episode_id` and other code data as columns\n\n    Returns:\n        A table containing columns `base_episode_id`, `other_episode_id`,\n            `base_episode_start`, `time_to_other_episode`, and code data columns\n            for the other episode. Note that the base episode itself is included\n            as an other episode.\n    \"\"\"\n\n    # Remove everything but the index episode_id (in case base_episodes\n    # already has the columns)\n    df = base_episodes[[]]\n\n    base_episode_info = df.merge(\n        data.episodes[[\"patient_id\", \"episode_start\"]], how=\"left\", on=\"episode_id\"\n    ).rename(columns={\"episode_start\": \"base_episode_start\"})\n\n    other_episodes = base_episode_info.reset_index(names=\"base_episode_id\").merge(\n        data.episodes[[\"episode_start\", \"patient_id\"]].reset_index(names=\"other_episode_id\"),\n        how=\"left\",\n        on=\"patient_id\",\n    )\n\n    other_episodes[\"time_to_other_episode\"] = (\n        other_episodes[\"episode_start\"] - other_episodes[\"base_episode_start\"]\n    )\n\n    with_codes = other_episodes.merge(\n        data.codes, how=\"left\", left_on=\"other_episode_id\", right_on=\"episode_id\"\n    ).drop(columns=[\"patient_id\", \"episode_start\", \"episode_id\"])\n\n    return with_codes\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes.counting.get_time_window","title":"<code>get_time_window(all_other_codes, window_start, window_end)</code>","text":"<p>Get the episodes that occurred in a time window with respect to the base episode</p> <p>Use the time_to_other_episode column to filter the all_other_codes table to just those that occurred between window_start and window_end with respect to the the base episode.</p> <p>The arguments window_start and window_end are time differences between the other episode and the base episode. Use positive values for a window after the base, and use negative values for a window before the base.</p> <p>Episodes on the boundary of the window are included.</p> <p>Note that the base episode itself will be included as a row if window_start is negative and window_end is positive (i.e. the window includes episodes before and after the base).</p> <p>Parameters:</p> Name Type Description Default <code>all_other_codes</code> <code>DataFrame</code> <p>Table containing at least <code>time_to_other_episode</code></p> required <code>window_start</code> <code>timedelta</code> <p>The smallest value of time_to_other_episode that will be included in the returned table. Can be negative, meaning episode before the base will be included.</p> required <code>window_end</code> <code>timedelta</code> <p>The largest value of time_to_other_episode that will be included in the returned table. Can be negative, meaning only episodes before the base will be included.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The episodes within the specified window range.</p> Source code in <code>src\\pyhbr\\clinical_codes\\counting.py</code> <pre><code>def get_time_window(\n    all_other_codes: DataFrame, window_start: timedelta, window_end: timedelta\n) -&gt; DataFrame:\n    \"\"\"Get the episodes that occurred in a time window with respect to the base episode\n\n    Use the time_to_other_episode column to filter the all_other_codes\n    table to just those that occurred between window_start and window_end\n    with respect to the the base episode.\n\n    The arguments window_start and window_end are time differences between the other\n    episode and the base episode. Use positive values for a window after the base,\n    and use negative values for a window before the base.\n\n    Episodes on the boundary of the window are included.\n\n    Note that the base episode itself will be included as a row if window_start\n    is negative and window_end is positive (i.e. the window includes episodes before\n    and after the base).\n\n    Args:\n        all_other_codes: Table containing at least `time_to_other_episode`\n        window_start: The smallest value of time_to_other_episode that will be included\n            in the returned table. Can be negative, meaning episode before the base\n            will be included.\n\n        window_end: The largest value of time_to_other_episode that will be included in\n            the returned table. Can be negative, meaning only episodes before the base\n            will be included.\n\n    Returns:\n        The episodes within the specified window range.\n    \"\"\"\n    df = all_other_codes\n    return df[\n        (df[\"time_to_other_episode\"] &lt;= window_end)\n        &amp; (df[\"time_to_other_episode\"] &gt;= window_start)\n    ]\n</code></pre>"},{"location":"reference/#pyhbr.common","title":"<code>common</code>","text":"<p>Common utilities for other modules.</p> <p>A collection of routines used by the data source or analysis functions.</p>"},{"location":"reference/#pyhbr.common.CheckedTable","title":"<code>CheckedTable</code>","text":"<p>Wrapper for sqlalchemy table with checks for table/columns</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>class CheckedTable:\n    \"\"\"Wrapper for sqlalchemy table with checks for table/columns\"\"\"\n\n    def __init__(self, table_name: str, engine: Engine) -&gt; None:\n        \"\"\"Get a CheckedTable by reading from the remote server\n\n        This is a wrapper around the sqlalchemy Table for\n        catching errors when accessing columns through the\n        c attribute.\n\n        Args:\n            table_name: The name of the table whose metadata should be retrieved\n            engine: The database connection\n\n        Returns:\n            The table data for use in SQL queries\n        \"\"\"\n        self.name = table_name\n        metadata_obj = MetaData()\n        try:\n            self.table = Table(self.name, metadata_obj, autoload_with=engine)\n        except NoSuchTableError as e:\n            raise RuntimeError(\n                f\"Could not find table '{e}' in database connection '{engine.url}'\"\n            ) from e\n\n    def col(self, column_name: str) -&gt; Column:\n        \"\"\"Get a column\n\n        Args:\n            column_name: The name of the column to fetch.\n\n        Raises:\n            RuntimeError: Thrown if the column does not exist\n        \"\"\"\n        try:\n            return self.table.c[column_name]\n        except AttributeError as e:\n            raise RuntimeError(\n                f\"Could not find column name '{column_name}' in table '{self.name}'\"\n            ) from e\n</code></pre>"},{"location":"reference/#pyhbr.common.CheckedTable.__init__","title":"<code>__init__(table_name, engine)</code>","text":"<p>Get a CheckedTable by reading from the remote server</p> <p>This is a wrapper around the sqlalchemy Table for catching errors when accessing columns through the c attribute.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>The name of the table whose metadata should be retrieved</p> required <code>engine</code> <code>Engine</code> <p>The database connection</p> required <p>Returns:</p> Type Description <code>None</code> <p>The table data for use in SQL queries</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def __init__(self, table_name: str, engine: Engine) -&gt; None:\n    \"\"\"Get a CheckedTable by reading from the remote server\n\n    This is a wrapper around the sqlalchemy Table for\n    catching errors when accessing columns through the\n    c attribute.\n\n    Args:\n        table_name: The name of the table whose metadata should be retrieved\n        engine: The database connection\n\n    Returns:\n        The table data for use in SQL queries\n    \"\"\"\n    self.name = table_name\n    metadata_obj = MetaData()\n    try:\n        self.table = Table(self.name, metadata_obj, autoload_with=engine)\n    except NoSuchTableError as e:\n        raise RuntimeError(\n            f\"Could not find table '{e}' in database connection '{engine.url}'\"\n        ) from e\n</code></pre>"},{"location":"reference/#pyhbr.common.CheckedTable.col","title":"<code>col(column_name)</code>","text":"<p>Get a column</p> <p>Parameters:</p> Name Type Description Default <code>column_name</code> <code>str</code> <p>The name of the column to fetch.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Thrown if the column does not exist</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def col(self, column_name: str) -&gt; Column:\n    \"\"\"Get a column\n\n    Args:\n        column_name: The name of the column to fetch.\n\n    Raises:\n        RuntimeError: Thrown if the column does not exist\n    \"\"\"\n    try:\n        return self.table.c[column_name]\n    except AttributeError as e:\n        raise RuntimeError(\n            f\"Could not find column name '{column_name}' in table '{self.name}'\"\n        ) from e\n</code></pre>"},{"location":"reference/#pyhbr.common.current_commit","title":"<code>current_commit()</code>","text":"<p>Get current commit.</p> <p>Returns:</p> Type Description <code>str</code> <p>Get the first 12 characters of the current commit, using the first repository found above the current working directory. If the working directory is not in a git repository, return \"nogit\".</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def current_commit() -&gt; str:\n    \"\"\"Get current commit.\n\n    Returns:\n        Get the first 12 characters of the current commit,\n            using the first repository found above the current\n            working directory. If the working directory is not\n            in a git repository, return \"nogit\".\n    \"\"\"\n    try:\n        repo = Repo(search_parent_directories=True)\n        sha = repo.head.object.hexsha[0:11]\n        return sha\n    except InvalidGitRepositoryError:\n        return \"nogit\"\n</code></pre>"},{"location":"reference/#pyhbr.common.current_timestamp","title":"<code>current_timestamp()</code>","text":"<p>Get the current timestamp.</p> <p>Returns:</p> Type Description <code>int</code> <p>The current timestamp (since epoch) rounded to the nearest second.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def current_timestamp() -&gt; int:\n    \"\"\"Get the current timestamp.\n\n    Returns:\n        The current timestamp (since epoch) rounded\n            to the nearest second.\n    \"\"\"\n    return int(time())\n</code></pre>"},{"location":"reference/#pyhbr.common.get_data","title":"<code>get_data(engine, query, *args)</code>","text":"<p>Convenience function to make a query and fetch data.</p> <p>Wraps a function like hic.demographics_query with a call to pd.read_data.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>The database connection</p> required <code>query</code> <code>Callable[[Engine, ...], Select]</code> <p>A function returning a sqlalchemy Select statement</p> required <code>*args</code> <code>...</code> <p>Positional arguments to be passed to query in addition to engine (which is passed first). Make sure they are passed in the same order expected by the query function.</p> <code>()</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The pandas dataframe containing the SQL data</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def get_data(\n    engine: Engine, query: Callable[[Engine, ...], Select], *args: ...\n) -&gt; DataFrame:\n    \"\"\"Convenience function to make a query and fetch data.\n\n    Wraps a function like hic.demographics_query with a\n    call to pd.read_data.\n\n    Args:\n        engine: The database connection\n        query: A function returning a sqlalchemy Select statement\n        *args: Positional arguments to be passed to query in addition\n            to engine (which is passed first). Make sure they are passed\n            in the same order expected by the query function.\n\n    Returns:\n        The pandas dataframe containing the SQL data\n    \"\"\"\n    stmt = query(engine, *args)\n    return read_sql(stmt, engine)\n</code></pre>"},{"location":"reference/#pyhbr.common.get_saved_files_by_name","title":"<code>get_saved_files_by_name(name, save_dir)</code>","text":"<p>Get all saved data files matching name</p> <p>Get the list of files in the save_dir folder matching name. Return the result as a table of file path, commit hash, and saved date. The table is sorted by timestamp, with the most recent file first.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If save_dir does not exist, or there are files in save_dir within invalid file names (not in the format name_commit_timestamp.pkl).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the saved file to load. This matches name in the filename name_commit_timestamp.pkl.</p> required <code>save_dir</code> <code>str</code> <p>The directory to search for files.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe with columns <code>path</code>, <code>commit</code> and <code>created_data</code>.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def get_saved_files_by_name(name: str, save_dir: str) -&gt; DataFrame:\n    \"\"\"Get all saved data files matching name\n\n    Get the list of files in the save_dir folder matching\n    name. Return the result as a table of file path, commit\n    hash, and saved date. The table is sorted by timestamp,\n    with the most recent file first.\n\n    Raises:\n        RuntimeError: If save_dir does not exist, or there are files\n            in save_dir within invalid file names (not in the format\n            name_commit_timestamp.pkl).\n\n    Args:\n        name: The name of the saved file to load. This matches name in\n            the filename name_commit_timestamp.pkl.\n        save_dir: The directory to search for files.\n\n    Returns:\n        A dataframe with columns `path`, `commit` and `created_data`.\n    \"\"\"\n\n    # Check for missing datasets directory\n    if not os.path.isdir(save_dir):\n        raise RuntimeError(\n            f\"Missing folder '{save_dir}'. Check your working directory.\"\n        )\n\n    # Read all the .pkl files in the directory\n    files = DataFrame({\"path\": os.listdir(save_dir)})\n\n    # Identify the file name part. The horrible regex matches the\n    # expression _[commit_hash]_[timestamp].pkl. It is important to\n    # match this part, because \"anything\" can happen in the name part\n    # (including underscores and letters and numbers), so splitting on\n    # _ would not work. The name can then be removed.\n    files[\"name\"] = files[\"path\"].str.replace(\n        r\"_([0-9]|[a-zA-Z])*_\\d*\\.pkl\", \"\", regex=True\n    )\n\n    # Remove all the files whose name does not match, and drop\n    # the name from the path\n    files = files[files[\"name\"] == name]\n    if files.shape[0] == 0:\n        raise ValueError(\n            f\"There is not dataset with the name '{name}' in the datasets directory\"\n        )\n    files[\"commit_and_timestamp\"] = files[\"path\"].str.replace(name + \"_\", \"\")\n\n    # Split the commit and timestamp up (note also the extension)\n    try:\n        files[[\"commit\", \"timestamp\", \"extension\"]] = files[\n            \"commit_and_timestamp\"\n        ].str.split(r\"_|\\.\", expand=True)\n    except Exception as exc:\n        raise RuntimeError(\n            \"Failed to parse files in the datasets folder. \"\n            \"Ensure that all files have the correct format \"\n            \"name_commit_timestamp.(rds|pkl), and \"\n            \"remove any files not matching this \"\n            \"pattern. TODO handle this error properly, \"\n            \"see save_datasets.py.\"\n        ) from exc\n\n    files[\"created_date\"] = to_datetime(files[\"timestamp\"].astype(int), unit=\"s\")\n    recent_first = files.sort_values(by=\"timestamp\", ascending=False).reset_index()[\n        [\"path\", \"commit\", \"created_date\"]\n    ]\n    return recent_first\n</code></pre>"},{"location":"reference/#pyhbr.common.load_item","title":"<code>load_item(name, interactive=False, save_dir='save_data')</code>","text":"<p>Load a previously saved item from file</p> <p>Use this function to load a file that was previously saved using save_item(). By default, the latest version of the item will be returned (the one with the most recent timestamp).</p> <p>None is returned if an interactive load is cancelled by the user.</p> <p>To load an item that is an object from a library (e.g. a pandas DataFrame), the library must be installed (otherwise you will get a ModuleNotFound exception). However, you do not have to import the library before calling this function.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the item to load</p> required <code>interactive</code> <code>bool</code> <p>If True, let the user pick which item version to load interactively. If False, non-interactively load the most recent item (i.e. with the most recent timestamp). The commit hash is not considered when loading the item.</p> <code>False</code> <code>save_fir</code> <p>Which folder to load the item from.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The python object loaded from file, or None for an interactive load that is cancelled by the user.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def load_item(name: str, interactive: bool = False, save_dir: str = \"save_data\") -&gt; Any:\n    \"\"\"Load a previously saved item from file\n\n    Use this function to load a file that was previously saved using\n    save_item(). By default, the latest version of the item will be returned\n    (the one with the most recent timestamp).\n\n    None is returned if an interactive load is cancelled by the user.\n\n    To load an item that is an object from a library (e.g. a pandas DataFrame),\n    the library must be installed (otherwise you will get a ModuleNotFound\n    exception). However, you do not have to import the library before calling this\n    function.\n\n    Args:\n        name: The name of the item to load\n        interactive: If True, let the user pick which item version to load interactively.\n            If False, non-interactively load the most recent item (i.e. with the most\n            recent timestamp). The commit hash is not considered when loading the item.\n        save_fir: Which folder to load the item from.\n\n    Returns:\n        The python object loaded from file, or None for an interactive load that\n            is cancelled by the user.\n\n    \"\"\"\n    if interactive:\n        item_path = pick_saved_file_interactive(name, save_dir)\n    else:\n        item_path = pick_most_recent_saved_file(name, save_dir)\n\n    if item_path is None:\n        print(\"Aborted (interactive) load item\")\n        return\n\n    print(f\"Loading {item_path}\")\n\n    # Load a generic pickle. Note that if this is a pandas dataframe,\n    # pandas must be installed (otherwise you will get module not found).\n    # The same goes for a pickle storing an object from any other library.\n    with open(item_path, \"rb\") as file:\n        return pickle.load(file)\n</code></pre>"},{"location":"reference/#pyhbr.common.make_engine","title":"<code>make_engine(con_string='mssql+pyodbc://dsn', database='hic_cv_test')</code>","text":"<p>Make a sqlalchemy engine</p> <p>This function is intended for use with Microsoft SQL Server. The preferred method to connect to the server on Windows is to use a Data Source Name (DSN). To use the default connection string argument, set up a data source name called \"dsn\" using the program \"ODBC Data Sources\".</p> <p>If you need to access multiple different databases on the same server, you will need different engines. Specify the database name while creating the engine (this will override a default database in the DSN, if there is one).</p> <p>Parameters:</p> Name Type Description Default <code>con_string</code> <code>str</code> <p>The sqlalchemy connection string.</p> <code>'mssql+pyodbc://dsn'</code> <code>database</code> <code>str</code> <p>The database name to connect to.</p> <code>'hic_cv_test'</code> <p>Returns:</p> Type Description <code>Engine</code> <p>The sqlalchemy engine</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def make_engine(\n    con_string: str = \"mssql+pyodbc://dsn\", database: str = \"hic_cv_test\"\n) -&gt; Engine:\n    \"\"\"Make a sqlalchemy engine\n\n    This function is intended for use with Microsoft SQL\n    Server. The preferred method to connect to the server\n    on Windows is to use a Data Source Name (DSN). To use the\n    default connection string argument, set up a data source\n    name called \"dsn\" using the program \"ODBC Data Sources\".\n\n    If you need to access multiple different databases on the\n    same server, you will need different engines. Specify the\n    database name while creating the engine (this will override\n    a default database in the DSN, if there is one).\n\n    Args:\n        con_string: The sqlalchemy connection string.\n        database: The database name to connect to.\n\n    Returns:\n        The sqlalchemy engine\n    \"\"\"\n    connect_args = {\"database\": database}\n    return create_engine(con_string, connect_args=connect_args)\n</code></pre>"},{"location":"reference/#pyhbr.common.pick_most_recent_saved_file","title":"<code>pick_most_recent_saved_file(name, save_dir)</code>","text":"<p>Get the path to the most recent file matching name.</p> <p>Like pick_saved_file_interactive, but automatically selects the most recent file in save_data.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the saved file to list</p> required <code>save_dir</code> <code>str</code> <p>The directory to search for files</p> required <p>Returns:</p> Type Description <code>str</code> <p>The absolute path to the most recent matching file.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def pick_most_recent_saved_file(name: str, save_dir: str) -&gt; str:\n    \"\"\"Get the path to the most recent file matching name.\n\n    Like pick_saved_file_interactive, but automatically selects the most\n    recent file in save_data.\n\n    Args:\n        name: The name of the saved file to list\n        save_dir: The directory to search for files\n\n    Returns:\n        The absolute path to the most recent matching file.\n    \"\"\"\n    recent_first = get_saved_files_by_name(name, save_dir)\n    full_path = os.path.join(save_dir, recent_first.loc[0, \"path\"])\n    return full_path\n</code></pre>"},{"location":"reference/#pyhbr.common.pick_saved_file_interactive","title":"<code>pick_saved_file_interactive(name, save_dir)</code>","text":"<p>Select a file matching name interactively</p> <p>Print a list of the saved items in the save_dir folder, along with the date and time it was generated, and the commit hash, and let the user pick which item should be loaded interactively. The full filename of the resulting file is returned, which can then be read by the user.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the saved file to list</p> required <code>save_dir</code> <code>str</code> <p>The directory to search for files</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>The absolute path to the interactively selected file, or None if the interactive load was aborted.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def pick_saved_file_interactive(name: str, save_dir: str) -&gt; str | None:\n    \"\"\"Select a file matching name interactively\n\n    Print a list of the saved items in the save_dir folder, along\n    with the date and time it was generated, and the commit hash,\n    and let the user pick which item should be loaded interactively.\n    The full filename of the resulting file is returned, which can\n    then be read by the user.\n\n    Args:\n        name: The name of the saved file to list\n        save_dir: The directory to search for files\n\n    Returns:\n        The absolute path to the interactively selected file, or None\n            if the interactive load was aborted.\n    \"\"\"\n\n    recent_first = get_saved_files_by_name(name, save_dir)\n    print(recent_first)\n\n    num_datasets = recent_first.shape[0]\n    while True:\n        try:\n            raw_choice = input(\n                f\"Pick a dataset to load: [{0} - {num_datasets-1}] (type q[uit]/exit, then Enter, to quit): \"\n            )\n            if \"exit\" in raw_choice or \"q\" in raw_choice:\n                return None\n            choice = int(raw_choice)\n        except Exception:\n            print(f\"{raw_choice} is not valid; try again.\")\n            continue\n        if choice &lt; 0 or choice &gt;= num_datasets:\n            print(f\"{choice} is not in range; try again.\")\n            continue\n        break\n\n    full_path = os.path.join(save_dir, recent_first.loc[choice, \"path\"])\n    return full_path\n</code></pre>"},{"location":"reference/#pyhbr.common.requires_commit","title":"<code>requires_commit()</code>","text":"<p>Check whether changes need committing</p> <p>To make most effective use of the commit hash stored with a save_item call, the current branch should be clean (all changes committed). Call this function to check.</p> <p>Returns False if there is no git repository.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the working directory is in a git repository that requires a commit; False otherwise.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def requires_commit() -&gt; bool:\n    \"\"\"Check whether changes need committing\n\n    To make most effective use of the commit hash stored with a\n    save_item call, the current branch should be clean (all changes\n    committed). Call this function to check.\n\n    Returns False if there is no git repository.\n\n    Returns:\n        True if the working directory is in a git repository that requires\n            a commit; False otherwise.\n    \"\"\"\n    try:\n        repo = Repo(search_parent_directories=True)\n        return repo.is_dirty(untracked_files=True)\n    except InvalidGitRepositoryError:\n        # No need to commit if not repository\n        return False\n</code></pre>"},{"location":"reference/#pyhbr.common.save_item","title":"<code>save_item(item, name, save_dir='save_data/', enforce_clean_branch=True)</code>","text":"<p>Save an item to a pickle file</p> <p>Saves a python object (e.g. a pandas DataFrame) dataframe in the save_dir folder, using a filename that includes the current timestamp and the current commit hash. Use load_item to retrieve the file.</p> <p>Important</p> <p>Ensure that <code>save_data/</code> (or your chosen <code>save_dir</code>) is added to the .gitignore of your repository to ensure sensitive data is not committed.</p> <p>By storing the commit hash and timestamp, it is possible to identify when items were created and what code created them. To make most effective use of the commit hash, ensure that you commit, and do not make any further code edits, before running a script that calls save_item (otherwise the commit hash will not quite reflect the state of the running code).</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Any</code> <p>The python object to saave (e.g. pandas DataFrame)</p> required <code>name</code> <code>str</code> <p>The name of the item. The filename will be created by adding a suffix for the current commit and the timestamp to show when the data was saved (format: <code>name_commit_timestamp.pkl</code>)</p> required <code>save_dir</code> <code>str</code> <p>Where to save the data, relative to the current working directory. The directory will be created if it does not exist.</p> <code>'save_data/'</code> <code>enforce_clean_branch</code> <p>If True, the function will raise an exception if an attempt is made to save an item when the repository has uncommitted changes.</p> <code>True</code> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def save_item(\n    item: Any, name: str, save_dir: str = \"save_data/\", enforce_clean_branch=True\n) -&gt; None:\n    \"\"\"Save an item to a pickle file\n\n    Saves a python object (e.g. a pandas DataFrame) dataframe in the save_dir\n    folder, using a filename that includes the current timestamp and the current\n    commit hash. Use load_item to retrieve the file.\n\n    !!! important\n        Ensure that `save_data/` (or your chosen `save_dir`) is added to the\n        .gitignore of your repository to ensure sensitive data is not committed.\n\n    By storing the commit hash and timestamp, it is possible to identify when items\n    were created and what code created them. To make most effective use of the\n    commit hash, ensure that you commit, and do not make any further code edits,\n    before running a script that calls save_item (otherwise the commit hash will\n    not quite reflect the state of the running code).\n\n    Args:\n        item: The python object to saave (e.g. pandas DataFrame)\n        name: The name of the item. The filename will be created by adding\n            a suffix for the current commit and the timestamp to show when the\n            data was saved (format: `name_commit_timestamp.pkl`)\n        save_dir: Where to save the data, relative to the current working directory.\n            The directory will be created if it does not exist.\n        enforce_clean_branch: If True, the function will raise an exception if an attempt\n            is made to save an item when the repository has uncommitted changes.\n    \"\"\"\n\n    if enforce_clean_branch and requires_commit():\n        raise RuntimeError(\n            \"Aborting save_item() because branch is not clean. Commit your changes before saving item to increase the chance of reproducing the item based on the filename commit hash.\"\n        )\n\n    if not os.path.isdir(save_dir):\n        print(f\"Creating missing folder '{save_dir}' for storing item\")\n        os.mkdir(save_dir)\n\n    # Make the file suffix out of the current git\n    # commit hash and the current time\n    filename = f\"{name}_{current_commit()}_{current_timestamp()}.pkl\"\n    path = os.path.join(save_dir, filename)\n\n    with open(path, \"wb\") as file:\n        pickle.dump(item, file)\n</code></pre>"},{"location":"reference/#pyhbr.data_source","title":"<code>data_source</code>","text":"<p>Routines for fetching data from sources.</p> <p>This module is intended to interface to the data source, and should be modified to port this package to new SQL databases.</p>"},{"location":"reference/#pyhbr.data_source.hes","title":"<code>hes</code>","text":"<p>SQL queries and other functions for HES (BNSSG ICB) data.</p>"},{"location":"reference/#pyhbr.data_source.hes.diagnosis_and_procedure_columns","title":"<code>diagnosis_and_procedure_columns()</code>","text":"<p>BNSSG ICB diagnosis/procedure column names</p> <p>Get the diagnosis and procedure part of the SQL query, which is common to both the episodes and spells queries. </p> <p>Column names are mapped to the names diagnosis_n, procedure_n, where n is zero-indexed. The primary diagnosis/procedure is diagnosis_0/procedure_0, and smaller n implies higher diagnosis/procedure priority.</p> <p>Returns:</p> Type Description <code>str</code> <p>part of SQL select query listing diagnosis/procedure columns</p> Source code in <code>src\\pyhbr\\data_source\\hes.py</code> <pre><code>def diagnosis_and_procedure_columns():\n    \"\"\"BNSSG ICB diagnosis/procedure column names\n\n    Get the diagnosis and procedure part of the\n    SQL query, which is common to both the episodes and\n    spells queries. \n\n    Column names are mapped to the names diagnosis_n,\n    procedure_n, where n is zero-indexed. The primary\n    diagnosis/procedure is diagnosis_0/procedure_0, and\n    smaller n implies higher diagnosis/procedure priority.\n\n    Returns:\n        (str): part of SQL select query listing diagnosis/procedure columns\n    \"\"\"\n    return (\n        \",diagnosisprimary_icd as diagnosis_0\"\n        \",diagnosis1stsecondary_icd as diagnosis_1\"\n        \",diagnosis2ndsecondary_icd as diagnosis_2\"\n        \",diagnosis3rdsecondary_icd as diagnosis_3\"\n        \",diagnosis4thsecondary_icd as diagnosis_4\"\n        \",diagnosis5thsecondary_icd as diagnosis_5\"\n        \",diagnosis6thsecondary_icd as diagnosis_6\"\n        \",diagnosis7thsecondary_icd as diagnosis_7\"\n        \",diagnosis8thsecondary_icd as diagnosis_8\"\n        \",diagnosis9thsecondary_icd as diagnosis_9\"\n        \",diagnosis10thsecondary_icd as diagnosis_10\"\n        \",diagnosis11thsecondary_icd as diagnosis_11\"\n        \",diagnosis12thsecondary_icd as diagnosis_12\"\n        \",diagnosis13thsecondary_icd as diagnosis_13\"\n        \",diagnosis14thsecondary_icd as diagnosis_14\"\n        \",diagnosis15thsecondary_icd as diagnosis_15\"\n        \",diagnosis16thsecondary_icd as diagnosis_16\"\n        \",diagnosis17thsecondary_icd as diagnosis_17\"\n        \",diagnosis18thsecondary_icd as diagnosis_18\"\n        \",diagnosis19thsecondary_icd as diagnosis_19\"\n        \",diagnosis20thsecondary_icd as diagnosis_20\"\n        \",diagnosis21stsecondary_icd as diagnosis_21\"\n        \",diagnosis22ndsecondary_icd as diagnosis_22\"\n        \",diagnosis23rdsecondary_icd as diagnosis_23\"\n        \",primaryprocedure_opcs as procedure_0\"\n        \",procedure2nd_opcs as procedure_1\"\n        \",procedure3rd_opcs as procedure_2\"\n        \",procedure4th_opcs as procedure_3\"\n        \",procedure5th_opcs as procedure_4\"\n        \",procedure6th_opcs as procedure_5\"\n        \",procedure7th_opcs as procedure_6\"\n        \",procedure8th_opcs as procedure_7\"\n        \",procedure9th_opcs as procedure_8\"\n        \",procedure10th_opcs as procedure_9\"\n        \",procedure11th_opcs as procedure_10\"\n        \",procedure12th_opcs as procedure_11\"\n        \",procedure13th_opcs as procedure_12\"\n        \",procedure14th_opcs as procedure_13\"\n        \",procedure15th_opcs as procedure_14\"\n        \",procedure16th_opcs as procedure_15\"\n        \",procedure17th_opcs as procedure_16\"\n        \",procedure18th_opcs as procedure_17\"\n        \",procedure19th_opcs as procedure_18\"\n        \",procedure20th_opcs as procedure_19\"\n        \",procedure21st_opcs as procedure_20\"\n        \",procedure22nd_opcs as procedure_21\"\n        \",procedure23rd_opcs as procedure_22\"\n        \",procedure24th_opcs as procedure_23\"\n    )\n</code></pre>"},{"location":"reference/#pyhbr.data_source.hes.episodes_query","title":"<code>episodes_query(start_date, end_date)</code>","text":"<p>BNSSG ICB SQL query for episodes table</p> <p>Make an SQL query to retrieve the episodes table between start_date and end_date (inclusive).</p> <p>Notes:</p> <p>The NHS number 9000219621 is used to indicate \"invalid NHS number\" in these tables, so should be excluded.</p> <p>Rows with the following commissioner codes are not valid: '5M8','11T','5QJ','11H','5A3','12A', '15C','14F','Q65'.</p> <p>The nhs_number not-NULL condition ensures that the columns will not be converted to floating  point by pandas.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>date</code> <p>first valid consultant-episode start date</p> required <code>end_date</code> <code>date</code> <p>last valid consultant-episode start date</p> required <p>Returns:</p> Type Description <code>str</code> <p>SQL query to retrieve episodes table</p> Source code in <code>src\\pyhbr\\data_source\\hes.py</code> <pre><code>def episodes_query(start_date, end_date):\n    \"\"\"BNSSG ICB SQL query for episodes table\n\n    Make an SQL query to retrieve the episodes table\n    between start_date and end_date (inclusive).\n\n    Notes:\n\n    The NHS number 9000219621 is used to indicate\n    \"invalid NHS number\" in these tables, so should\n    be excluded.\n\n    Rows with the following commissioner codes are\n    not valid: '5M8','11T','5QJ','11H','5A3','12A',\n    '15C','14F','Q65'.\n\n    The nhs_number not-NULL condition ensures that\n    the columns will not be converted to floating \n    point by pandas.\n\n    Args:\n        start_date (datetime.date): first valid consultant-episode start date\n        end_date (datetime.date): last valid consultant-episode start date\n\n    Returns:\n        (str): SQL query to retrieve episodes table\n    \"\"\"\n    return (\n        \"select aimtc_pseudo_nhs as patient_id\"\n        \",aimtc_age as age\"\n        \",sex as gender\"\n        \",pbrspellid as spell_id\"\n        \",aimtc_providerspell_start_date as spell_start\"\n        \",aimtc_providerspell_end_date as spell_end\"\n        \",startdate_consultantepisode as episode_start\"\n        \",enddate_consultantepisode as episode_end\"\n        + diagnosis_and_procedure_columns()\n        + \" from abi.dbo.vw_apc_sem_001\"\n        f\" where startdate_consultantepisode between '{start_date}' and '{end_date}'\"\n        \" and aimtc_pseudo_nhs is not null\"\n        \" and aimtc_pseudo_nhs != '9000219621'\"\n        \" and aimtc_organisationcode_codeofcommissioner in ('5M8','11T','5QJ','11H','5A3','12A','15C','14F','Q65')\"\n    )\n</code></pre>"},{"location":"reference/#pyhbr.data_source.hic","title":"<code>hic</code>","text":"<p>SQL queries and functions for HIC (v3, UHBW) data.</p> <p>Most data available in the HIC tables is fetched in the  queries below, apart from columns which are all-NULL, provide keys/IDs that will not be used, or provide duplicate information (e.g. duplicated in two tables).</p>"},{"location":"reference/#pyhbr.data_source.hic.demographics_query","title":"<code>demographics_query(engine)</code>","text":"<p>Get demographic information from HIC data</p> <p>The date/time at which the data was obtained is not stored in the table, but patient age can be computed from the date of the episode under consideration and the year_of_birth in this table.</p> <p>The underlying table does have a cause_of_death column, but it is all null, so not included.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>the connection to the database</p> required <p>Returns:</p> Type Description <code>Select</code> <p>SQL query to retrieve episodes table</p> Source code in <code>src\\pyhbr\\data_source\\hic.py</code> <pre><code>def demographics_query(engine: Engine) -&gt; Select:\n    \"\"\"Get demographic information from HIC data\n\n    The date/time at which the data was obtained is\n    not stored in the table, but patient age can be\n    computed from the date of the episode under consideration\n    and the year_of_birth in this table.\n\n    The underlying table does have a cause_of_death column,\n    but it is all null, so not included.\n\n    Args:\n        engine: the connection to the database\n\n    Returns:\n        SQL query to retrieve episodes table\n    \"\"\"\n    table = CheckedTable(\"cv1_demographics\", engine)\n    return select(\n        table.col(\"subject\").cast(String).label(\"patient_id\"),\n        table.col(\"gender\"),\n        table.col(\"year_of_birth\"),\n        table.col(\"death_date\"),\n    )\n</code></pre>"},{"location":"reference/#pyhbr.data_source.hic.diagnoses_query","title":"<code>diagnoses_query(engine)</code>","text":"<p>Get the diagnoses corresponding to episodes</p> <p>This should be linked to the episodes table to obtain information about the diagnoses in the episode.</p> <p>Diagnoses are encoded using ICD-10 codes, and the position column contains the order of diagnoses in the episode (1-indexed).</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>the connection to the database</p> required <p>Returns:</p> Type Description <code>Select</code> <p>SQL query to retrieve diagnoses table</p> Source code in <code>src\\pyhbr\\data_source\\hic.py</code> <pre><code>def diagnoses_query(engine: Engine) -&gt; Select:\n    \"\"\"Get the diagnoses corresponding to episodes\n\n    This should be linked to the episodes table to\n    obtain information about the diagnoses in the episode.\n\n    Diagnoses are encoded using ICD-10 codes, and the\n    position column contains the order of diagnoses in\n    the episode (1-indexed).\n\n    Args:\n        engine: the connection to the database\n\n    Returns:\n        SQL query to retrieve diagnoses table\n    \"\"\"\n    table = CheckedTable(\"cv1_episodes_diagnosis\", engine)\n    return select(\n        table.col(\"episode_identifier\").cast(String).label(\"episode_id\"),\n        table.col(\"diagnosis_date_time\").label(\"time\"),\n        table.col(\"diagnosis_position\").label(\"position\"),\n        table.col(\"diagnosis_code_icd\").label(\"code\"),\n    )\n</code></pre>"},{"location":"reference/#pyhbr.data_source.hic.episodes_query","title":"<code>episodes_query(engine, start_date, end_date)</code>","text":"<p>Get the episodes list in the HIC data</p> <p>This table does not contain any episode information, just a patient and an episode id for linking to diagnosis and procedure information in other tables.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>the connection to the database</p> required <code>start_date</code> <code>date</code> <p>first valid consultant-episode start date</p> required <code>end_date</code> <code>date</code> <p>last valid consultant-episode start date</p> required <p>Returns:</p> Type Description <code>Select</code> <p>SQL query to retrieve episodes table</p> Source code in <code>src\\pyhbr\\data_source\\hic.py</code> <pre><code>def episodes_query(engine: Engine, start_date: date, end_date: date) -&gt; Select:\n    \"\"\"Get the episodes list in the HIC data\n\n    This table does not contain any episode information,\n    just a patient and an episode id for linking to diagnosis\n    and procedure information in other tables.\n\n    Args:\n        engine: the connection to the database\n        start_date: first valid consultant-episode start date\n        end_date: last valid consultant-episode start date\n\n    Returns:\n        SQL query to retrieve episodes table\n    \"\"\"\n    table = CheckedTable(\"cv1_episodes\", engine)\n    return select(\n        table.col(\"subject\").cast(String).label(\"patient_id\"),\n        table.col(\"episode_identifier\").cast(String).label(\"episode_id\"),\n        table.col(\"spell_identifier\").cast(String).label(\"spell_id\"),\n        table.col(\"episode_start_time\").label(\"episode_start\"),\n        table.col(\"episode_end_time\").label(\"episode_end\"),\n    ).where(\n        table.col(\"episode_start_time\") &gt;= start_date,\n        table.col(\"episode_end_time\") &lt;= end_date,\n    )\n</code></pre>"},{"location":"reference/#pyhbr.data_source.hic.pathology_blood_query","title":"<code>pathology_blood_query(engine, investigations)</code>","text":"<p>Get the table of blood test results in the HIC data</p> <p>Since blood tests in this table are not associated with an episode directly by key, it is necessary to link them based on the patient identifier and date. This operation can be quite slow if the blood tests table is large. One way to reduce the size is to filter by investigation using the investigations parameter. The investigation codes in the HIC data are shown below:</p> <code>investigation</code> Description OBR_BLS_UL LFT OBR_BLS_UE UREA,CREAT + ELECTROLYTES OBR_BLS_FB FULL BLOOD COUNT OBR_BLS_UT THYROID FUNCTION TEST OBR_BLS_TP TOTAL PROTEIN OBR_BLS_CR C-REACTIVE PROTEIN OBR_BLS_CS CLOTTING SCREEN OBR_BLS_FI FIB-4 OBR_BLS_AS AST OBR_BLS_CA CALCIUM GROUP OBR_BLS_TS TSH AND FT4 OBR_BLS_FO SERUM FOLATE OBR_BLS_PO PHOSPHATE OBR_BLS_LI LIPID PROFILE OBR_POC_VG POCT BLOOD GAS VENOUS SAMPLE OBR_BLS_HD HDL CHOLESTEROL OBR_BLS_FT FREE T4 OBR_BLS_FE SERUM FERRITIN OBR_BLS_GP ELECTROLYTES NO POTASSIUM OBR_BLS_CH CHOLESTEROL OBR_BLS_MG MAGNESIUM OBR_BLS_CO CORTISOL <p>Each test is similarly encoded. The valid test codes in the full blood count and U+E investigations are shown below:</p> <code>investigation</code> <code>test</code> Description OBR_BLS_FB OBX_BLS_NE Neutrophils OBR_BLS_FB OBX_BLS_PL Platelets OBR_BLS_FB OBX_BLS_WB White Cell Count OBR_BLS_FB OBX_BLS_LY Lymphocytes OBR_BLS_FB OBX_BLS_MC MCV OBR_BLS_FB OBX_BLS_HB Haemoglobin OBR_BLS_FB OBX_BLS_HC Haematocrit OBR_BLS_UE OBX_BLS_NA Sodium OBR_BLS_UE OBX_BLS_UR Urea OBR_BLS_UE OBX_BLS_K Potassium OBR_BLS_UE OBX_BLS_CR Creatinine OBR_BLS_UE OBX_BLS_EP eGFR/1.73m2 (CKD-EPI) <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>the connection to the database</p> required <code>investigations</code> <code>list[str]</code> <p>Which types of laboratory test to include in the query. Fetching fewer types of test makes the query faster.</p> required <p>Returns:</p> Type Description <code>Engine</code> <p>SQL query to retrieve blood tests table</p> Source code in <code>src\\pyhbr\\data_source\\hic.py</code> <pre><code>def pathology_blood_query(engine: Engine, investigations: list[str]) -&gt; Engine:\n    \"\"\"Get the table of blood test results in the HIC data\n\n    Since blood tests in this table are not associated with an episode\n    directly by key, it is necessary to link them based on the patient\n    identifier and date. This operation can be quite slow if the blood\n    tests table is large. One way to reduce the size is to filter by\n    investigation using the investigations parameter. The investigation\n    codes in the HIC data are shown below:\n\n    | `investigation` | Description                 |\n    |-----------------|-----------------------------|\n    | OBR_BLS_UL      |                          LFT|\n    | OBR_BLS_UE      |    UREA,CREAT + ELECTROLYTES|\n    | OBR_BLS_FB      |             FULL BLOOD COUNT|\n    | OBR_BLS_UT      |        THYROID FUNCTION TEST|\n    | OBR_BLS_TP      |                TOTAL PROTEIN|\n    | OBR_BLS_CR      |           C-REACTIVE PROTEIN|\n    | OBR_BLS_CS      |              CLOTTING SCREEN|\n    | OBR_BLS_FI      |                        FIB-4|\n    | OBR_BLS_AS      |                          AST|\n    | OBR_BLS_CA      |                CALCIUM GROUP|\n    | OBR_BLS_TS      |                  TSH AND FT4|\n    | OBR_BLS_FO      |                SERUM FOLATE|\n    | OBR_BLS_PO      |                    PHOSPHATE|\n    | OBR_BLS_LI      |                LIPID PROFILE|\n    | OBR_POC_VG      | POCT BLOOD GAS VENOUS SAMPLE|\n    | OBR_BLS_HD      |              HDL CHOLESTEROL|\n    | OBR_BLS_FT      |                      FREE T4|\n    | OBR_BLS_FE      |               SERUM FERRITIN|\n    | OBR_BLS_GP      |    ELECTROLYTES NO POTASSIUM|\n    | OBR_BLS_CH      |                  CHOLESTEROL|\n    | OBR_BLS_MG      |                    MAGNESIUM|\n    | OBR_BLS_CO      |                     CORTISOL|\n\n    Each test is similarly encoded. The valid test codes in the full\n    blood count and U+E investigations are shown below:\n\n    | `investigation` | `test`     | Description          |\n    |-----------------|------------|----------------------|\n    | OBR_BLS_FB      | OBX_BLS_NE |           Neutrophils|\n    | OBR_BLS_FB      | OBX_BLS_PL |             Platelets|\n    | OBR_BLS_FB      | OBX_BLS_WB |      White Cell Count|\n    | OBR_BLS_FB      | OBX_BLS_LY |           Lymphocytes|\n    | OBR_BLS_FB      | OBX_BLS_MC |                   MCV|\n    | OBR_BLS_FB      | OBX_BLS_HB |           Haemoglobin|\n    | OBR_BLS_FB      | OBX_BLS_HC |           Haematocrit|\n    | OBR_BLS_UE      | OBX_BLS_NA |                Sodium|\n    | OBR_BLS_UE      | OBX_BLS_UR |                  Urea|\n    | OBR_BLS_UE      | OBX_BLS_K  |             Potassium|\n    | OBR_BLS_UE      | OBX_BLS_CR |            Creatinine|\n    | OBR_BLS_UE      | OBX_BLS_EP | eGFR/1.73m2 (CKD-EPI)|\n\n    Args:\n        engine: the connection to the database\n        investigations: Which types of laboratory\n            test to include in the query. Fetching fewer types of\n            test makes the query faster.\n\n    Returns:\n        SQL query to retrieve blood tests table\n    \"\"\"\n    table = CheckedTable(\"cv1_pathology_blood\", engine)\n    return select(\n        table.col(\"subject\").cast(String).label(\"patient_id\"),\n        table.col(\"investigation_code\").label(\"investigation\"),\n        table.col(\"test_code\").label(\"test\"),\n        table.col(\"test_result\").label(\"result\"),\n        table.col(\"test_result_unit\").label(\"unit\"),\n        table.col(\"sample_collected_date_time\").label(\"sample_date\"),\n        table.col(\"result_available_date_time\").label(\"result_date\"),\n        table.col(\"result_flag\"),\n        table.col(\"result_lower_range\"),\n        table.col(\"result_upper_range\"),\n    ).where(table.col(\"investigation_code\").in_(investigations))\n</code></pre>"},{"location":"reference/#pyhbr.data_source.hic.pharmacy_prescribing_query","title":"<code>pharmacy_prescribing_query(engine)</code>","text":"<p>Get medicines prescribed to patients over time</p> <p>This table contains information about medicines  prescribed to patients, identified by patient and time (i.e. it is not associated to an episode). The information includes the medicine name, dose (includes unit), frequency,  form (e.g. tablets), route (e.g. oral), and whether the medicine was present on admission.</p> <p>The most commonly occurring formats for various relevant medicines are shown in the table below:</p> <code>name</code> <code>dose</code> <code>frequency</code> <code>drug_form</code> <code>route</code> aspirin 75 mg in the MORNING NaN Oral aspirin 75 mg in the MORNING dispersible tablet Oral clopidogrel 75 mg in the MORNING film coated tablets Oral ticagrelor 90 mg TWICE a day tablets Oral warfarin 3 mg ONCE a day  at 18:00 NaN Oral warfarin 5 mg ONCE a day  at 18:00 tablets Oral apixaban 5 mg TWICE a day tablets Oral dabigatran etexilate 110 mg TWICE a day capsules Oral edoxaban 60 mg in the MORNING tablets Oral rivaroxaban 20 mg in the MORNING film coated tablets Oral <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>the connection to the database</p> required <p>Returns:</p> Type Description <code>Select</code> <p>SQL query to retrieve procedures table</p> Source code in <code>src\\pyhbr\\data_source\\hic.py</code> <pre><code>def pharmacy_prescribing_query(engine: Engine) -&gt; Select:\n    \"\"\"Get medicines prescribed to patients over time\n\n    This table contains information about medicines \n    prescribed to patients, identified by patient and time\n    (i.e. it is not associated to an episode). The information\n    includes the medicine name, dose (includes unit), frequency, \n    form (e.g. tablets), route (e.g. oral), and whether the\n    medicine was present on admission.\n\n    The most commonly occurring formats for various relevant\n    medicines are shown in the table below:\n\n    | `name`       | `dose`  | `frequency`    | `drug_form`         | `route` |\n    |--------------|---------|----------------|---------------------|---------|\n    | aspirin      | 75 mg   | in the MORNING | NaN                 | Oral    |\n    | aspirin      | 75 mg   | in the MORNING | dispersible tablet  | Oral    |\n    | clopidogrel  | 75 mg   | in the MORNING | film coated tablets | Oral    |\n    | ticagrelor   | 90 mg   | TWICE a day    | tablets             | Oral    |\n    | warfarin     | 3 mg    | ONCE a day  at 18:00 | NaN           | Oral    |\n    | warfarin     | 5 mg    | ONCE a day  at 18:00 | tablets       | Oral    |       \n    | apixaban     | 5 mg    | TWICE a day          | tablets       | Oral    |\n    | dabigatran etexilate | 110 mg | TWICE a day   | capsules      | Oral    |\n    | edoxaban     | 60 mg   | in the MORNING       | tablets       | Oral    |\n    | rivaroxaban  | 20 mg   | in the MORNING | film coated tablets | Oral    |\n\n    Args:\n        engine: the connection to the database\n\n    Returns:\n        SQL query to retrieve procedures table\n    \"\"\"\n    table = CheckedTable(\"cv1_pharmacy_prescribing\", engine)\n    return select(\n        table.col(\"subject\").cast(String).label(\"patient_id\"),\n        table.col(\"order_date_time\").label(\"order_date\"),\n        table.col(\"medication_name\").label(\"name\"),\n        table.col(\"ordered_dose\").label(\"dose\"),\n        table.col(\"ordered_frequency\").label(\"frequency\"),\n        table.col(\"ordered_drug_form\").label(\"drug_form\"),\n        table.col(\"ordered_route\").label(\"route\"),\n        table.col(\"admission_medicine_y_n\").label(\"on_admission\"),\n    )\n</code></pre>"},{"location":"reference/#pyhbr.data_source.hic.procedures_query","title":"<code>procedures_query(engine)</code>","text":"<p>Get the procedures corresponding to episodes</p> <p>This should be linked to the episodes table to obtain information about the procedures in the episode.</p> <p>Procedures are encoded using OPCS-4 codes, and the position column contains the order of procedures in the episode (1-indexed).</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>the connection to the database</p> required <p>Returns:</p> Type Description <code>Select</code> <p>SQL query to retrieve procedures table</p> Source code in <code>src\\pyhbr\\data_source\\hic.py</code> <pre><code>def procedures_query(engine: Engine) -&gt; Select:\n    \"\"\"Get the procedures corresponding to episodes\n\n    This should be linked to the episodes table to\n    obtain information about the procedures in the episode.\n\n    Procedures are encoded using OPCS-4 codes, and the\n    position column contains the order of procedures in\n    the episode (1-indexed).\n\n    Args:\n        engine: the connection to the database\n\n    Returns:\n        SQL query to retrieve procedures table\n    \"\"\"\n    table = CheckedTable(\"cv1_episodes_procedures\", engine)\n    return select(\n        table.col(\"episode_identifier\").cast(String).label(\"episode_id\"),\n        table.col(\"procedure_date_time\").label(\"time\"),\n        table.col(\"procedure_position\").label(\"position\"),\n        table.col(\"procedure_code_opcs\").label(\"code\"),\n    )\n</code></pre>"},{"location":"reference/#pyhbr.data_source.hic_covid","title":"<code>hic_covid</code>","text":"<p>SQL queries and functions for HIC (COVID-19, UHBW) data.</p>"},{"location":"reference/#pyhbr.data_source.hic_covid.episodes_query","title":"<code>episodes_query(engine)</code>","text":"<p>Get the episodes list in the HIC data</p> <p>This table does not contain any episode information, just a patient and an episode id for linking to diagnosis and procedure information in other tables.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>the connection to the database</p> required <code>start_date</code> <p>first valid consultant-episode start date</p> required <code>end_date</code> <p>last valid consultant-episode start date</p> required <p>Returns:</p> Type Description <code>Select</code> <p>SQL query to retrieve episodes table</p> Source code in <code>src\\pyhbr\\data_source\\hic_covid.py</code> <pre><code>def episodes_query(engine: Engine) -&gt; Select:\n    \"\"\"Get the episodes list in the HIC data\n\n    This table does not contain any episode information,\n    just a patient and an episode id for linking to diagnosis\n    and procedure information in other tables.\n\n    Args:\n        engine: the connection to the database\n        start_date: first valid consultant-episode start date\n        end_date: last valid consultant-episode start date\n\n    Returns:\n        SQL query to retrieve episodes table\n    \"\"\"\n    table = CheckedTable(\"cv_covid_episodes\", engine)\n    return select(\n        table.col(\"NHS_NUMBER\").cast(String).label(\"nhs_number\"),\n        table.col(\"Other Number\").cast(String).label(\"t_number\"),\n        table.col(\"episode_identifier\").cast(String).label(\"episode_id\"),\n    )\n</code></pre>"},{"location":"reference/#pyhbr.example","title":"<code>example</code>","text":""},{"location":"reference/#pyhbr.example.bar","title":"<code>bar(x)</code>","text":"<p>Bar returns x + 1</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyhbr.example import bar\n&gt;&gt;&gt; bar(3)\n4\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>int</code> <p>The value to use</p> required Source code in <code>src\\pyhbr\\example.py</code> <pre><code>def bar(x):\n    \"\"\"Bar returns x + 1\n\n    Examples:\n        &gt;&gt;&gt; from pyhbr.example import bar\n        &gt;&gt;&gt; bar(3)\n        4\n\n    Args:\n        x (int): The value to use\n    \"\"\"\n    return x+1\n</code></pre>"},{"location":"reference/#pyhbr.example.foo","title":"<code>foo()</code>","text":"<p>Print Foo</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyhbr.example import foo\n&gt;&gt;&gt; foo()\nFoo ran\n</code></pre> Source code in <code>src\\pyhbr\\example.py</code> <pre><code>def foo():\n    \"\"\"Print Foo\n\n    Examples:\n        &gt;&gt;&gt; from pyhbr.example import foo\n        &gt;&gt;&gt; foo()\n        Foo ran\n\n    \"\"\"\n    print(\"Foo ran\")\n</code></pre>"},{"location":"reference/#pyhbr.middle","title":"<code>middle</code>","text":"<p>Routines for interfacing between the data sources and analysis functions</p>"},{"location":"reference/#pyhbr.middle.from_hic","title":"<code>from_hic</code>","text":"<p>Convert HIC tables into the formats required for analysis</p>"},{"location":"reference/#pyhbr.middle.from_hic.HicData","title":"<code>HicData</code>","text":"Source code in <code>src\\pyhbr\\middle\\from_hic.py</code> <pre><code>class HicData:\n    def __init__(self, engine: Engine, start_date: date, end_date: date):\n        \"\"\"Get the HIC dataset (collection of 5 tables)\n\n        Args:\n            engine: The connection to the database\n            start_date: The start date (inclusive) for returned episodes\n            end_date:  The end date (inclusive) for returned episodes\n\n        Raises:\n            RuntimeError: if the episode_id is not unique\n        \"\"\"\n\n        self.codes = get_clinical_codes(\n            engine, \"icd10_arc_hbr.yaml\", \"opcs4_arc_hbr.yaml\"\n        )  # slow\n        self.episodes = get_episodes(engine, start_date, end_date)  # fast\n        self.prescriptions = get_prescriptions(engine, self.episodes)  # fast\n        self.lab_results = get_lab_results(engine, self.episodes)  # really slow\n        self.demographics = get_demographics(engine)\n\n        if self.episodes.value_counts(\"episode_id\").max() &gt; 1:\n            raise RuntimeError(\n                \"Found non-unique episode IDs; subsequent script will be invalid\"\n            )\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_hic.HicData.__init__","title":"<code>__init__(engine, start_date, end_date)</code>","text":"<p>Get the HIC dataset (collection of 5 tables)</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>The connection to the database</p> required <code>start_date</code> <code>date</code> <p>The start date (inclusive) for returned episodes</p> required <code>end_date</code> <code>date</code> <p>The end date (inclusive) for returned episodes</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the episode_id is not unique</p> Source code in <code>src\\pyhbr\\middle\\from_hic.py</code> <pre><code>def __init__(self, engine: Engine, start_date: date, end_date: date):\n    \"\"\"Get the HIC dataset (collection of 5 tables)\n\n    Args:\n        engine: The connection to the database\n        start_date: The start date (inclusive) for returned episodes\n        end_date:  The end date (inclusive) for returned episodes\n\n    Raises:\n        RuntimeError: if the episode_id is not unique\n    \"\"\"\n\n    self.codes = get_clinical_codes(\n        engine, \"icd10_arc_hbr.yaml\", \"opcs4_arc_hbr.yaml\"\n    )  # slow\n    self.episodes = get_episodes(engine, start_date, end_date)  # fast\n    self.prescriptions = get_prescriptions(engine, self.episodes)  # fast\n    self.lab_results = get_lab_results(engine, self.episodes)  # really slow\n    self.demographics = get_demographics(engine)\n\n    if self.episodes.value_counts(\"episode_id\").max() &gt; 1:\n        raise RuntimeError(\n            \"Found non-unique episode IDs; subsequent script will be invalid\"\n        )\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_hic.check_const_column","title":"<code>check_const_column(df, col_name, expect)</code>","text":"<p>Raise an error if a column is not constant</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The table to check</p> required <code>col_name</code> <code>str</code> <p>The name of the column which should be constant</p> required <code>expect</code> <code>str</code> <p>The expected constant value of the column</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Raised if the column is not constant with the expected value.</p> Source code in <code>src\\pyhbr\\middle\\from_hic.py</code> <pre><code>def check_const_column(df: pd.DataFrame, col_name: str, expect: str):\n    \"\"\"Raise an error if a column is not constant\n\n    Args:\n        df: The table to check\n        col_name: The name of the column which should be constant\n        expect: The expected constant value of the column\n\n    Raises:\n        RuntimeError: Raised if the column is not constant with\n            the expected value.\n    \"\"\"\n    if not all(df[col_name] == expect):\n        raise RuntimeError(\n            f\"Found unexpected value in '{col_name}' column. \"\n            f\"Expected constant '{expect}', but got: \"\n            f\"{df[col_name].unique()}\"\n        )\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_hic.filter_to_groups","title":"<code>filter_to_groups(codes_table, codes)</code>","text":"<p>Filter a table of raw clinical codes to only keep codes in groups</p> <p>Use this function to drop clinical codes which are not of interest, and convert all codes to normalised form (lowercase, no whitespace, no dot).</p> <p>This function is tested on the HIC dataset, but should be modifiable for use with any data source returning diagnoses and procedures as separate tables in long format. Consider modifying the columns of codes_table that are contained in the output.</p> <p>Parameters:</p> Name Type Description Default <code>codes_table</code> <code>DataFrame</code> <p>Either a diagnoses or procedures table. For this function to work, it needs:</p> <ul> <li>A <code>code</code> column containing the clinical code.</li> <li>An <code>episode_id</code> identifying which episode contains the code.</li> <li>A <code>position</code> identifying the primary/secondary position of the     code in the episode.</li> </ul> required <code>codes</code> <code>ClinicalCodeTree</code> <p>The clinical codes object (previously loaded from a file) containing code groups to use.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A table containing the episode ID, the clinical code (normalised),</p> <code>DataFrame</code> <p>the group containing the code, and the code position.</p> Source code in <code>src\\pyhbr\\middle\\from_hic.py</code> <pre><code>def filter_to_groups(\n    codes_table: pd.DataFrame, codes: ClinicalCodeTree\n) -&gt; pd.DataFrame:\n    \"\"\"Filter a table of raw clinical codes to only keep codes in groups\n\n    Use this function to drop clinical codes which are not of interest,\n    and convert all codes to normalised form (lowercase, no whitespace, no dot).\n\n    This function is tested on the HIC dataset, but should be modifiable\n    for use with any data source returning diagnoses and procedures as\n    separate tables in long format. Consider modifying the columns of\n    codes_table that are contained in the output.\n\n    Args:\n        codes_table: Either a diagnoses or procedures table. For this\n            function to work, it needs:\n\n            * A `code` column containing the clinical code.\n            * An `episode_id` identifying which episode contains the code.\n            * A `position` identifying the primary/secondary position of the\n                code in the episode.\n\n        codes: The clinical codes object (previously loaded from a file)\n            containing code groups to use.\n\n    Returns:\n        A table containing the episode ID, the clinical code (normalised),\n        the group containing the code, and the code position.\n\n    \"\"\"\n    codes_with_groups = codes_in_any_group(codes)\n    codes_table[\"code\"] = codes_table[\"code\"].apply(normalise_code)\n    codes_table = pd.merge(codes_table, codes_with_groups, on=\"code\", how=\"inner\")\n    codes_table = codes_table[[\"episode_id\", \"code\", \"docs\", \"group\", \"position\"]]\n\n    return codes_table\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_hic.get_clinical_codes","title":"<code>get_clinical_codes(engine, diagnoses_file, procedures_file)</code>","text":"<p>Main diagnoses/procedures fetch for the HIC data</p> <p>This function wraps the diagnoses/procedures queries and a filtering operation to reduce the tables to only those rows which contain a code in a group. One table is returned which contains both the diagnoses and procedures in long format, along with the associated episode ID and the primary/secondary position of the code in the episode.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>The connection to the database</p> required <code>diagnoses_file</code> <code>str</code> <p>The diagnoses codes file name (loaded from the package)</p> required <code>procedures_file</code> <code>str</code> <p>The procedures codes file name (loaded from the package)</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A table containing diagnoses/procedures, normalised codes, code groups, diagnosis positions, and associated episode ID.</p> Source code in <code>src\\pyhbr\\middle\\from_hic.py</code> <pre><code>def get_clinical_codes(\n    engine: Engine, diagnoses_file: str, procedures_file: str\n) -&gt; pd.DataFrame:\n    \"\"\"Main diagnoses/procedures fetch for the HIC data\n\n    This function wraps the diagnoses/procedures queries and a filtering\n    operation to reduce the tables to only those rows which contain a code\n    in a group. One table is returned which contains both the diagnoses and\n    procedures in long format, along with the associated episode ID and the\n    primary/secondary position of the code in the episode.\n\n    Args:\n        engine: The connection to the database\n        diagnoses_file: The diagnoses codes file name (loaded from the package)\n        procedures_file: The procedures codes file name (loaded from the package)\n\n    Returns:\n        A table containing diagnoses/procedures, normalised codes, code groups,\n            diagnosis positions, and associated episode ID.\n    \"\"\"\n\n    diagnosis_codes = load_from_package(diagnoses_file)\n    procedures_codes = load_from_package(procedures_file)\n\n    # Fetch the data from the server\n    diagnoses = get_data(engine, hic.diagnoses_query)\n    procedures = get_data(engine, hic.procedures_query)\n\n    # Reduce data to only code groups, and combine diagnoses/procedures\n    filtered_diagnoses = filter_to_groups(diagnoses, diagnosis_codes)\n    filtered_procedures = filter_to_groups(procedures, procedures_codes)\n\n    # Tag the diagnoses/procedures, and combine the tables\n    filtered_diagnoses[\"type\"] = \"diagnosis\"\n    filtered_procedures[\"type\"] = \"procedure\"\n\n    codes = pd.concat([filtered_diagnoses, filtered_procedures])\n    codes[\"type\"] = codes[\"type\"].astype(\"category\")\n\n    return codes\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_hic.get_demographics","title":"<code>get_demographics(engine)</code>","text":"<p>Get patient demographic information</p> <p>Gender is encoded using the NHS data dictionary values, which is mapped to a category column in the table. (Note that initial values are strings, not integers.)</p> <ul> <li>\"0\": Not known. Mapped to \"unknown\"</li> <li>\"1\": Male: Mapped to \"male\"</li> <li>\"2\": Female. Mapped to \"female\"</li> <li>\"9\": Not specified. Mapped to \"unknown\".</li> </ul> <p>Not mapping 0/9 to NA in case either is related to non-binary genders (i.e. it contains information, rather than being a NULL field).</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>The connection to the database</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A table indexed by patient_id, containing gender, birth year, and death_date (if applicable).</p> Source code in <code>src\\pyhbr\\middle\\from_hic.py</code> <pre><code>def get_demographics(engine: Engine) -&gt; pd.DataFrame:\n    \"\"\"Get patient demographic information\n\n    Gender is encoded using the NHS data dictionary values, which\n    is mapped to a category column in the table. (Note that initial\n    values are strings, not integers.)\n\n    * \"0\": Not known. Mapped to \"unknown\"\n    * \"1\": Male: Mapped to \"male\"\n    * \"2\": Female. Mapped to \"female\"\n    * \"9\": Not specified. Mapped to \"unknown\".\n\n    Not mapping 0/9 to NA in case either is related to non-binary\n    genders (i.e. it contains information, rather than being a NULL field).\n\n    Args:\n        engine: The connection to the database\n\n    Returns:\n        A table indexed by patient_id, containing gender, birth\n            year, and death_date (if applicable).\n\n    \"\"\"\n    df = get_data(engine, hic.demographics_query)\n    df.set_index(\"patient_id\", drop=True, inplace=True)\n\n    # Convert gender to categories\n    df[\"gender\"] = df[\"gender\"].replace(\"9\", \"0\")\n    df[\"gender\"] = df[\"gender\"].astype(\"category\")\n    df[\"gender\"] = df[\"gender\"].cat.rename_categories(\n        {\"0\": \"unknown\", \"1\": \"male\", \"2\": \"female\"}\n    )\n\n    return df\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_hic.get_episodes","title":"<code>get_episodes(engine, start_date, end_date)</code>","text":"<p>Get the table of episodes</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>The connection to the database</p> required <code>start_date</code> <code>date</code> <p>The start date (inclusive) for returned episodes</p> required <code>end_date</code> <code>date</code> <p>The end date (inclusive) for returned episodes</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The episode date, indexed by episode_id</p> Source code in <code>src\\pyhbr\\middle\\from_hic.py</code> <pre><code>def get_episodes(engine: Engine, start_date: date, end_date: date) -&gt; pd.DataFrame:\n    \"\"\"Get the table of episodes\n\n    Args:\n        engine: The connection to the database\n        start_date: The start date (inclusive) for returned episodes\n        end_date:  The end date (inclusive) for returned episodes\n\n    Returns:\n        The episode date, indexed by episode_id\n    \"\"\"\n    df = get_data(engine, hic.episodes_query, start_date, end_date)\n    return df.set_index(\"episode_id\", drop=True)\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_hic.get_lab_results","title":"<code>get_lab_results(engine, episodes)</code>","text":"<p>Get relevant laboratory results from the HIC data, linked to episode</p> <p>For information about the contents of the table, refer to the documentation for get_unlinked_lab_results().</p> <p>This function links each laboratory test to the first episode containing the sample collected date in its date range. For more about this, see link_to_episodes().</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>The connection to the database</p> required <code>episodes</code> <code>DataFrame</code> <p>The episodes table, used for linking. Must contain <code>patient_id</code>, <code>episode_id</code>, <code>episode_start</code> and <code>episode_end</code>.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Table of laboratory results, including Hb (haemoglobin), platelet count, and eGFR (kidney function). The columns are <code>sample_date</code>, <code>test_name</code>, <code>episode_id</code>.</p> Source code in <code>src\\pyhbr\\middle\\from_hic.py</code> <pre><code>def get_lab_results(engine: Engine, episodes: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Get relevant laboratory results from the HIC data, linked to episode\n\n    For information about the contents of the table, refer to the\n    documentation for [get_unlinked_lab_results()][pyhbr.middle.from_hic.get_unlinked_lab_results].\n\n    This function links each laboratory test to the first episode containing\n    the sample collected date in its date range. For more about this, see\n    [link_to_episodes()][pyhbr.middle.from_hic.link_to_episodes].\n\n    Args:\n        engine: The connection to the database\n        episodes: The episodes table, used for linking. Must contain\n            `patient_id`, `episode_id`, `episode_start` and `episode_end`.\n\n    Returns:\n        Table of laboratory results, including Hb (haemoglobin),\n            platelet count, and eGFR (kidney function). The columns are\n            `sample_date`, `test_name`, `episode_id`.\n    \"\"\"\n    lab_results = get_unlinked_lab_results(engine)\n    return link_to_episodes(lab_results, episodes, \"sample_date\")\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_hic.get_prescriptions","title":"<code>get_prescriptions(engine, episodes)</code>","text":"<p>Get relevant prescriptions from the HIC data, linked to episode</p> <p>For information about the contents of the table, refer to the documentation for get_unlinked_prescriptions().</p> <p>This function links each prescription to the first episode containing the prescription order date in its date range. For more about this, see link_to_episodes().</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>The connection to the database</p> required <code>episodes</code> <code>DataFrame</code> <p>The episodes table, used for linking. Must contain <code>patient_id</code>, <code>episode_id</code>, <code>episode_start</code> and <code>episode_end</code>.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The table of prescriptions, including the prescription name, prescription group (oac or nsaid), frequency (in doses per day), and link to the associated episode.</p> Source code in <code>src\\pyhbr\\middle\\from_hic.py</code> <pre><code>def get_prescriptions(engine: Engine, episodes: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Get relevant prescriptions from the HIC data, linked to episode\n\n    For information about the contents of the table, refer to the\n    documentation for [get_unlinked_prescriptions()][pyhbr.middle.from_hic.get_unlinked_prescriptions].\n\n    This function links each prescription to the first episode containing\n    the prescription order date in its date range. For more about this, see\n    [link_to_episodes()][pyhbr.middle.from_hic.link_to_episodes].\n\n    Args:\n        engine: The connection to the database\n        episodes: The episodes table, used for linking. Must contain\n            `patient_id`, `episode_id`, `episode_start` and `episode_end`.\n\n    Returns:\n        The table of prescriptions, including the prescription name,\n            prescription group (oac or nsaid), frequency (in doses per day),\n            and link to the associated episode.\n    \"\"\"\n    prescriptions = get_unlinked_prescriptions(engine)\n    return link_to_episodes(prescriptions, episodes, \"order_date\")\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_hic.get_unlinked_lab_results","title":"<code>get_unlinked_lab_results(engine)</code>","text":"<p>Get laboratory results from the HIC database (unlinked to episode)</p> <p>This function returns data for the following three tests, identified by one of these values in the <code>test_name</code> column:</p> <ul> <li><code>hb</code>: haemoglobin (unit: g/dL)</li> <li><code>egfr</code>: eGFR (unit: mL/min)</li> <li><code>platelets</code>: platelet count (unit: 10^9/L)</li> </ul> <p>The test result is associated to a <code>patient_id</code>, and the time when the sample for the test was collected is stored in the <code>sample_date</code> column.</p> <p>Some values in the underlying table contain inequalities in the results column, which have been removed (so egfr &gt;90 becomes 90).</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>The connection to the database</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Table of laboratory results, including Hb (haemoglobin), platelet count, and eGFR (kidney function). The columns are <code>patient_id</code>, <code>test_name</code>, and <code>sample_date</code>.</p> Source code in <code>src\\pyhbr\\middle\\from_hic.py</code> <pre><code>def get_unlinked_lab_results(engine: Engine) -&gt; pd.DataFrame:\n    \"\"\"Get laboratory results from the HIC database (unlinked to episode)\n\n    This function returns data for the following three\n    tests, identified by one of these values in the\n    `test_name` column:\n\n    * `hb`: haemoglobin (unit: g/dL)\n    * `egfr`: eGFR (unit: mL/min)\n    * `platelets`: platelet count (unit: 10^9/L)\n\n    The test result is associated to a `patient_id`,\n    and the time when the sample for the test was collected\n    is stored in the `sample_date` column.\n\n    Some values in the underlying table contain inequalities\n    in the results column, which have been removed (so\n    egfr &gt;90 becomes 90).\n\n    Args:\n        engine: The connection to the database\n\n    Returns:\n        Table of laboratory results, including Hb (haemoglobin),\n            platelet count, and eGFR (kidney function). The columns are\n            `patient_id`, `test_name`, and `sample_date`.\n\n    \"\"\"\n    df = get_data(engine, hic.pathology_blood_query, [\"OBR_BLS_UE\", \"OBR_BLS_FB\"])\n\n    df[\"test_name\"] = df[\"investigation\"] + \"_\" + df[\"test\"]\n\n    test_of_interest = {\n        \"OBR_BLS_FB_OBX_BLS_HB\": \"hb\",\n        \"OBR_BLS_UE_OBX_BLS_EP\": \"egfr\",\n        \"OBR_BLS_FB_OBX_BLS_PL\": \"platelets\",\n    }\n\n    # Only keep tests of interest: platelets, egfr, and hb\n    df = df[df[\"test_name\"].isin(test_of_interest.keys())]\n\n    # Rename the items\n    df[\"test_name\"] = df[\"test_name\"].map(test_of_interest)\n\n    # Check egfr unit\n    rows = df[df[\"test_name\"] == \"egfr\"]\n    check_const_column(rows, \"unit\", \"mL/min\")\n\n    # Check hb unit\n    rows = df[df[\"test_name\"] == \"hb\"]\n    check_const_column(rows, \"unit\", \"g/L\")\n\n    # Check platelets unit (note 10*9/L is not a typo)\n    rows = df[df[\"test_name\"] == \"platelets\"]\n    check_const_column(rows, \"unit\", \"10*9/L\")\n\n    # Some values include an inequality; e.g.:\n    # - egfr: &gt;90\n    # - platelets: &lt;3\n    #\n    # Remove instances of &lt; or &gt; to enable conversion\n    # to float.\n    df[\"result\"] = df[\"result\"].str.replace(\"&lt;|&gt;\", \"\", regex=True)\n\n    # Convert results column to float\n    df[\"result\"] = df[\"result\"].astype(float)\n\n    # Convert hb units to g/dL (to match ARC HBR definition)\n    df.loc[df[\"test_name\"] == \"hb\", \"result\"] /= 10.0\n\n    return df[[\"patient_id\", \"sample_date\", \"test_name\", \"result\"]]\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_hic.get_unlinked_prescriptions","title":"<code>get_unlinked_prescriptions(engine)</code>","text":"<p>Get relevant prescriptions from the HIC data (unlinked to episode)</p> <p>This function is tailored towards the calculation of the ARC HBR score, so it focusses on prescriptions on oral anticoagulants (e.g. warfarin) and non-steroidal anti-inflammatory drugs (NSAIDs, e.g. ibuprofen).</p> <p>The frequency column reflects the maximum allowable doses per day. For the purposes of ARC HBR, where NSAIDs must be prescribed &gt; 4 days/week, all prescriptions in the HIC data indicate frequency &gt; 1 (i.e. at least one per day), and therefore qualify for ARC HBR purposes.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>The connection to the database</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The table of prescriptions, including the patient_id, order_date (to link to an episode), prescription name, prescription group (oac or nsaid), and frequency (in doses per day).</p> Source code in <code>src\\pyhbr\\middle\\from_hic.py</code> <pre><code>def get_unlinked_prescriptions(engine: Engine) -&gt; pd.DataFrame:\n    \"\"\"Get relevant prescriptions from the HIC data (unlinked to episode)\n\n    This function is tailored towards the calculation of the\n    ARC HBR score, so it focusses on prescriptions on oral\n    anticoagulants (e.g. warfarin) and non-steroidal\n    anti-inflammatory drugs (NSAIDs, e.g. ibuprofen).\n\n    The frequency column reflects the maximum allowable\n    doses per day. For the purposes of ARC HBR, where NSAIDs\n    must be prescribed &gt; 4 days/week, all prescriptions in\n    the HIC data indicate frequency &gt; 1 (i.e. at least one\n    per day), and therefore qualify for ARC HBR purposes.\n\n    Args:\n        engine: The connection to the database\n\n    Returns:\n        The table of prescriptions, including the patient_id,\n            order_date (to link to an episode), prescription name,\n            prescription group (oac or nsaid), and frequency (in\n            doses per day).\n    \"\"\"\n\n    df = get_data(engine, hic.pharmacy_prescribing_query)\n\n    prescriptions_of_interest = {\n        \"warfarin\": \"oac\",\n        \"apixaban\": \"oac\",\n        \"dabigatran etexilate\": \"oac\",\n        \"edoxaban\": \"oac\",\n        \"rivaroxaban\": \"oac\",\n        \"ibuprofen\": \"nsaid\",\n        \"naproxen\": \"nsaid\",\n        \"diclofenac\": \"nsaid\",\n        \"diclofenac sodium\": \"nsaid\",\n        \"celecoxib\": \"nsaid\",  # Not present in HIC data\n        \"mefenamic acid\": \"nsaid\",  # Not present in HIC data\n        \"etoricoxib\": \"nsaid\",\n        \"indometacin\": \"nsaid\",  # This spelling is used in HIC data\n        \"indomethacin\": \"nsaid\",  # Alternative spelling\n        # \"aspirin\": \"nsaid\" -- not accounting for high dose\n    }\n\n    # Only keep prescriptions of interest\n    df = df[df[\"name\"].isin(prescriptions_of_interest.keys())]\n\n    # Add the type of prescription to the table\n    df[\"group\"] = df[\"name\"].map(prescriptions_of_interest)\n\n    # Replace alternative spellings\n    df[\"name\"] = df[\"name\"].str.replace(\"indomethacin\", \"indometacin\")\n\n    # Replace admission medicine column with bool\n    on_admission_map = {\"y\": True, \"n\": False}\n    df[\"on_admission\"] = df[\"on_admission\"].map(on_admission_map)\n\n    # Extra spaces are not typos.\n    per_day = {\n        \"TWICE a day\": 2,\n        \"in the MORNING\": 1,\n        \"THREE times a day\": 3,\n        \"TWICE a day at 08:00 and 22:00\": 2,\n        \"ONCE a day  at 18:00\": 1,\n        \"up to every SIX hours\": 4,\n        \"up to every EIGHT hours\": 3,\n        \"TWICE a day at 08:00 and 20:00\": 2,\n        \"up to every 24 hours\": 1,\n        \"THREE times a day at 08:00 15:00 and 22:00\": 3,\n        \"TWICE a day at 08:00 and 19:00\": 2,\n        \"ONCE a day  at 20:00\": 1,\n        \"ONCE a day  at 08:00\": 1,\n        \"up to every 12 hours\": 2,\n        \"ONCE a day  at 19:00\": 1,\n        \"THREE times a day at 08:00 15:00 and 20:00\": 3,\n        \"THREE times a day at 08:00 14:00 and 22:00\": 3,\n        \"ONCE a day  at 22:00\": 1,\n        \"every EIGHT hours\": 24,\n        \"ONCE a day  at 09:00\": 1,\n        \"up to every FOUR hours\": 6,\n        \"TWICE a day at 06:00 and 18:00\": 2,\n        \"at NIGHT\": 1,\n        \"ONCE a day  at 14:00\": 1,\n        \"ONCE a day  at 12:00\": 1,\n        \"THREE times a day at 08:00 14:00 and 20:00\": 3,\n        \"THREE times a day at 00:00 08:00 and 16:00\": 3,\n    }\n\n    # Replace frequencies strings with doses per day\n    df[\"frequency\"] = df[\"frequency\"].map(per_day)\n\n    return df[\n        [\"patient_id\", \"order_date\", \"name\", \"group\", \"frequency\", \"on_admission\"]\n    ].reset_index(drop=True)\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_hic.link_to_episodes","title":"<code>link_to_episodes(items, episodes, date_col_name)</code>","text":"<p>Link HIC laboratory test/prescriptions to episode by date</p> <p>Use this function to add an episode_id to the laboratory tests table or the prescriptions table. Tests/prescriptions are generically referred to as items below.</p> <p>This function associates each item with the first episode containing the item date in its [episode_start, episode_end) range. The column containing the item date is given by <code>date_col_name</code>.</p> <p>For prescriptions, use the prescription order date for linking. For laboratory tests, use the sample collected date.</p> <p>This function assumes that the episode_id in the episodes table is unique (i.e. no patients share an episode ID).</p> <p>For higher performance, reduce the item table to items of interest before calling this function.</p> <p>Since episodes may slightly overlap, an item may be associated with more than one episode. In this case, the function will associate the item with the earliest episode (the returned table will not contain duplicate items).</p> <p>The final table does not use episode_id as an index, because an episode may contain multiple items.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>DataFrame</code> <p>The prescriptions or laboratory tests table. Must contain a <code>date_col_name</code> column, which is used to compare with episode start/end dates, and the <code>patient_id</code>.</p> required <code>episodes</code> <code>DataFrame</code> <p>The episodes table. Must contain <code>patient_id</code>, <code>episode_id</code>, <code>episode_start</code> and <code>episode_end</code>.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The items table with additional <code>episode_id</code> and <code>spell_id</code> columns.</p> Source code in <code>src\\pyhbr\\middle\\from_hic.py</code> <pre><code>def link_to_episodes(\n    items: pd.DataFrame, episodes: pd.DataFrame, date_col_name: str\n) -&gt; pd.DataFrame:\n    \"\"\"Link HIC laboratory test/prescriptions to episode by date\n\n    Use this function to add an episode_id to the laboratory tests\n    table or the prescriptions table. Tests/prescriptions are generically\n    referred to as items below.\n\n    This function associates each item with the first episode containing\n    the item date in its [episode_start, episode_end) range. The column\n    containing the item date is given by `date_col_name`.\n\n    For prescriptions, use the prescription order date for linking. For\n    laboratory tests, use the sample collected date.\n\n    This function assumes that the episode_id in the episodes table is\n    unique (i.e. no patients share an episode ID).\n\n    For higher performance, reduce the item table to items of interest\n    before calling this function.\n\n    Since episodes may slightly overlap, an item may be associated\n    with more than one episode. In this case, the function will associate\n    the item with the earliest episode (the returned table will\n    not contain duplicate items).\n\n    The final table does not use episode_id as an index, because an episode\n    may contain multiple items.\n\n    Args:\n        items: The prescriptions or laboratory tests table. Must contain a\n            `date_col_name` column, which is used to compare with episode\n            start/end dates, and the `patient_id`.\n\n        episodes: The episodes table. Must contain `patient_id`, `episode_id`,\n            `episode_start` and `episode_end`.\n\n    Returns:\n        The items table with additional `episode_id` and `spell_id` columns.\n    \"\"\"\n\n    # Before linking to episodes, add an item ID. This is to\n    # remove duplicated items in the last step of linking,\n    # due ot overlapping episode time windows.\n    items[\"item_id\"] = range(items.shape[0])\n\n    # Join together all items and episode information by patient. Use\n    # a left join on items (assuming items is narrowed to the item types\n    # of interest) to keep the result smaller. Reset the index to move\n    # episode_id to a column.\n    with_episodes = pd.merge(items, episodes.reset_index(), how=\"left\", on=\"patient_id\")\n\n    # Thinking of each row as both an episode and a item, drop any\n    # rows where the item date does not fall within the start\n    # and end of the episode (start date inclusive).\n    consistent_dates = (\n        with_episodes[date_col_name] &gt;= with_episodes[\"episode_start\"]\n    ) &amp; (with_episodes[date_col_name] &lt; with_episodes[\"episode_end\"])\n    overlapping_episodes = with_episodes[consistent_dates]\n\n    # Since some episodes overlap in time, some items will end up\n    # being associated with more than one episode. Remove any\n    # duplicates by associating only with the earliest episode.\n    deduplicated = (\n        overlapping_episodes.sort_values(\"episode_start\").groupby(\"item_id\").head(1)\n    )\n\n    # Keep episode_id, drop other episodes/unnecessary columns.\n    return deduplicated.drop(columns=[\"item_id\"]).drop(columns=episodes.columns)\n</code></pre>"},{"location":"reference/#analysis","title":"Analysis","text":"<p>Routines for performing statistics, analysis, or fitting models</p>"},{"location":"reference/#common-utilities","title":"Common Utilities","text":"<p>Common utilities for other modules.</p> <p>A collection of routines used by the data source or analysis functions.</p>"},{"location":"reference/#pyhbr.common.CheckedTable","title":"<code>CheckedTable</code>","text":"<p>Wrapper for sqlalchemy table with checks for table/columns</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>class CheckedTable:\n    \"\"\"Wrapper for sqlalchemy table with checks for table/columns\"\"\"\n\n    def __init__(self, table_name: str, engine: Engine) -&gt; None:\n        \"\"\"Get a CheckedTable by reading from the remote server\n\n        This is a wrapper around the sqlalchemy Table for\n        catching errors when accessing columns through the\n        c attribute.\n\n        Args:\n            table_name: The name of the table whose metadata should be retrieved\n            engine: The database connection\n\n        Returns:\n            The table data for use in SQL queries\n        \"\"\"\n        self.name = table_name\n        metadata_obj = MetaData()\n        try:\n            self.table = Table(self.name, metadata_obj, autoload_with=engine)\n        except NoSuchTableError as e:\n            raise RuntimeError(\n                f\"Could not find table '{e}' in database connection '{engine.url}'\"\n            ) from e\n\n    def col(self, column_name: str) -&gt; Column:\n        \"\"\"Get a column\n\n        Args:\n            column_name: The name of the column to fetch.\n\n        Raises:\n            RuntimeError: Thrown if the column does not exist\n        \"\"\"\n        try:\n            return self.table.c[column_name]\n        except AttributeError as e:\n            raise RuntimeError(\n                f\"Could not find column name '{column_name}' in table '{self.name}'\"\n            ) from e\n</code></pre>"},{"location":"reference/#pyhbr.common.CheckedTable.__init__","title":"<code>__init__(table_name, engine)</code>","text":"<p>Get a CheckedTable by reading from the remote server</p> <p>This is a wrapper around the sqlalchemy Table for catching errors when accessing columns through the c attribute.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>The name of the table whose metadata should be retrieved</p> required <code>engine</code> <code>Engine</code> <p>The database connection</p> required <p>Returns:</p> Type Description <code>None</code> <p>The table data for use in SQL queries</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def __init__(self, table_name: str, engine: Engine) -&gt; None:\n    \"\"\"Get a CheckedTable by reading from the remote server\n\n    This is a wrapper around the sqlalchemy Table for\n    catching errors when accessing columns through the\n    c attribute.\n\n    Args:\n        table_name: The name of the table whose metadata should be retrieved\n        engine: The database connection\n\n    Returns:\n        The table data for use in SQL queries\n    \"\"\"\n    self.name = table_name\n    metadata_obj = MetaData()\n    try:\n        self.table = Table(self.name, metadata_obj, autoload_with=engine)\n    except NoSuchTableError as e:\n        raise RuntimeError(\n            f\"Could not find table '{e}' in database connection '{engine.url}'\"\n        ) from e\n</code></pre>"},{"location":"reference/#pyhbr.common.CheckedTable.col","title":"<code>col(column_name)</code>","text":"<p>Get a column</p> <p>Parameters:</p> Name Type Description Default <code>column_name</code> <code>str</code> <p>The name of the column to fetch.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Thrown if the column does not exist</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def col(self, column_name: str) -&gt; Column:\n    \"\"\"Get a column\n\n    Args:\n        column_name: The name of the column to fetch.\n\n    Raises:\n        RuntimeError: Thrown if the column does not exist\n    \"\"\"\n    try:\n        return self.table.c[column_name]\n    except AttributeError as e:\n        raise RuntimeError(\n            f\"Could not find column name '{column_name}' in table '{self.name}'\"\n        ) from e\n</code></pre>"},{"location":"reference/#pyhbr.common.current_commit","title":"<code>current_commit()</code>","text":"<p>Get current commit.</p> <p>Returns:</p> Type Description <code>str</code> <p>Get the first 12 characters of the current commit, using the first repository found above the current working directory. If the working directory is not in a git repository, return \"nogit\".</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def current_commit() -&gt; str:\n    \"\"\"Get current commit.\n\n    Returns:\n        Get the first 12 characters of the current commit,\n            using the first repository found above the current\n            working directory. If the working directory is not\n            in a git repository, return \"nogit\".\n    \"\"\"\n    try:\n        repo = Repo(search_parent_directories=True)\n        sha = repo.head.object.hexsha[0:11]\n        return sha\n    except InvalidGitRepositoryError:\n        return \"nogit\"\n</code></pre>"},{"location":"reference/#pyhbr.common.current_timestamp","title":"<code>current_timestamp()</code>","text":"<p>Get the current timestamp.</p> <p>Returns:</p> Type Description <code>int</code> <p>The current timestamp (since epoch) rounded to the nearest second.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def current_timestamp() -&gt; int:\n    \"\"\"Get the current timestamp.\n\n    Returns:\n        The current timestamp (since epoch) rounded\n            to the nearest second.\n    \"\"\"\n    return int(time())\n</code></pre>"},{"location":"reference/#pyhbr.common.get_data","title":"<code>get_data(engine, query, *args)</code>","text":"<p>Convenience function to make a query and fetch data.</p> <p>Wraps a function like hic.demographics_query with a call to pd.read_data.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>The database connection</p> required <code>query</code> <code>Callable[[Engine, ...], Select]</code> <p>A function returning a sqlalchemy Select statement</p> required <code>*args</code> <code>...</code> <p>Positional arguments to be passed to query in addition to engine (which is passed first). Make sure they are passed in the same order expected by the query function.</p> <code>()</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The pandas dataframe containing the SQL data</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def get_data(\n    engine: Engine, query: Callable[[Engine, ...], Select], *args: ...\n) -&gt; DataFrame:\n    \"\"\"Convenience function to make a query and fetch data.\n\n    Wraps a function like hic.demographics_query with a\n    call to pd.read_data.\n\n    Args:\n        engine: The database connection\n        query: A function returning a sqlalchemy Select statement\n        *args: Positional arguments to be passed to query in addition\n            to engine (which is passed first). Make sure they are passed\n            in the same order expected by the query function.\n\n    Returns:\n        The pandas dataframe containing the SQL data\n    \"\"\"\n    stmt = query(engine, *args)\n    return read_sql(stmt, engine)\n</code></pre>"},{"location":"reference/#pyhbr.common.get_saved_files_by_name","title":"<code>get_saved_files_by_name(name, save_dir)</code>","text":"<p>Get all saved data files matching name</p> <p>Get the list of files in the save_dir folder matching name. Return the result as a table of file path, commit hash, and saved date. The table is sorted by timestamp, with the most recent file first.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If save_dir does not exist, or there are files in save_dir within invalid file names (not in the format name_commit_timestamp.pkl).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the saved file to load. This matches name in the filename name_commit_timestamp.pkl.</p> required <code>save_dir</code> <code>str</code> <p>The directory to search for files.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe with columns <code>path</code>, <code>commit</code> and <code>created_data</code>.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def get_saved_files_by_name(name: str, save_dir: str) -&gt; DataFrame:\n    \"\"\"Get all saved data files matching name\n\n    Get the list of files in the save_dir folder matching\n    name. Return the result as a table of file path, commit\n    hash, and saved date. The table is sorted by timestamp,\n    with the most recent file first.\n\n    Raises:\n        RuntimeError: If save_dir does not exist, or there are files\n            in save_dir within invalid file names (not in the format\n            name_commit_timestamp.pkl).\n\n    Args:\n        name: The name of the saved file to load. This matches name in\n            the filename name_commit_timestamp.pkl.\n        save_dir: The directory to search for files.\n\n    Returns:\n        A dataframe with columns `path`, `commit` and `created_data`.\n    \"\"\"\n\n    # Check for missing datasets directory\n    if not os.path.isdir(save_dir):\n        raise RuntimeError(\n            f\"Missing folder '{save_dir}'. Check your working directory.\"\n        )\n\n    # Read all the .pkl files in the directory\n    files = DataFrame({\"path\": os.listdir(save_dir)})\n\n    # Identify the file name part. The horrible regex matches the\n    # expression _[commit_hash]_[timestamp].pkl. It is important to\n    # match this part, because \"anything\" can happen in the name part\n    # (including underscores and letters and numbers), so splitting on\n    # _ would not work. The name can then be removed.\n    files[\"name\"] = files[\"path\"].str.replace(\n        r\"_([0-9]|[a-zA-Z])*_\\d*\\.pkl\", \"\", regex=True\n    )\n\n    # Remove all the files whose name does not match, and drop\n    # the name from the path\n    files = files[files[\"name\"] == name]\n    if files.shape[0] == 0:\n        raise ValueError(\n            f\"There is not dataset with the name '{name}' in the datasets directory\"\n        )\n    files[\"commit_and_timestamp\"] = files[\"path\"].str.replace(name + \"_\", \"\")\n\n    # Split the commit and timestamp up (note also the extension)\n    try:\n        files[[\"commit\", \"timestamp\", \"extension\"]] = files[\n            \"commit_and_timestamp\"\n        ].str.split(r\"_|\\.\", expand=True)\n    except Exception as exc:\n        raise RuntimeError(\n            \"Failed to parse files in the datasets folder. \"\n            \"Ensure that all files have the correct format \"\n            \"name_commit_timestamp.(rds|pkl), and \"\n            \"remove any files not matching this \"\n            \"pattern. TODO handle this error properly, \"\n            \"see save_datasets.py.\"\n        ) from exc\n\n    files[\"created_date\"] = to_datetime(files[\"timestamp\"].astype(int), unit=\"s\")\n    recent_first = files.sort_values(by=\"timestamp\", ascending=False).reset_index()[\n        [\"path\", \"commit\", \"created_date\"]\n    ]\n    return recent_first\n</code></pre>"},{"location":"reference/#pyhbr.common.load_item","title":"<code>load_item(name, interactive=False, save_dir='save_data')</code>","text":"<p>Load a previously saved item from file</p> <p>Use this function to load a file that was previously saved using save_item(). By default, the latest version of the item will be returned (the one with the most recent timestamp).</p> <p>None is returned if an interactive load is cancelled by the user.</p> <p>To load an item that is an object from a library (e.g. a pandas DataFrame), the library must be installed (otherwise you will get a ModuleNotFound exception). However, you do not have to import the library before calling this function.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the item to load</p> required <code>interactive</code> <code>bool</code> <p>If True, let the user pick which item version to load interactively. If False, non-interactively load the most recent item (i.e. with the most recent timestamp). The commit hash is not considered when loading the item.</p> <code>False</code> <code>save_fir</code> <p>Which folder to load the item from.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The python object loaded from file, or None for an interactive load that is cancelled by the user.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def load_item(name: str, interactive: bool = False, save_dir: str = \"save_data\") -&gt; Any:\n    \"\"\"Load a previously saved item from file\n\n    Use this function to load a file that was previously saved using\n    save_item(). By default, the latest version of the item will be returned\n    (the one with the most recent timestamp).\n\n    None is returned if an interactive load is cancelled by the user.\n\n    To load an item that is an object from a library (e.g. a pandas DataFrame),\n    the library must be installed (otherwise you will get a ModuleNotFound\n    exception). However, you do not have to import the library before calling this\n    function.\n\n    Args:\n        name: The name of the item to load\n        interactive: If True, let the user pick which item version to load interactively.\n            If False, non-interactively load the most recent item (i.e. with the most\n            recent timestamp). The commit hash is not considered when loading the item.\n        save_fir: Which folder to load the item from.\n\n    Returns:\n        The python object loaded from file, or None for an interactive load that\n            is cancelled by the user.\n\n    \"\"\"\n    if interactive:\n        item_path = pick_saved_file_interactive(name, save_dir)\n    else:\n        item_path = pick_most_recent_saved_file(name, save_dir)\n\n    if item_path is None:\n        print(\"Aborted (interactive) load item\")\n        return\n\n    print(f\"Loading {item_path}\")\n\n    # Load a generic pickle. Note that if this is a pandas dataframe,\n    # pandas must be installed (otherwise you will get module not found).\n    # The same goes for a pickle storing an object from any other library.\n    with open(item_path, \"rb\") as file:\n        return pickle.load(file)\n</code></pre>"},{"location":"reference/#pyhbr.common.make_engine","title":"<code>make_engine(con_string='mssql+pyodbc://dsn', database='hic_cv_test')</code>","text":"<p>Make a sqlalchemy engine</p> <p>This function is intended for use with Microsoft SQL Server. The preferred method to connect to the server on Windows is to use a Data Source Name (DSN). To use the default connection string argument, set up a data source name called \"dsn\" using the program \"ODBC Data Sources\".</p> <p>If you need to access multiple different databases on the same server, you will need different engines. Specify the database name while creating the engine (this will override a default database in the DSN, if there is one).</p> <p>Parameters:</p> Name Type Description Default <code>con_string</code> <code>str</code> <p>The sqlalchemy connection string.</p> <code>'mssql+pyodbc://dsn'</code> <code>database</code> <code>str</code> <p>The database name to connect to.</p> <code>'hic_cv_test'</code> <p>Returns:</p> Type Description <code>Engine</code> <p>The sqlalchemy engine</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def make_engine(\n    con_string: str = \"mssql+pyodbc://dsn\", database: str = \"hic_cv_test\"\n) -&gt; Engine:\n    \"\"\"Make a sqlalchemy engine\n\n    This function is intended for use with Microsoft SQL\n    Server. The preferred method to connect to the server\n    on Windows is to use a Data Source Name (DSN). To use the\n    default connection string argument, set up a data source\n    name called \"dsn\" using the program \"ODBC Data Sources\".\n\n    If you need to access multiple different databases on the\n    same server, you will need different engines. Specify the\n    database name while creating the engine (this will override\n    a default database in the DSN, if there is one).\n\n    Args:\n        con_string: The sqlalchemy connection string.\n        database: The database name to connect to.\n\n    Returns:\n        The sqlalchemy engine\n    \"\"\"\n    connect_args = {\"database\": database}\n    return create_engine(con_string, connect_args=connect_args)\n</code></pre>"},{"location":"reference/#pyhbr.common.pick_most_recent_saved_file","title":"<code>pick_most_recent_saved_file(name, save_dir)</code>","text":"<p>Get the path to the most recent file matching name.</p> <p>Like pick_saved_file_interactive, but automatically selects the most recent file in save_data.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the saved file to list</p> required <code>save_dir</code> <code>str</code> <p>The directory to search for files</p> required <p>Returns:</p> Type Description <code>str</code> <p>The absolute path to the most recent matching file.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def pick_most_recent_saved_file(name: str, save_dir: str) -&gt; str:\n    \"\"\"Get the path to the most recent file matching name.\n\n    Like pick_saved_file_interactive, but automatically selects the most\n    recent file in save_data.\n\n    Args:\n        name: The name of the saved file to list\n        save_dir: The directory to search for files\n\n    Returns:\n        The absolute path to the most recent matching file.\n    \"\"\"\n    recent_first = get_saved_files_by_name(name, save_dir)\n    full_path = os.path.join(save_dir, recent_first.loc[0, \"path\"])\n    return full_path\n</code></pre>"},{"location":"reference/#pyhbr.common.pick_saved_file_interactive","title":"<code>pick_saved_file_interactive(name, save_dir)</code>","text":"<p>Select a file matching name interactively</p> <p>Print a list of the saved items in the save_dir folder, along with the date and time it was generated, and the commit hash, and let the user pick which item should be loaded interactively. The full filename of the resulting file is returned, which can then be read by the user.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the saved file to list</p> required <code>save_dir</code> <code>str</code> <p>The directory to search for files</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>The absolute path to the interactively selected file, or None if the interactive load was aborted.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def pick_saved_file_interactive(name: str, save_dir: str) -&gt; str | None:\n    \"\"\"Select a file matching name interactively\n\n    Print a list of the saved items in the save_dir folder, along\n    with the date and time it was generated, and the commit hash,\n    and let the user pick which item should be loaded interactively.\n    The full filename of the resulting file is returned, which can\n    then be read by the user.\n\n    Args:\n        name: The name of the saved file to list\n        save_dir: The directory to search for files\n\n    Returns:\n        The absolute path to the interactively selected file, or None\n            if the interactive load was aborted.\n    \"\"\"\n\n    recent_first = get_saved_files_by_name(name, save_dir)\n    print(recent_first)\n\n    num_datasets = recent_first.shape[0]\n    while True:\n        try:\n            raw_choice = input(\n                f\"Pick a dataset to load: [{0} - {num_datasets-1}] (type q[uit]/exit, then Enter, to quit): \"\n            )\n            if \"exit\" in raw_choice or \"q\" in raw_choice:\n                return None\n            choice = int(raw_choice)\n        except Exception:\n            print(f\"{raw_choice} is not valid; try again.\")\n            continue\n        if choice &lt; 0 or choice &gt;= num_datasets:\n            print(f\"{choice} is not in range; try again.\")\n            continue\n        break\n\n    full_path = os.path.join(save_dir, recent_first.loc[choice, \"path\"])\n    return full_path\n</code></pre>"},{"location":"reference/#pyhbr.common.requires_commit","title":"<code>requires_commit()</code>","text":"<p>Check whether changes need committing</p> <p>To make most effective use of the commit hash stored with a save_item call, the current branch should be clean (all changes committed). Call this function to check.</p> <p>Returns False if there is no git repository.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the working directory is in a git repository that requires a commit; False otherwise.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def requires_commit() -&gt; bool:\n    \"\"\"Check whether changes need committing\n\n    To make most effective use of the commit hash stored with a\n    save_item call, the current branch should be clean (all changes\n    committed). Call this function to check.\n\n    Returns False if there is no git repository.\n\n    Returns:\n        True if the working directory is in a git repository that requires\n            a commit; False otherwise.\n    \"\"\"\n    try:\n        repo = Repo(search_parent_directories=True)\n        return repo.is_dirty(untracked_files=True)\n    except InvalidGitRepositoryError:\n        # No need to commit if not repository\n        return False\n</code></pre>"},{"location":"reference/#pyhbr.common.save_item","title":"<code>save_item(item, name, save_dir='save_data/', enforce_clean_branch=True)</code>","text":"<p>Save an item to a pickle file</p> <p>Saves a python object (e.g. a pandas DataFrame) dataframe in the save_dir folder, using a filename that includes the current timestamp and the current commit hash. Use load_item to retrieve the file.</p> <p>Important</p> <p>Ensure that <code>save_data/</code> (or your chosen <code>save_dir</code>) is added to the .gitignore of your repository to ensure sensitive data is not committed.</p> <p>By storing the commit hash and timestamp, it is possible to identify when items were created and what code created them. To make most effective use of the commit hash, ensure that you commit, and do not make any further code edits, before running a script that calls save_item (otherwise the commit hash will not quite reflect the state of the running code).</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Any</code> <p>The python object to saave (e.g. pandas DataFrame)</p> required <code>name</code> <code>str</code> <p>The name of the item. The filename will be created by adding a suffix for the current commit and the timestamp to show when the data was saved (format: <code>name_commit_timestamp.pkl</code>)</p> required <code>save_dir</code> <code>str</code> <p>Where to save the data, relative to the current working directory. The directory will be created if it does not exist.</p> <code>'save_data/'</code> <code>enforce_clean_branch</code> <p>If True, the function will raise an exception if an attempt is made to save an item when the repository has uncommitted changes.</p> <code>True</code> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def save_item(\n    item: Any, name: str, save_dir: str = \"save_data/\", enforce_clean_branch=True\n) -&gt; None:\n    \"\"\"Save an item to a pickle file\n\n    Saves a python object (e.g. a pandas DataFrame) dataframe in the save_dir\n    folder, using a filename that includes the current timestamp and the current\n    commit hash. Use load_item to retrieve the file.\n\n    !!! important\n        Ensure that `save_data/` (or your chosen `save_dir`) is added to the\n        .gitignore of your repository to ensure sensitive data is not committed.\n\n    By storing the commit hash and timestamp, it is possible to identify when items\n    were created and what code created them. To make most effective use of the\n    commit hash, ensure that you commit, and do not make any further code edits,\n    before running a script that calls save_item (otherwise the commit hash will\n    not quite reflect the state of the running code).\n\n    Args:\n        item: The python object to saave (e.g. pandas DataFrame)\n        name: The name of the item. The filename will be created by adding\n            a suffix for the current commit and the timestamp to show when the\n            data was saved (format: `name_commit_timestamp.pkl`)\n        save_dir: Where to save the data, relative to the current working directory.\n            The directory will be created if it does not exist.\n        enforce_clean_branch: If True, the function will raise an exception if an attempt\n            is made to save an item when the repository has uncommitted changes.\n    \"\"\"\n\n    if enforce_clean_branch and requires_commit():\n        raise RuntimeError(\n            \"Aborting save_item() because branch is not clean. Commit your changes before saving item to increase the chance of reproducing the item based on the filename commit hash.\"\n        )\n\n    if not os.path.isdir(save_dir):\n        print(f\"Creating missing folder '{save_dir}' for storing item\")\n        os.mkdir(save_dir)\n\n    # Make the file suffix out of the current git\n    # commit hash and the current time\n    filename = f\"{name}_{current_commit()}_{current_timestamp()}.pkl\"\n    path = os.path.join(save_dir, filename)\n\n    with open(path, \"wb\") as file:\n        pickle.dump(item, file)\n</code></pre>"}]}