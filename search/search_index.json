{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the PyHBR Docs","text":"<p>This python package contains tools relating to estimating the risk of bleeding/ischaemia risk in cardiology patients. The main content of the package is a set of scripts for generating a report containing models of bleeding and ischaemia risk.</p>"},{"location":"#obtaining-the-code","title":"Obtaining the code","text":"<p>The code for the python package <code>pyhbr</code> is available at this GitHub repository, and the repository also contains the folder required for generating the report (not part of the Python package). The folder containing the report template is called <code>report</code>.</p> <p>After setting up a python environment (on Windows, install Python here -- you do not need admin rights), you can download <code>pyhbr</code> either from Pip using <code>pip install pyhbr</code>, or from the git repository directly if you want to make changes to the package. In that case, read the instructions in the README.</p> <p>To test that <code>pyhbr</code> is installed, open a console (i.e. a terminal, not an interactive Python prompt), and run <code>fetch-data -h</code>. If this produces a help message (instead of something like <code>command not found</code>) then <code>pyhbr</code> is installed correctly.</p> <p>A separate program is used to create files of ICD-10 and OPCS-4 code groups (with names like <code>icd10.yaml</code> and <code>opcs4.yaml</code>). This program is contained in a GitHub repository under the <code>codes_editor</code> folder, and the program is available to download from the releases page.</p>"},{"location":"#running-the-code","title":"Running the code","text":"<p>The models are run using five scripts: </p> <ul> <li><code>fetch-data</code>, which fetches tables from the database and extracts index events, outcomes, and features.</li> <li><code>plot-describe</code>, which creates descriptive plots of the data (including features and outcomes).</li> <li><code>run-model</code>, which fits a primary model for bleeding and ischaemia and other bootstrap models for stability.</li> <li><code>make-results</code>, which takes the fitted models and creates plots of model fit, stability and calibration</li> <li><code>generate-report</code>, which creates a Quarto report in source code form containing the figures and results generated from the other scripts. If you have Quarto installed, this script can also render the report into PDF and Word format.</li> </ul> <p>All scripts take a flag <code>-h</code> for help, which shows other flags, and all scripts require a YAML config file passed using <code>-f</code>. In the <code>report</code> folder, this file is called <code>icb_hic.yaml</code> and contains all the settings required to generate the models/report.</p> <p>All data and results are saved into a folder referred to as <code>save_dir</code>, which is set to <code>save_data</code>. All datasets, models, figures and other results will be stored in this folder, which should be gitignored. Items are saved with a timestamp and a git commit if the scripts are run from inside a git repository.</p>"},{"location":"#fetch-data","title":"<code>fetch-data</code>","text":"<p>The two purposes of this script are to fetch the data from the database, and process it into index events and features. The first time it is run, use:</p> <pre><code># The -q flag runs the queries. The raw data are saved into the save_data\nfetch-data -f icb_hic.yaml -q\n</code></pre> <p>The script will log to a file <code>icb_hic_fetch_data_sql_{timestamp}.log</code> for the SQL-query part, and log to <code>icb_hic_fetch_data_process_{timestamp}.log</code> for the index, features, etc.</p> <p>The raw data (fetched from the SQL queries) is saved to a file <code>icb_hic_raw_{commit}_{timestamp}.pkl</code></p> <p>Passing <code>-q</code> will not stop the script processing the data into index events and features. This part is logged to the file <code>icb_hic_fetch_data_process_{timestamp}.log</code>. The main data output from the script is saved to the file <code>icb_hic_data_{commit}_{timestamp}.pkl</code>. </p> <p>On subsequent runs, to speed things up, you can run <code>fetch-data -f icb_hic.yaml</code> (without <code>-q</code>). This will load the latest raw data from the <code>save_data</code> folder instead of getting it from the SQL server.</p> <p>Extract CSV files from data</p> <p>You can get CSV file versions of the DataFrames saved by <code>fetch-data</code> using the <code>get-csv</code> script (run <code>get-csv -h</code> for help). For example, to get tables from the <code>icb_hic_data_{commit}_{timestamp}.pkl</code> files, run <code>get-csv -f icb_hic.yaml -n data</code>. The <code>-n data</code> argument is important, and specifies what file you want to load. You only need to specify the <code>name</code> part of the file. To get this, strip off the <code>analysis_name</code> from the front (<code>icb_hic_</code> in this case, see <code>icb_hic.yaml</code>), and the commit/timestamp information (<code>_{commit}_{timestamp}.pkl</code>) from the end.</p> <p>If you have multiple files with different timestamps (e.g. because you ran <code>fetch-data</code> multiple times), you will be prompted interactively for which file you want to load.</p> <p>The <code>get-csv</code> script can be used to access DataFrames from any data file containing a dictionary mapping strings to DataFrames (this is most files).</p> <p>The tables contained in the output data from the <code>fetch-data</code> script are:</p> <ul> <li><code>index_spells</code>: The list of index ACS/PCI spells .</li> <li><code>code_groups</code>: The table of diagnosis/procedure code groups defined by the <code>icd10.yaml</code> and <code>opcs4.yaml</code> files.</li> <li><code>codes</code>: Which diagnosis/procedure codes occurred in patient episodes.</li> <li><code>episodes</code>: Patient episodes, including age on admission and gender.</li> <li><code>outcomes</code>: Bleeding/ischaemia fatal and non-fatal outcomes within one year (boolean).</li> <li><code>non_fatal_bleeding</code>/<code>fatal_bleeding</code>/<code>non_fatal_ischaemia</code>/<code>fatal_ischaemia</code>: Details of the outcomes.</li> <li><code>bleeding_survival</code>/<code>ischaemia_survival</code>: Data about when the outcomes occurred within one year</li> <li><code>features_index</code>/<code>features_codes</code>/<code>features_attributes</code>/<code>features_prescriptions</code>/<code>features_measurements</code>/<code>features_secondary_prescriptions</code>/<code>features_lab</code>: Features derived from each dataset.</li> <li><code>arc_hbr_score</code>: The ARC HBR score for each patient, including components.</li> </ul>"},{"location":"#plot-describe","title":"<code>plot-describe</code>","text":"<p>This script is run after the <code>fetch-data</code> script as follows:</p> <pre><code># Plots are saved in the save_data folder\nplot-describe -f icb_hic.yaml\n</code></pre> <p>A log of this script is saved to <code>icb_hic_plot_decsribe_{timestamp}.log</code>. To view the plots instead of saving them, add the <code>-p</code> flag to the line above. Each plot will be draw one after the other -- to move onto the next plot, mouse-over the plot and press <code>q</code>.</p>"},{"location":"#run-model","title":"<code>run-model</code>","text":"<p>The list of models to run are described under the <code>models</code> key in the <code>icb_hic.yaml</code> file. Although it is possible to run all the models at once, it is easier and quicker to run them one-at-a-time by passing the <code>-m</code> argument as follows:</p> <pre><code>run-model -f icb_hic.yaml -m logistic_regression\n</code></pre> <p>Each run of the script saves results in the <code>save_data</code> directory. The script writes to a log called <code>icb_hic_run_model_{timestamp}.log</code>. </p> <p>The model results are saved to a file called <code>icb_hic_{model_name}_{commit}_{timestamp}.pkl</code>, which stores the training and test sets (features and outcomes), and the model predictions.</p> <p>Before fitting the models, the training set is resampled to create a number of bootstrap datasets (the number is configured in <code>icb_hic.yaml</code>). The model is fitted on each of these bootstrap models to assess model stability. The probabilities for bleeding and ischaemia predicted by each bootstrap model is accessible in CSV format by running:</p> <pre><code>get_csv -f icb_hic.yaml -n logistic_regression\n</code></pre> <p>The relevant DataFrames have names like <code>fit_results_probs_{outcome_name}</code>, and have one column for each bootstrap model. The first column (<code>prob_M0</code>) is the primary model, and is fitted using the original training set (not a resample of it).</p> <p>The calibration and ROC curves are also available as CSV files; these are indexed by the bootstrap model (0 for the primary model, &gt;0 for bootstraps).</p>"},{"location":"#make_results","title":"<code>make_results</code>","text":"<p>This script takes the output from the fitted model and plots the graphs. You can run it as follows, on a per-model basis:</p> <pre><code># Plot the results for logistic_regression interactively\nmake-results -f icb_hic.yaml -m logistic_regression\n</code></pre> <p>Passing the <code>-m</code> parameter means the plots are shown interactively (cancel by pressing <code>q</code>), and not saved. It is useful to be able to switch between <code>run-model</code> and <code>make-results</code> to test the models, without having to wait for all models to run.</p> <p>To save the model plots, remove the <code>-m</code> parameter. This will cause the script to loop through all the models in the <code>icb_hic.yaml</code> file, plot all the figures, and save them all in the <code>save_data</code> directory. It is necessary to have run all the models first before doing this (otherwise errors will occur due to missing model files).</p>"},{"location":"#generate-report","title":"<code>generate-report</code>","text":"<p>This script generates the source code for a Quarto report in the <code>build</code> folder. To generate and also render the report, run:</p> <pre><code># Use -r to render the report\ngenerate-report -f icb_hic.yaml -r\n</code></pre> <p>To make the script faster, omit the <code>-r</code> to skip the render step. This will produce just the source code for the report, which is easier to use while modifying the report.</p> <p>The report text is generated using the files inside the <code>template</code> folder. These are Jinja2 templates, which contain markup like <code>{{ variable }}</code> to replace part of the text with a variable. You can edit these files to change the text of the report.</p> <p>All files used by the report are copied from the <code>save_data</code> folder to the <code>build</code> folder, so that the Quarto report is self-contained. There is no Python code inside the Quarto report, meaning it can be rendered using just Quarto without needing Python to be installed.</p> <p>The rationale for generating report source code is to allow flexibility in editing the document without needing to rerun Python code. In addition, figures are referred to by file name, which contains the timestamp and commit hash when the file was generated.</p>"},{"location":"arc_hbr/","title":"ARC HBR Calculation","text":"<p>The ARC HBR score is a concensus-based risk score to identify patients at high bleeding risk. It is formed from a weighted sum of 13 criteria, involving patient characteristics, patient history, laboratory test results, prescriptions information, and planned healthcare activity. This page describes how the ARC HBR score is calculated and assessed in PyHBR.</p>"},{"location":"arc_hbr/#steps-to-calculate-the-arc-hbr-score","title":"Steps to Calculate the ARC HBR Score","text":"<p>Using source data that includes diagnosis/procedure codes, laboratory tests, and prescriptions information, the ARC HBR score can be calculated as follows:</p>"},{"location":"arc_hbr/#preprocessing-steps","title":"Preprocessing Steps","text":"<p>Before identifying patients/index events, or calculating the score, some preprocessing is required. This is the section that is likely to require modification if a new data source is used.</p> <ol> <li>For diagnosis and procedure codes, add the group they belong to, and drop codes not in any group. Retain the code, the group, and the code position in long format in a table. The following groups are required:<ul> <li><code>acs</code>: Diagnosis codes corresponding to acute coronary syndromes (includes myocardial infarction/unstable angina). Used to identify index events.</li> <li><code>pci</code>: Percutaneous coronary intervention codes (e.g. stent implantation). Used to identify index events.</li> <li><code>ckd</code>: Chronic kidney disease: includes N18.5, N18.4, N18.3, N18.2, N18.1. Used as a fall-back in case eGFR is missing.</li> <li><code>anaemia</code>: Used as a fall-back in case Hb measurement is missing.</li> <li><code>bleeding</code>: Used to identify prior bleeding.</li> <li><code>cbd</code>: Chronic bleeding diatheses.</li> <li><code>cph</code>: Cirrhosis with portal hypertension.</li> <li><code>cancer_diag</code>: Used to identify cancer diagnoses.</li> <li><code>cancer_proc</code>: Used to identify cancer therapy.</li> <li><code>bavm</code>: Brain arteriovenous malformation diagnoses.</li> <li><code>istroke</code>: Ischaemic stroke.</li> <li><code>ich</code>: Intracranial haemorrhage.</li> <li><code>csurgery</code>: Cardiac surgery. Used to exclude cardiac surgery for one criterion.</li> <li><code>surgery</code>: All surgery. Used as a proxy for \"major surgery\" criteria</li> </ul> </li> <li> <p>For laboratory results, narrow to the subset of results shown below. Convert all tests to the standard unit used in the ARC definition, and drop the unit from the table. Keep the date/time at which the laboratory sample was collected, and the patient ID (in this data, associated episode is not linked, and must be inferred from the date).</p> <ul> <li><code>egfr</code>: Used to assess kidney function (unit: mL/min)</li> <li><code>hb</code>: Haemoglobin, used to assess anaemia (unit: g/dL)</li> <li><code>platelets</code>: Platelet count, used to assess thrombocytopenia (unit: <code>x10^9/L</code>)</li> </ul> </li> <li> <p>For prescriptions, narrow to the set of medicines shown below. Keep the medicine name, flag for present-on-admission, patient ID, and prescription order date (used to infer link to episode, as above).</p> <ul> <li><code>oac</code>: any of warfarin, apixaban, edoxaban, dabigatran, rivaroxaban</li> <li><code>nsaid</code>: any of ibuprofen, naproxen, diclofenac, celecoxib, mefenamic acid, etoricoxib, indomethacin (high-does aspirin excluded for now).</li> </ul> </li> <li> <p>For demographics, retain age and gender. This calculation may be postoned until after index events are calculated (for example, if the demographics table contains year of birth instead of age).</p> </li> </ol> <p>NOTE: The <code>episodes</code>, <code>prescriptions</code>, and <code>lab_results</code> tables have <code>episode_id</code> as Pandas index. The <code>demographics</code> table uses <code>patient_id</code> as index. The <code>episodes</code> table contains <code>patient_id</code> as a column for linking to <code>demographics</code>.</p>"},{"location":"arc_hbr/#link-laboratory-results-and-prescriptions-to-episodes","title":"Link Laboratory Results and Prescriptions to Episodes","text":"<p>In the HIC data, laboratory results and prescriptions do not contain an episode_id; instead, they contain a date/time (either a sample date for laboratory tests or an order date for prescriptions).</p> <p>To link each test/prescription to an episode, use the episode start and end times. If the sample date/order date falls within the episode start and end time, then it should be associated with that episode.</p> <p>A complication with this process is that episodes sometimes overlap (i.e. the start time of the next is before the end time of the previous one). This will be solved by associating a test/prescription with the earliest episode containing the time.</p>"},{"location":"arc_hbr/#identify-index-events","title":"Identify Index Events","text":"<p>Inclusion criteria for calculation of the ARC HBR score is having a hospital visit (spell) where the first episode of the spell contains an ACS diagnosis in the primary position, or a PCI procedure in any position.</p> <p>The table is indexed by the episode ID, and contains flag columns <code>acs_index</code> for <code>pci_index</code> for which inclusion condition is satisfied.</p> <p>NOTE: The <code>index_event</code> table is indexed by <code>episode_id</code>, and also contains the <code>patient_id</code> as a column.</p>"},{"location":"arc_hbr/#calculating-the-score","title":"Calculating the Score","text":"<p>The score is calculated differently for each different class of critera:</p>"},{"location":"data_sources/","title":"Data Sources","text":"<p>This page documents the raw data sources that have been used as the basis for this analysis. </p>"},{"location":"data_sources/#uhbw-data-sources","title":"UHBW Data Sources","text":""},{"location":"data_sources/#bnssg-icb-data-sources","title":"BNSSG ICB Data Sources","text":"<p>Access to regional patient hospital episode statistics and primary care data was available from the Bristol, North Somerset and South Gloucestershire Integrated Care Board (BNSSG ICB). </p>"},{"location":"design/","title":"PyHBR Design","text":"<p>This section describes the design of PyHBR and how the code is structured.</p>"},{"location":"design/#data-sources-and-analysis","title":"Data Sources and Analysis","text":"<p>The package contains routines for performing data analysis and fitting models. The source data for this analysis are tables stored in Microsoft SQL Server.</p> <p>In order to make the models reusable, the analysis/model code expects the tables in a particular format, which is documented for each analysis/model script. The code for analysis is in the <code>pyhbr.analysis</code> module.</p> <p>The database query and data fetch is performed by separate code, which is expected to be modified to port this package to a new data source. These data collection scripts are stored in the <code>pyhbr.data_source</code> module.</p> <p>A middle preprocessing layer <code>pyhbr.middle</code> is used to converted raw data from the data sources into the form expected by analysis. This helps keep the raw data sources clean (there is no need for extensive transformations in the SQL layer).</p>"},{"location":"design/#sql-queries","title":"SQL Queries","text":"<p>The approach taken to prepare SQL statements is to use SQLAlchemy to prepare a query, and then pass it to Pandas read_sql for execution. The advantage of using SQLAlchemy statements instead of raw strings in <code>read_sql</code> is the ability to construct statements using a declarative syntax (including binding parameters), and increased opportunity for error checking (which may be useful for porting the scripts to new databases).</p> <p>An example of how SQL statements are built is shown below:</p> <pre><code>from sqlalchemy import select\nfrom pyhbr.common import make_engine, CheckedTable\nimport pandas as pd\nimport datetime as dt\n\n# All interactions with the database (including building queries,\n# which queries the server to check columns) needs an sqlalchemy \n# engine\nengine = make_engine()\n\n# The CheckedTable is a simple wrapper around sqlalchemy.Table,\n# for the purpose of checking for missing columns. It replaces\n# sqlalchemy syntax table.c.column_name with table.col(\"column_name\")\ntable = CheckedTable(\"cv1_episodes\", engine)\n\n# The SQL statement is built up using the sqlalchemy select function.\n# The declarative syntax reduces the chance of errors typing a raw\n# SQL string. This line will throw an error if any of the columns are\n# wrong.\nstmt = select(\n    table.col(\"subject\").label(\"patient_id\"),\n    table.col(\"episode_identifier\").label(\"episode_id\"),\n    table.col(\"spell_identifier\").label(\"spell_id\"),\n    table.col(\"episode_start_time\").label(\"episode_start\"),\n    table.col(\"episode_end_time\").label(\"episode_end\"),\n).where(\n    table.col(\"episode_start_time\") &gt;= dt.date(2000, 1, 1),\n    table.col(\"episode_end_time\") &lt;= dt.date(2005, 1, 1)\n)\n\n# The SQL query can be printed for inspection if required,\n# or for using directly in a SQL script\nprint(stmt)\n\n# Otherwise, execute the query using pandas to get a dataframe\ndf = pd.read_sql(stmt, engine)\n</code></pre> <p>See the <code>pyhbr.data_source</code> module for more examples of functions that return the <code>stmt</code> variable for different tables.</p> <p>The following are some tips for building statements using the <code>CheckedTable</code> object:</p> <ul> <li><code>CheckedTable</code> contains the SQLAlchemy <code>Table</code> as the <code>table</code> member. This means you can use <code>select(table.table)</code> to initially fetch all the columns (useful for seeing what the table contains)</li> <li>If you need to rename a column (using <code>AS</code> in SQL), use <code>label</code>; e.g. <code>select(table.col(\"old_name\").label(\"new_name\"))</code>.</li> <li>Sometimes (particularly with ID columns which are typed incorrectly), it is useful to be able to cast to a different type. You can do this using <code>select(table.col(\"col_to_cast\").cast(String))</code>. The list of generic types is provided here; import the one you need using a line like <code>from sqlalchemy import String</code>.</li> </ul>"},{"location":"design/#middle-layer","title":"Middle Layer","text":"<p>To account for differences in data sources and the analysis, the module <code>pyhbr.middle</code> contains modules like <code>from_hic</code> which contain function that return transformed versions of the data sources more suitable for analysis.</p> <p>The outputs from this layer are documented here so that it is possible to take a new data source and write a new module in <code>pyhbr.middle</code> which exposes the new data source for analysis. These tables are grouped together into classes and (where the table name is used as the attribute name) used as the argument to analysis functions. Analysis functions may not use all the columns of each table, but when a column is present it should have the name and meaning given below.</p> <p>All tables are Pandas DataFrames.</p>"},{"location":"design/#episodes","title":"Episodes","text":"<p>Episodes correspond to individual consultant interactions within a hospital visit (called a spell). Episode information is stored in a table called <code>episodes</code>, which has the following columns:</p> <ul> <li><code>episode_id</code> (<code>str</code>, Pandas index): uniquely identifies the episode.</li> <li><code>patient_id</code> (<code>str</code>): the unique identifier for the patient.</li> <li><code>spell_id</code> (<code>str</code>): identifies the spell containing the episode.</li> <li><code>episode_start</code> (<code>datetime.date</code>): the episode start time.</li> <li><code>age</code> (<code>float64</code>): The patient age at the start of the episode. </li> <li><code>gender</code> (<code>category</code>): One of \"male\", \"female\", or \"unknown\". </li> </ul> <p>Even though age and gender are demographic properties, it is convenient to keep them in the episodes table because they are eventually stored with index events, which come directly from episodes.</p> <p>Note</p> <p>Consider filtering the episodes table based on a date range of interest when it is fetched from the data source. This will speed up subsequent processing.</p>"},{"location":"design/#code-groups","title":"Code Groups","text":"<p>A table of code groups is required as input to functions to identify patients based on aspects of their coded history. This table is called <code>code_groups</code>, and has the following structure:</p> <ul> <li><code>code</code> (<code>str</code>): The ICD-10 or OPCS-4 code in normalised form (e.g. \"i200\")</li> <li><code>codes</code> (<code>str</code>): The description of the code</li> <li><code>group</code> (<code>str</code>): The name of the code group containing the code</li> <li><code>type</code> (<code>category</code>): One of \"diagnosis\" (for ICD-10 codes) or \"procedure\" (for OPCS-4 codes)</li> </ul> <p>Only codes which are in a code group are included in the table. Codes may be present in multiple rows if they occur in multiple groups.</p>"},{"location":"design/#codes","title":"Codes","text":"<p>Episodes contain clinical code data, which lists the diagnoses made and the procedures performed in an episode. This is stored in a table called <code>codes</code>, with the following columns:</p> <ul> <li><code>episode_id</code> (<code>str</code>): which episode contains this clinical code.</li> <li><code>code</code> (<code>str</code>): the clinical code, all lowercase with no whitespace or dot, e.g. <code>i212</code></li> <li><code>position</code> (<code>int</code>): the position of the code in the episode. 1 means the primary position (e.g. for a primary diagnosis), and &gt;1 means a secondary code. Often episodes contain 5-10 clinical codes, and the maximum number depends on the data source.</li> <li><code>type</code> (<code>category</code>): either \"diagnosis\" (for ICD-10 diagnosis codes) or \"procedure\" (for OPCS-4 codes)</li> <li><code>group</code> (<code>str</code>): which group contains this clinical code.</li> </ul> <p>The Pandas index is a unique integer (note that <code>episode_id</code> is not unique, since a single episode can contain many codes).</p> <p>Note</p> <p>This table only contains codes that are in a code group (i.e. the function making <code>codes</code> should filter out codes not in any group; the <code>group</code> column is not NaN). If all codes are required, make a code group \"all\" which contains every code. Note that codes occupy multiple rows in the <code>codes</code> table if they are in more than one group (take care when counting rows). In these cases, a duplicate code is identified by having the same <code>code</code>, <code>position</code> and <code>type</code> values, but a different group.</p>"},{"location":"design/#demographics","title":"Demographics","text":"<p>Demographic information is stored in a table called <code>demographics</code>, which has the following columns:</p> <ul> <li><code>patient_id</code> (<code>str</code>, Pandas index): the unique patient identifier</li> <li><code>gender</code> (<code>category</code>): One of \"male\", \"female\", or \"unknown\". </li> </ul>"},{"location":"design/#laboratory-tests","title":"Laboratory Tests","text":"<p>Write me please</p>"},{"location":"design/#prescriptions","title":"Prescriptions","text":"<p>Write me please</p>"},{"location":"design/#collections-of-dataframes","title":"Collections of DataFrames","text":"<p>Once the data source has been converted into the standard form described above, multiple tables are collected together into a dictionary mapping strings to Pandas DataFrames. The value of the keys matches the table name in the sections above.</p>"},{"location":"design/#datamodelanalysis-save-points","title":"Data/Model/Analysis Save Points","text":"<p>To support saving intermediate results of calculations, <code>pyhbr.common</code> includes two functions <code>save_item</code> and <code>load_item</code>, which save a Python object to a directory (by default <code>save_data/</code> in your working directory).</p> <p>The scripts in hbr_uhbw use these functions to create these checkpoints:</p> <ul> <li>Data: After fetching data from databases or data sources and converting it into the raw format suitable for modelling or analysis. These files have <code>_data</code> in the file name. This data is then loaded again for modelling or analysis</li> <li>Model: After training models using the data stored in the <code>_data</code> files. These files have <code>_model</code> in the file name. This data is loaded for analysis.</li> <li>Analysis: After performing analysis using the <code>_data</code> or <code>_model</code> files. These files have <code>_analysis</code> in the file name. This data can be loaded and used to generate reports/outputs.</li> </ul> <p>Splitting up the scripts in this way makes them easier to develop, because each of the three parts above can take quite long to run.</p> <p>Multiple objects can be saved under one file by including them in a dictionary. It is up to the script to determine the format of the items being saved and loaded.</p> <p>Warning</p> <p>By default, <code>save_item</code> puts the saved files into a directory called <code>save_data/</code> relative to your current working directory. Ensure that this is added to the .gitignore if the files contain sensitive data, to avoid committing them to your repository.</p>"},{"location":"design/#saving-data","title":"Saving Data","text":"<p>In addition to saving the item, <code>pyhbr.common.save_item</code> also includes in the file name:</p> <ul> <li>The commit hash of the git repository at the time <code>save_item</code> is called. This is intended to make it easier to reproduce the state of the repository that generated the file. By default, <code>save_item</code> requires you to commit any changes before saving a file. (The cleanest/most reproducible thing to do is commit changes, and then run a script non-interactively from the top.) If you are not using a git repository, then \"nogit\" is used in place of the commit hash.</li> <li>The timestamp when <code>save_item</code> was called, which is more granular than the commit hash (or useful in case you do not have a git repository).</li> </ul> <p>Note</p> <p>You can save multiple items with the same name, because the file names will use different timestamps. By default, <code>load_item</code> will load the most recently saved file with a given name.</p> <p>The <code>save_item</code> function is shown below. The simplest way to call it is <code>save_item(df, \"my_df\")</code>, which will save the DataFrame <code>df</code> to the directory <code>save_data/</code> using the name \"my_df\".</p> Use this function to save a Python object (e.g. a DataFrame) <code></code> <code>save_item(item, name, save_dir='save_data/', enforce_clean_branch=True, prompt_commit=False)</code> <p>Save an item to a pickle file</p> <p>Saves a python object (e.g. a pandas DataFrame) dataframe in the save_dir folder, using a filename that includes the current timestamp and the current commit hash. Use load_item to retrieve the file.</p> <p>Important</p> <p>Ensure that <code>save_data/</code> (or your chosen <code>save_dir</code>) is added to the .gitignore of your repository to ensure sensitive data is not committed.</p> <p>By storing the commit hash and timestamp, it is possible to identify when items were created and what code created them. To make most effective use of the commit hash, ensure that you commit, and do not make any further code edits, before running a script that calls save_item (otherwise the commit hash will not quite reflect the state of the running code).</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Any</code> <p>The python object to save (e.g. pandas DataFrame)</p> required <code>name</code> <code>str</code> <p>The name of the item. The filename will be created by adding a suffix for the current commit and the timestamp to show when the data was saved (format: <code>name_commit_timestamp.pkl</code>)</p> required <code>save_dir</code> <code>str</code> <p>Where to save the data, relative to the current working directory. The directory will be created if it does not exist.</p> <code>'save_data/'</code> <code>enforce_clean_branch</code> <p>If True, the function will raise an exception if an attempt is made to save an item when the repository has uncommitted changes.</p> <code>True</code> <code>prompt_commit</code> <p>if enforce_clean_branch is true, choose whether the prompt the user to commit on an unclean branch. This can help avoiding losing the results of a long-running script. Prefer to use false if the script is cheap to run.</p> <code>False</code> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def save_item(\n    item: Any,\n    name: str,\n    save_dir: str = \"save_data/\",\n    enforce_clean_branch=True,\n    prompt_commit=False,\n) -&gt; None:\n    \"\"\"Save an item to a pickle file\n\n    Saves a python object (e.g. a pandas DataFrame) dataframe in the save_dir\n    folder, using a filename that includes the current timestamp and the current\n    commit hash. Use load_item to retrieve the file.\n\n    !!! important\n        Ensure that `save_data/` (or your chosen `save_dir`) is added to the\n        .gitignore of your repository to ensure sensitive data is not committed.\n\n    By storing the commit hash and timestamp, it is possible to identify when items\n    were created and what code created them. To make most effective use of the\n    commit hash, ensure that you commit, and do not make any further code edits,\n    before running a script that calls save_item (otherwise the commit hash will\n    not quite reflect the state of the running code).\n\n    Args:\n        item: The python object to save (e.g. pandas DataFrame)\n        name: The name of the item. The filename will be created by adding\n            a suffix for the current commit and the timestamp to show when the\n            data was saved (format: `name_commit_timestamp.pkl`)\n        save_dir: Where to save the data, relative to the current working directory.\n            The directory will be created if it does not exist.\n        enforce_clean_branch: If True, the function will raise an exception if an attempt\n            is made to save an item when the repository has uncommitted changes.\n        prompt_commit: if enforce_clean_branch is true, choose whether the prompt the\n            user to commit on an unclean branch. This can help avoiding losing\n            the results of a long-running script. Prefer to use false if the script\n            is cheap to run.\n    \"\"\"\n\n    if enforce_clean_branch:\n\n        abort_msg = \"Aborting save_item() because branch is not clean. Commit your changes before saving item to increase the chance of reproducing the item based on the filename commit hash.\"\n\n        if prompt_commit:\n            # If the branch is not clean, prompt the user to commit to avoid losing\n            # long-running model results. Take care to only commit if the state of\n            # the repository truly reflects what was run (i.e. if no changes were made\n            # while the script was running).\n            while requires_commit():\n                print(abort_msg)\n                print(\n                    \"You can commit now and then retry the save after committing.\"\n                )\n                retry_save = query_yes_no(\n                    \"Do you want to retry the save? Commit, then select yes, or choose no to abort the save.\"\n                )\n\n                if not retry_save:\n                    print(f\"Aborting save of {name}\")\n                    return\n\n            # If we get out the loop without returning, then the branch\n            # is not clean and the save can proceed.\n            print(\"Branch now clean, proceeding to save\")\n\n        else:\n\n            if requires_commit():\n                # In this case, unconditionally throw an error\n                raise RuntimeError(abort_msg)\n\n    if not Path(save_dir).exists():\n        print(f\"Creating missing folder '{save_dir}' for storing item\")\n        Path(save_dir).mkdir(parents=True, exist_ok=True)\n\n    path = make_new_save_item_path(name, save_dir, \"pkl\")\n    with open(path, \"wb\") as file:\n        print(f\"Saving {str(path)}\")\n        pickle.dump(item, file)\n</code></pre>"},{"location":"design/#loading-data","title":"Loading Data","text":"<p>To load a previously saved item, using <code>pyhbr.common.load_item</code>. It can be called most simply using <code>load_item(\"my_df\")</code>, assuming you previously saved an object in the default directory (<code>save_data</code>) with the name \"my_df\". By default, the most recent item is loaded, but using <code>load_item(\"my_df\", True)</code> will let you pick which file you want to load.</p> <p>The function <code>load_item</code> is shown below:</p> Use this function to load a previously saved Python object <code></code> <code>load_item(name, interactive=False, save_dir='save_data')</code> <p>Load a previously saved item (pickle) from file</p> <p>Use this function to load a file that was previously saved using save_item(). By default, the latest version of the item will be returned (the one with the most recent timestamp).</p> <p>None is returned if an interactive load is cancelled by the user.</p> <p>To load an item that is an object from a library (e.g. a pandas DataFrame), the library must be installed (otherwise you will get a ModuleNotFound exception). However, you do not have to import the library before calling this function.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the item to load</p> required <code>interactive</code> <code>bool</code> <p>If True, let the user pick which item version to load interactively. If False, non-interactively load the most recent item (i.e. with the most recent timestamp). The commit hash is not considered when loading the item.</p> <code>False</code> <code>save_fir</code> <p>Which folder to load the item from.</p> required <p>Returns:</p> Type Description <code>(Any, Path)</code> <p>A tuple, with the python object loaded from file as first element and the Path to the item as the second element, or None if the user cancelled an interactive load.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def load_item(\n    name: str, interactive: bool = False, save_dir: str = \"save_data\"\n) -&gt; (Any, Path):\n    \"\"\"Load a previously saved item (pickle) from file\n\n    Use this function to load a file that was previously saved using\n    save_item(). By default, the latest version of the item will be returned\n    (the one with the most recent timestamp).\n\n    None is returned if an interactive load is cancelled by the user.\n\n    To load an item that is an object from a library (e.g. a pandas DataFrame),\n    the library must be installed (otherwise you will get a ModuleNotFound\n    exception). However, you do not have to import the library before calling this\n    function.\n\n    Args:\n        name: The name of the item to load\n        interactive: If True, let the user pick which item version to load interactively.\n            If False, non-interactively load the most recent item (i.e. with the most\n            recent timestamp). The commit hash is not considered when loading the item.\n        save_fir: Which folder to load the item from.\n\n    Returns:\n        A tuple, with the python object loaded from file as first element and the\n            Path to the item as the second element, or None if the user cancelled\n            an interactive load.\n\n    \"\"\"\n    if interactive:\n        item_path = pick_saved_file_interactive(name, save_dir, \"pkl\")\n    else:\n        item_path = pick_most_recent_saved_file(name, save_dir, \"pkl\")\n\n    if item_path is None:\n        print(\"Aborted (interactive) load item\")\n        return None, None\n\n    print(f\"Loading {item_path}\")\n\n    # Load a generic pickle. Note that if this is a pandas dataframe,\n    # pandas must be installed (otherwise you will get module not found).\n    # The same goes for a pickle storing an object from any other library.\n    with open(item_path, \"rb\") as file:\n        return pickle.load(file), item_path\n</code></pre>"},{"location":"design/#clinical-codes","title":"Clinical Codes","text":"<p>PyHBR has functions for creating and using lists of ICD-10 and OPCS-4 codes. A prototype version of the graphical program to create the code lists was written in Tauri here. However, it is simpler and more portable to have the codes editor bundled in this python package, and written in python.</p> <p>Users should be able to do the following things with the codes editor GUI:</p> <ul> <li>Open the GUI program, and select a blank ICD-10 or OPCS-4 codes tree to begin creating code groups.</li> <li>Create new code groups starting from the blank template.</li> <li>Search for strings within the ICD-10/OPCS-4 descriptions to make creation of groups easier.</li> <li>Save the resulting codes file to a working directory.</li> <li>Open and edit a previously saved codes file from a working directory.</li> </ul> <p>Once the groups have been defined, the user should be able to perform the following actions with the code groups files:</p> <ul> <li>Import codes files from the package (i.e. predefined code groups).</li> <li>Import codes files (containing custom groups) from a working directory.</li> <li>Extract the code groups, and show which codes are in which groups.</li> <li>Use the code groups in analysis (i.e. get a Pandas DataFrame showing which codes are in which groups)</li> </ul> <p>Multiple code groups are stored in a single file, which means that only two codes files are necessary: <code>icd10-yaml</code> and <code>opcs4.yaml</code>. There is no limit to the number of code groups.</p> <p>Previously implemented functionality to check whether a clinical code is valid will not be implemented here, because sufficiently performant code cannot be written in pure python (and this package is intended to contain only pure Python to maximise portability).</p> <p>Instead, all codes are converted to a standard \"normal form\" where upper-case letters are replaced with lower-case, and dots/whitespace is removed. Codes can then be compared, and most codes will match under this condition. (Codes that will not match include those with suffixes, such as dagger or asterix, or codes that contain further qualifying suffixes that are not present in the codes tree.).</p>"},{"location":"design/#counting-codes","title":"Counting Codes","text":"<p>Diagnosis and procedure codes can be grouped together and used as features for building models. One way to do this is to count the codes in a particular time window (for example, one year before an index event), and use that as a predictor for subsequent outcomes.</p> <p>This sections describes how raw episode data is converted into this counted form in PyHBR.</p>"},{"location":"design/#getting-clinical-code-data","title":"Getting Clinical Code Data","text":"<p>Hospital episodes contain multiple diagnosis and procedure codes. The starting point for counting codes is using the <code>pyhbr.middle.*.get_clinical_codes</code> function, which returns a data frame with the following columns:</p> <ul> <li><code>episode_id</code>: Which episode the code was in</li> <li><code>code</code>: The name of the clinical code in normal form (lowercase, no whitespace/dots), e.g. \"n183\"</li> <li><code>group</code>: The group containing the code. The table only contains codes that are defined in a code group, which is based on the codes files from the previous section</li> <li><code>position</code>: The priority of the clinical code, where 1 means the primary diagnosis/procedure, and &gt; 1 means a secondary code.</li> <li><code>type</code>: Either \"diagnosis\" or \"procedure\" depending on the type of code.</li> </ul> <p>This table does not use <code>episode_id</code> as the index because a single episode ID often has many rows.</p> <p>An example of this function in <code>pyhbr.middle.from_hic</code> is:</p> Example function which fetches clinical codes <code></code> <code>get_clinical_codes(engine, diagnoses_file, procedures_file)</code> <p>Main diagnoses/procedures fetch for the HIC data</p> <p>This function wraps the diagnoses/procedures queries and a filtering operation to reduce the tables to only those rows which contain a code in a group. One table is returned which contains both the diagnoses and procedures in long format, along with the associated episode ID and the primary/secondary position of the code in the episode.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>The connection to the database</p> required <code>diagnoses_file</code> <code>str</code> <p>The diagnoses codes file name (loaded from the package)</p> required <code>procedures_file</code> <code>str</code> <p>The procedures codes file name (loaded from the package)</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A table containing diagnoses/procedures, normalised codes, code groups, diagnosis positions, and associated episode ID.</p> Source code in <code>src\\pyhbr\\middle\\from_hic.py</code> <pre><code>def get_clinical_codes(\n    engine: Engine, diagnoses_file: str, procedures_file: str\n) -&gt; pd.DataFrame:\n    \"\"\"Main diagnoses/procedures fetch for the HIC data\n\n    This function wraps the diagnoses/procedures queries and a filtering\n    operation to reduce the tables to only those rows which contain a code\n    in a group. One table is returned which contains both the diagnoses and\n    procedures in long format, along with the associated episode ID and the\n    primary/secondary position of the code in the episode.\n\n    Args:\n        engine: The connection to the database\n        diagnoses_file: The diagnoses codes file name (loaded from the package)\n        procedures_file: The procedures codes file name (loaded from the package)\n\n    Returns:\n        A table containing diagnoses/procedures, normalised codes, code groups,\n            diagnosis positions, and associated episode ID.\n    \"\"\"\n\n    diagnosis_codes = clinical_codes.load_from_package(diagnoses_file)\n    procedures_codes = clinical_codes.load_from_package(procedures_file)\n\n    # Fetch the data from the server\n    diagnoses = get_data(engine, hic.diagnoses_query)\n    procedures = get_data(engine, hic.procedures_query)\n\n    # Reduce data to only code groups, and combine diagnoses/procedures\n    filtered_diagnoses = clinical_codes.filter_to_groups(diagnoses, diagnosis_codes)\n    filtered_procedures = clinical_codes.filter_to_groups(procedures, procedures_codes)\n\n    # Tag the diagnoses/procedures, and combine the tables\n    filtered_diagnoses[\"type\"] = \"diagnosis\"\n    filtered_procedures[\"type\"] = \"procedure\"\n\n    codes = pd.concat([filtered_diagnoses, filtered_procedures])\n    codes[\"type\"] = codes[\"type\"].astype(\"category\")\n\n    return codes\n</code></pre>"},{"location":"design/#codes-in-other-episodes-relative-to-a-base-episode","title":"Codes in Other Episodes Relative to a Base Episode","text":"<p>To count up codes that occur in a time window before or after a particular base episode, it is necessary to join together each base episode with all the other episodes for the same patient.</p> <p>To do this, three tables are needed:</p> <ul> <li><code>base_episodes</code>: A table of the base episodes of interest, containing <code>episode_id</code> as an index.</li> <li><code>episodes</code>: A table of episode information (all episodes), which is indexed by <code>episode_id</code> and contains <code>patient_id</code> and <code>episode_start</code> as columns.</li> <li><code>codes</code>: The table of diagnosis/procedure codes from the previous section, containing a column <code>episode_id</code> and other code data columns.</li> </ul> <p>A function which combines these into a table containing all codes for other episodes relative to a base episode is <code>pyhbr.clinical_codes.counting.get_all_other_codes</code>:</p> Function that gets data from other episodes relative to a base episode <code></code> <code>get_all_other_codes(index_spells, episodes, codes)</code> <p>For each patient, get clinical codes in other episodes before/after the index</p> <p>This makes a table of index episodes (which is the first episode of the index spell) along with all other episodes for a patient. Two columns <code>index_episode_id</code> and <code>other_episode_id</code> identify the two episodes for each row (they may be equal), and other information is stored such as the time of the base episode, the time to the other episode, and clinical code information for the other episode.</p> <p>This table is used as the basis for all processing involving counting codes before and after an episode.</p> <p>Note</p> <p>Episodes will not be included in the result if they do not have any clinical     codes that are in any code group.</p> <p>Parameters:</p> Name Type Description Default <code>index_spells</code> <code>DataFrame</code> <p>Contains <code>episode_id</code> as an index.</p> required <code>episodes</code> <code>DataFrame</code> <p>Contains <code>episode_id</code> as an index, and <code>patient_id</code> and <code>episode_start</code> as columns</p> required <code>codes</code> <code>DataFrame</code> <p>Contains <code>episode_id</code> and other code data as columns</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A table containing columns <code>index_episode_id</code>, <code>other_episode_id</code>, <code>index_episode_start</code>, <code>time_to_other_episode</code>, and code data columns for the other episode. Note that the base episode itself is included as an other episode.</p> Source code in <code>src\\pyhbr\\clinical_codes\\counting.py</code> <pre><code>def get_all_other_codes(\n    index_spells: DataFrame, episodes: DataFrame, codes: DataFrame\n) -&gt; DataFrame:\n    \"\"\"For each patient, get clinical codes in other episodes before/after the index\n\n    This makes a table of index episodes (which is the first episode of the index spell)\n    along with all other episodes for a patient. Two columns `index_episode_id` and\n    `other_episode_id` identify the two episodes for each row (they may be equal), and\n    other information is stored such as the time of the base episode, the time to the\n    other episode, and clinical code information for the other episode.\n\n    This table is used as the basis for all processing involving counting codes before\n    and after an episode.\n\n    !!! note\n        Episodes will not be included in the result if they do not have any clinical\n            codes that are in any code group.\n\n    Args:\n        index_spells: Contains `episode_id` as an index.\n        episodes: Contains `episode_id` as an index, and `patient_id` and `episode_start` as columns\n        codes: Contains `episode_id` and other code data as columns\n\n    Returns:\n        A table containing columns `index_episode_id`, `other_episode_id`,\n            `index_episode_start`, `time_to_other_episode`, and code data columns\n            for the other episode. Note that the base episode itself is included\n            as an other episode.\n    \"\"\"\n\n    # Remove everything but the index episode_id (in case base_episodes\n    # already has the columns)\n    df = index_spells.reset_index(names=\"spell_id\").set_index(\"episode_id\")[\n        [\"spell_id\"]\n    ]\n\n    index_episode_info = df.merge(\n        episodes[[\"patient_id\", \"episode_start\"]], how=\"left\", on=\"episode_id\"\n    ).rename(\n        columns={\"episode_start\": \"index_episode_start\", \"spell_id\": \"index_spell_id\"}\n    )\n\n    other_episodes = (\n        index_episode_info.reset_index(names=\"index_episode_id\")\n        .merge(\n            episodes[[\"episode_start\", \"patient_id\", \"spell_id\"]].reset_index(\n                names=\"other_episode_id\"\n            ),\n            how=\"left\",\n            on=\"patient_id\",\n        )\n        .rename(columns={\"spell_id\": \"other_spell_id\"})\n    )\n\n    other_episodes[\"time_to_other_episode\"] = (\n        other_episodes[\"episode_start\"] - other_episodes[\"index_episode_start\"]\n    )\n\n    # Use an inner join to filter out other episodes that have no associated codes\n    # in any group.\n    with_codes = other_episodes.merge(\n        codes, how=\"inner\", left_on=\"other_episode_id\", right_on=\"episode_id\"\n    ).drop(columns=[\"patient_id\", \"episode_start\", \"episode_id\"])\n\n    return with_codes\n</code></pre>"},{"location":"design/#counting-codes-group-occurrences","title":"Counting Codes Group Occurrences","text":"<p>In any table that has ``</p>"},{"location":"modelling/","title":"Bleeding/Ischaemia Risk Modelling","text":"<p>Bleeding and ischaemia risk prediction models using PyHBR are trained to predict a bleeding and ischaemia outcome defined by clinical codes (ICD-10 and OPCS-4), and are trained on a number of different datasets. The methods used to develop and test the models is explained below.</p>"},{"location":"modelling/#data-sources","title":"Data Sources","text":"<p>Index events and outcomes are determined from diagnosis (ICD-10) and procedure (OPCS-4) codes in Hospital Episode Statistics (HES) data.</p> <p>In HES, hospital visits are structured into groups of episodes called spells, where each episode corresponds to a continuous period of care (usually) led by one consultant. Each consultant episode contains a list of diagnoses and procedures. Of these, one diagnosis and one procedure is marked as primary.</p> <p>The clinical coding guidelines<sup>1</sup> define the use of the primary diagnosis field in DGCS.1:</p> <p>Definition of Primary Diagnosis (extract from DGCS.1)</p> <ol> <li>The first diagnosis field(s) of the coded clinical record (the primary diagnosis) will contain the main condition treated or investigated during the relevant episode of healthcare.</li> <li>Where a definitive diagnosis has not been made by the responsible clinician the main symptom, abnormal findings, or problem should be recorded in the first diagnosis field of the coded clinical record.</li> </ol> <p>The primary diagnosis is not necessary the main clinical diagnosis; instead, it is the main diagnosis being treated in the current episode<sup>2</sup>. Secondary diagnoses are other diagnoses relevant to the current episode, but which are not the primary diagnosis.</p> <p>Note</p> <p>Secondary diagnoses may contain historical diagnoses if they are relevant to the current episode.</p> <p>Similarly, the primary procedure is defined as the main procedure that occurred in the episode<sup>3</sup>:</p> <p>Definition of Primary Procedure (extract from PRule 2)</p> <p>When classifying diagnostic information, the International Classification of Diseases and Health Related Problems (ICD) recommend criteria for the selection of the MAIN condition for single-cause analysis. OPCS-4 follows this precedent in that the intervention selected for single procedure analysis from records of episodes of hospital care should be the MAIN intervention or procedure carried out during the relevant episode which may not always be the first procedure performed during the consultant episode.</p> <p>Secondary procedures are other interventions that occurred in the consultant episode.</p>"},{"location":"modelling/#index-events","title":"Index Events","text":"<p>The target cohort for risk prediction is patients who present in hospital with an acute coronary syndrome (ACS), or receive a percutaneous coronary intervention (PCI), such as a stent implant; these groups are likely to be placed on a blood thinning medication such as dual antiplatelet therapy. </p> <p>To capture acute presentation for ACS, patients are included if the ACS diagnosis is listed as the primary diagnosis in any episode of the spell. This is to rule out episodes where a historical ACS is coded. A PCI is allowed in any primary or secondary position, on the assumption that inclusion of the procedure means that the procedure was performed.</p> <p>A UK Biobank report identifies a validated group of codes for identification of MI (both STEMI and NSTEMI) based on HES data, with PPV greater than 70% for each group<sup>4</sup>. However, the codes contain I25.2 (old myocardial infarction), which would capture patients in index events who do not necessarily have ACS at that time. This issue was addressed in a study validating ACS code groups in a French administrative database <sup>5</sup>. Of the different code groups they present, the I20.0, I21. and I24. was identified as a good compromise between validated ACS and PPV (84%).</p> <p>ALL OPCS-4 PCI codes are included, based on the list provided in Pathak et al. (2023).</p> <p>The code groups used to define the index event are shown below:</p> List of ACS and PCI codes used to define index events Category ICD-10 Description ACS I20.0 Unstable angina I21.* Acute myocardial infarction I24.* Other acute ischaemic heart diseases PCI K49.* Transluminal balloon angioplasty of coronary artery K50.* Other therapeutic transluminal operations on coronary artery K75.* Percutaneous transluminal balloon angioplasty and insertion of stent into coronary artery"},{"location":"modelling/#outcome-definition","title":"Outcome Definition","text":"<p>Bleeding and ischaemia outcomes are defined by looking for ICD-10 codes in the spells that occur after the index presentation, up to one year.</p>"},{"location":"modelling/#bleeding-outcome","title":"Bleeding Outcome","text":"<p>The bleeding outcome definition should map to a clinically relevant definition of bleeding for patients on anticoagulant therapy. One such definition is bleeding academic research consortium (BARC) 3 or 5 bleeding<sup>6</sup>, which is the basis of the high bleeding risk definition by the Academic Research Consortium (ARC)<sup>7</sup>.</p> <p>Many ICD-10-coded bleeding events may qualify for BARC-2, because being written explicitly in the patient notes (a criterion for coding) could imply that an \"overt\" bleed is present, or that the bleed is \"more than would be expected for the clinical circumstance\". </p> <p>Determining that a bleed qualifies for BARC-3 requires a haemoglobin drop due to the bleed of greater than 3 g/dL. This cannot be determined from an analysis of ICD-10 codes alone. </p> <p>Several attempts to capture \"severe\" bleeding from administrative codes exist in the literature. For example, one study identifies a group of ICD-10 codes with a particularly high positive predictive value (PPV; chance that a ICD-10-coded bleed is in fact a clinically relevant bleed), of 88%<sup>8</sup>. The list of ICD-10 codes is shown below:</p> List of major bleeding codes<sup>8</sup> Description ICD-10CM Codes Subarachnoid hemorrhage I60 Intracranial hemorrhage I61 Subdural hemorrhage I62 Upper gastrointestinal bleeding I85.0, K22.1, K22.6, K25.0, K25.2, K25.4, K25.6,K26.0, K26.2, K26.4, K26.6, K27.0, K27.2, K27.4,K27.6, K28.0, K28.2, K28.4, K28.6, K29.0, K31.80,K63.80, K92.0, K92.1, K92.2 Lower gastrointestinal bleeding K55.2, K51, K57, K62.5, K92.0, K92.1, K92.2 <p>This groups has been interpreted as the following set of (UK) ICD-10 codes:</p> ICD-10 Description I60 Subarachnoid haemorrhage I61 Intracerebral haemorrhage I62 Other nontraumatic intracranial haemorrhage I85.0 Oesophageal varices with bleeding K22.1 Ulcer of oesophagus K22.6 Gastro-oesophageal laceration-haemorrhage syndrome K25.0 Gastric ulcer : acute with haemorrhage K25.2 Gastric ulcer : acute with both haemorrhage and perforation K25.4 Gastric ulcer : chronic or unspecified with haemorrhage K25.6 Gastric ulcer : chronic or unspecified with both haemorrhage and perforation K26.0 Duodenal ulcer : acute with haemorrhage K26.2 Duodenal ulcer : acute with both haemorrhage and perforation K26.4 Duodenal ulcer : chronic or unspecified with haemorrhage K26.6 Duodenal ulcer : chronic or unspecified with both haemorrhage and perforation K27.0 Peptic ulcer, site unspecified : acute with haemorrhage K27.2 Peptic ulcer, site unspecified : acute with both haemorrhage and perforation K27.4 Peptic ulcer, site unspecified : chronic or unspecified with haemorrhage K27.6 Peptic ulcer, site unspecified : chronic or unspecified with both haemorrhage and perforation K28.0 Gastrojejunal ulcer : acute with haemorrhage K28.2 Gastrojejunal ulcer : acute with both haemorrhage and perforation K28.4 Gastrojejunal ulcer : chronic or unspecified with haemorrhage K28.6 Gastrojejunal ulcer : chronic or unspecified with both haemorrhage and perforation K29.0 Acute haemorrhagic gastritis K92.0 Haematemesis K92.1 Melaena K92.2 Gastrointestinal haemorrhage, unspecified K55.2 Angiodysplasia of colon K51 Ulcerative colitis K57 Diverticular disease of intestine K62.5 Haemorrhage of anus and rectum <p>The primary rationale for adopting such a code group would be that:</p> <ul> <li>It originates from a study where the PPV of the code group was measured (offsetting the risk that coded bleeding definitions do not correspond to real bleeds);</li> <li>The study qualifies the group as \"major\" bleeds (so that it might be taken as a reasonable proxy for BARC 3 or 5 bleeding).</li> </ul> <p>Disadvantages, however, include differences in coding between the UK and Canada (the location of the study), particularly the difference between ICD-10 (UK) and ICD-10CM (Canada). In addition, the presence of unqualified diverticulosis within the bleeding group is not directly a bleeding condition, and may significantly reduce the PPV in older patients.</p> <p>As a result, we define a bleeding outcome based on the BARC 2-5 criteria<sup>9</sup>.</p> Bleeding codes corresponding to BARC 2-5 Category ICD-10 Description Gastrointestinal I85.0 Oesophageal varices with bleeding K25.0 Gastric ulcer, acute with haemorrhage K25.2 Gastric ulcer, acute with both haemorrhage and perforation K25.4 Gastric ulcer, chronic or unspecified with haemorrhage K25.6 Chronic or unspecified with both haemorrhage and perforation K26.0 Duodenal ulcer, acute with haemorrhage K26.2 Duodenal ulcer, acute with both haemorrhage and perforation K26.4 Duodenal ulcer, chronic or unspecified with haemorrhage K26.6 Chronic or unspecified with both haemorrhage and perforation K27.0 Peptic ulcer, acute with haemorrhage K27.2 Peptic ulcer, acute with both haemorrhage and perforation K27.4 Peptic ulcer, chronic or unspecified with haemorrhage K27.6 Chronic or unspecified with both haemorrhage and perforation K28.0 Gastrojejunal ulcer, acute with haemorrhage K28.2 Acute with both haemorrhage and perforation K28.4 Gastrojejunal ulcer, chronic or unspecified with haemorrhage K28.6 Chronic or unspecified with both haemorrhage and perforation K29.0 Acute haemorrhagic gastritis K62.5 Haemorrhage of anus and rectum K66.1 Haemoperitoneum K92.0 Haematemesis K92.1 Melaena K92.2 Gastrointestinal haemorrhage, unspecified Intracerebral I60.* Subarachnoid haemorrhage I61.* Intracerebral haemorrhage I62.* Other nontraumatic intracranial haemorrhage I69.0 Sequelae of subarachnoid haemorrhage I69.1 Sequelae of intracerebral haemorrhage I69.2 Sequelae of other nontraumatic intracranial haemorrhage S06.4 Epidural haemorrhage Genitourinary N93.8 Other specified abnormal uterine and vaginal bleeding N93.9 Abnormal uterine and vaginal bleeding, unspecified Other R04.* Haemorrhage from respiratory passages I23.0 Haemopericardium as current complication following acute myocardial infarction <p>The group is generated based on UK ICD-10 data, which is likely to reduce coding discrepancies, and does not contain the generic diverticulosis category. </p> <p>No PPV is available for this code group. A basic chart review should be performed on the patients identified by this bleeding group to increase confidence that they match relevant bleeding events.</p> <p>A spell is considered to be a bleeding outcome if any episode of the spell contains a bleeding code in the primary position.</p> <p>Fatal bleeding (BARC 5) is included in the bleeding outcome. Mortality information is available from the Civil Registration of Deaths, which includes a primary cause of death and multiple secondary causes of death. A death is included in the bleeding outcome when the primary cause of death (an ICD-10 code) is an ADAPTT bleeding code as described above.</p>"},{"location":"modelling/#ischaemia-outcome","title":"Ischaemia Outcome","text":"<p>Various definitions of ischaemia outcomes are commonly used when deriving outcomes from administrative databases<sup>10</sup>. We require a definition of major adverse cardiovascular event (MACE) that uses ICD-10 codes, and includes only ischaemia outcomes (for example, excludes haemorrhagic stroke), due to the requirement for comparing bleeding and ischaemia models for the purposes of assessing a bleeding/ischaemia trade-off. The definition should also include cardiovascular mortality, to match the BARC-5 fatal bleeding included in the bleeding outcome definition.</p> <p>The best matching definition in Bosco et al. (2021)<sup>10</sup> (Table 1) is Ohm et al. (2018)<sup>11</sup>, because:</p> <ul> <li>It is simple (three-point), and includes only ischaemia outcomes (AMI, ischaemic stroke, and CV death)</li> <li>All codes used were fully defined;</li> <li>Codes used ICD-10 (instead of ICD-9).</li> </ul> <p>The code groups are defined as follows:</p> AMI, ischaemic stroke, and CV death components of MACE used for ischaemia outcomes Category ICD-10 Description AMI I21.* Acute myocardial infarction I22.* Subsequent myocardial infarction Ischaemic stroke I63.* Cerebral infarction CV death I46.1 Sudden cardiac death, so described I46.9 Cardiac arrest, unspecified I21.* (Fatal) acute myocardial infarction I22.* (Fatal) subsequent myocardial infarction I63.* (Fatal) ischaemic stroke <p>Although the reference only lists <code>I46.1</code> and <code>I46.9</code> as CV death, deaths with a primary cause of death of myocardial infarction or ischaemic stroke are also included in our CV death definition (<code>Fatal</code> is prepended to indicate that the code must be present in a cause-of-death ICD-10 code column).</p> <p>Disadvantages of this code group includes the lack of definition in whether the primary/secondary positions are used, and lack of validation. Similarly to the bleeding groups, basic validation may be performed by a chart analysis.</p>"},{"location":"modelling/#predictors","title":"Predictors","text":"<p>Clinical codes are used to define predictors, but an exclusion period of one month is applied to avoid using ICD-10 and OPCS-4 codes that would not have been coded yet before the index (clinical coding happens monthly, and clinical codes are not available until this processing has occurred). </p> <p>Depending on which data sources are used, other predictors may be available. For example, predictors in some models including primary care attribute flags, and physiological measurements such as HbA1c and blood pressure.</p>"},{"location":"modelling/#models","title":"Models","text":"<p>Classification models are trained on the bleeding and ischaemia outcomes within one year, which are binary (either an adverse event occurred or it did not). </p> <p>Both adverse outcomes have a low prevalence (0 - 20%). If individual patient risk is broadly localised at low values around the prevalence, it is not possible for any classification model to predict who will have adverse outcomes. Evidence for more determinstic outcomes, where a low number of patients have nearly 100% chance of adverse events and others have nearly 0% risk, would be provided by models that are able to predict adverse outcomes very well (i.e. with very high true negative/positive rates, and a very high ROC AUC).</p> <p>Since very high ROC AUCs are not observed, the former interpretation of risk distribution is more likely. In this case, the adverse outcome is considered a proxy for high risk or low risk (instead of outcome occurred or outcome did not occur), and the result may be used in two possible ways:</p> <ul> <li>The classification outcome itself may be utilised directly as an estimate of whether the patient is high or low risk.</li> <li>The continuous parameter underlying the classification model decision may be used as an estimate of continuous patient risk (i.e. between 0% and 100%).</li> </ul> <p>To test the accuracy of the model under the first interpretation, it is necessary to establish that patients estimated at high risk typically have more adverse events. In the second interpretation, estimates must be made of patients' true continuous risk, which requires an aggregate over patients in the testing set. This is discussed in the next section.</p>"},{"location":"modelling/#model-testing","title":"Model Testing","text":"<p>Models are assessed according to two criteria:</p> <ul> <li>Accuracy: Whether the model produces the correct risk classifications or estimates;</li> <li>Stability: Whether the model produces consistent risk classifications or estimates.</li> </ul> <p>Both these metrics are assessed by applying the models to data in the testing set, which is not used in the model fitting process. Accuracy is assessed through calibration plots, by grouping patients into similar-risk groups and comparing the prevalence in each group to the estimated risk.</p> <p>Stability is assessed by training multiple models on bootstrap resamples of the training set, and comparing the model outputs for each patient.</p>"},{"location":"modelling/#minimum-performance-criteria","title":"Minimum Performance Criteria","text":"<p>A minimum performance criteria for the models can be established by estimating the number of patients either positively or negatively affected by a change in intervention based on model risk estimates for bleeding and ischaemia.</p> <p>The simplest model to perform these calculations is hosted here. It assumes determinstic outcomes, meaning that all patients are assumed to be either 0% or 100% risk before an intervention is applied. More complex versions of the calculation could assume a more realistic distribution of patient risk, and/or allow for differences in the effect of the intervention depending on the real patient risk.</p> <p>The model can be used to understand possible benefit or harm caused by taking action based on the estimates from bleeding/ischaemia models, based on the background prevalence of the outcomes, the performance of the model, and the modifications in risk caused by the intervention.</p>"},{"location":"modelling/#limitations-of-administrative-data","title":"Limitations of Administrative Data","text":"<p>While use of ICD-10 and OPCS-4 coded patient data is useful because of its size and availability, there are caveats regarding its use for models intended directly for patient care.</p> <p>Coding is performed manually for financial purposes. There is evidence that internal inconsistencies in the data exist<sup>12</sup>, that may impact tools designed for direct patient care. This modification of the intended purpose of the dataset should be considered if the data is to be used in clinical decision support tools.</p> <p>There is a wide degree of choice involved in selecting code groups to define index events and outcomes[@bosco2022major], both of which directly determine what question a given model is addressing (rather than how accurate it is). It is not possible to assess how well the coding maps to clinical reality using any model metric; the only available method is chart review, which is not possible to perform directly using only HES data.</p> <p>In addition to variability in choice of code groups, variability also exists regarding which episode (or epsiodes) of a spell is used to define an event, and which primary/secondary positions should be utilised. There is no systematic consensus on how these choices should be made when creating model for patient-care purposes.</p> <p>Finally, it is hard to assess coding accuracy by comparing the prevalence of events with trial or study data, because trials and studies often introduce some bias due to the recruitment criteria causing a discrepancy compared to counts of events in the entire patient population.</p> <ol> <li> <p>National Clinical Coding Standards ICD-10 5th Edition. NHS England; 2023.\u00a0\u21a9</p> </li> <li> <p>Short Guide to Clinical Coding for Clinicians\u00a0\u21a9 <li> <p>National Clinical Coding Standards OPCS-4. NHS England; 2024.\u00a0\u21a9</p> </li> <li> <p>Biobank U. Definitions of acute myocardial infarction and main myocardial infarction pathological types: UK biobank phase 1 outcomes adjudication. 2017.\u00a0\u21a9</p> </li> <li> <p>Bezin J, Girodet PO, Rambelomanana S, Touya M, Ferreira P, Gilleron V, et al. Choice of ICD-10 codes for the identification of acute coronary syndrome in the french hospitalization database. Fundamental   &amp; clinical pharmacology. 2015;29(6):586\u201391.\u00a0\u21a9</p> </li> <li> <p>Mehran R, Rao SV, Bhatt DL, Gibson CM, Caixeta A, Eikelboom J, et al. Standardized bleeding definitions for cardiovascular clinical trials: A consensus report from the bleeding academic research consortium. Circulation [Internet]. 2011;123(23):2736\u201347. Available from: https://www.ahajournals.org/doi/10.1161/circulationaha.110.009449 \u21a9</p> </li> <li> <p>Urban P, Mehran R, Colleran R, Angiolillo DJ, Byrne RA, Capodanno D, et al. Defining high bleeding risk in patients undergoing percutaneous coronary intervention: A consensus document from the academic research consortium for high bleeding risk. Circulation [Internet]. 2019;140(3):240\u201361. Available from: https://www.ahajournals.org/doi/10.1161/CIRCULATIONAHA.119.040167 \u21a9</p> </li> <li> <p>Al-Ani F, Shariff S, Siqueira L, Seyam A, Lazo-Langner A. Identifying venous thromboembolism and major bleeding in emergency room discharges using administrative data. Thrombosis research [Internet]. 2015;136(6):1195\u20138. Available from: https://www.thrombosisresearch.com/article/S0049-3848(15)30167-5/abstract \u21a9\u21a9</p> </li> <li> <p>Pufulete M, Harris J, Sterne JA, Johnson TW, Lasserson D, Mumford A, et al. Comprehensive ascertainment of bleeding in patients prescribed different combinations of dual antiplatelet therapy (DAPT) and triple therapy (TT) in the UK: Study protocol for three population-based cohort studies emulating \u201ctarget trials\u201d(the ADAPTT study). BMJ open [Internet]. 2019;9(6):e029388. Available from: https://bmjopen.bmj.com/content/9/6/e029388 \u21a9</p> </li> <li> <p>Bosco E, Hsueh L, McConeghy KW, Gravenstein S, Saade E. Major adverse cardiovascular event definitions used in observational analysis of administrative databases: A systematic review. BMC Medical Research Methodology. 2021;21(1):1\u201318.\u00a0\u21a9\u21a9</p> </li> <li> <p>Ohm J, Skoglund PH, Discacciati A, Sundstr\u00f6m J, Hambraeus K, Jernberg T, et al. Socioeconomic status predicts second cardiovascular event in 29,226 survivors of a first myocardial infarction. European Journal of Preventive Cardiology. 2018;25(9):985\u201393.\u00a0\u21a9</p> </li> <li> <p>Hardy F, Heyl J, Tucker K, Hopper A, March textasciitilde a MJ, Briggs TW, et al. Data consistency in the english hospital episodes statistics database. BMJ Health   &amp; Care Informatics. 2022;29(1).\u00a0\u21a9</p> </li>"},{"location":"reference/","title":"PyHBR Function Reference","text":"<p>This page contains the documentation for all objects in PyHBR.</p>"},{"location":"reference/#data-sources","title":"Data Sources","text":""},{"location":"reference/#pyhbr.analysis","title":"<code>analysis</code>","text":"<p>Routines for performing statistics, analysis, or fitting models</p>"},{"location":"reference/#pyhbr.analysis.acs","title":"<code>acs</code>","text":""},{"location":"reference/#pyhbr.analysis.acs.filter_by_code_groups","title":"<code>filter_by_code_groups(episode_codes, code_group, max_position, exclude_index_spell)</code>","text":"<p>Filter based on matching code conditions occurring in other episodes</p> <p>From any table derived from get_all_other_episodes (e.g. the output of get_time_window), identify clinical codes (and therefore episodes) which correspond to an outcome of interest.</p> <p>The input table has one row per clinical code, which is grouped into episodes and spells by other columns. The outcome only contains codes that define an episode or spell as an outcome. The result from this function can be used to analyse the make-up of outcomes.</p> <p>Parameters:</p> Name Type Description Default <code>episode_codes</code> <code>DataFrame</code> <p>Table of other episodes to filter. This can be narrowed to either the previous or subsequent year, or a different time frame. (In particular, exclude the index event if required.) The table must contain these columns:</p> <ul> <li><code>other_episode_id</code>: The ID of the other episode     containing the code (relative to the index episode).</li> <li><code>other_spell_id</code>: The spell containing the other episode.</li> <li><code>group</code>: The name of the code group.</li> <li><code>type</code>: The code type, \"diagnosis\" or \"procedure\".</li> <li><code>position</code>: The position of the code (1 for primary, &gt; 1     for secondary).</li> <li><code>time_to_other_episode</code>: The time elapsed between the index     episode start and the other episode start.</li> </ul> required <code>code_group</code> <code>str</code> <p>The code group name used to identify outcomes</p> required <code>max_position</code> <code>int</code> <p>The maximum clinical code position that will be allowed to define an outcome. Pass 1 to allow primary diagnosis only, 2 to allow primary diagnosis and the first secondary diagnosis, etc.</p> required <code>exclude_index_spell</code> <code>bool</code> <p>Do not allow any code present in the index spell to define an outcome.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A series containing the number of code group occurrences in the other_episodes table.</p> Source code in <code>src\\pyhbr\\analysis\\acs.py</code> <pre><code>def filter_by_code_groups(\n    episode_codes: DataFrame,\n    code_group: str,\n    max_position: int,\n    exclude_index_spell: bool,\n) -&gt; DataFrame:\n    \"\"\"Filter based on matching code conditions occurring in other episodes\n\n    From any table derived from get_all_other_episodes (e.g. the\n    output of get_time_window), identify clinical codes (and\n    therefore episodes) which correspond to an outcome of interest.\n\n    The input table has one row per clinical code, which is grouped\n    into episodes and spells by other columns. The outcome only\n    contains codes that define an episode or spell as an outcome.\n    The result from this function can be used to analyse the make-up\n    of outcomes.\n\n    Args:\n        episode_codes: Table of other episodes to filter.\n            This can be narrowed to either the previous or subsequent\n            year, or a different time frame. (In particular, exclude the\n            index event if required.) The table must contain these\n            columns:\n\n            * `other_episode_id`: The ID of the other episode\n                containing the code (relative to the index episode).\n            * `other_spell_id`: The spell containing the other episode.\n            * `group`: The name of the code group.\n            * `type`: The code type, \"diagnosis\" or \"procedure\".\n            * `position`: The position of the code (1 for primary, &gt; 1\n                for secondary).\n            * `time_to_other_episode`: The time elapsed between the index\n                episode start and the other episode start.\n\n        code_group: The code group name used to identify outcomes\n        max_position: The maximum clinical code position that will be allowed\n            to define an outcome. Pass 1 to allow primary diagnosis only,\n            2 to allow primary diagnosis and the first secondary diagnosis,\n            etc.\n        exclude_index_spell: Do not allow any code present in the index\n            spell to define an outcome.\n\n    Returns:\n        A series containing the number of code group occurrences in the\n            other_episodes table.\n    \"\"\"\n\n    # Reduce to only the code groups of interest\n    df = episode_codes[episode_codes[\"group\"] == code_group]\n\n    # Keep only necessary columns\n    df = df[\n        [\n            \"index_spell_id\",\n            \"other_spell_id\",\n            \"code\",\n            \"docs\",\n            \"position\",\n            \"time_to_other_episode\",\n        ]\n    ]\n\n    # Optionally remove rows corresponding to the index spell\n    if exclude_index_spell:\n        df = df[~(df[\"other_spell_id\"] == df[\"index_spell_id\"])]\n\n    # Only keep codes that match the position-based inclusion criterion\n    df = df[df[\"position\"] &lt;= max_position]\n\n    return df\n</code></pre>"},{"location":"reference/#pyhbr.analysis.acs.get_code_features","title":"<code>get_code_features(index_spells, all_other_codes)</code>","text":"<p>Get counts of previous clinical codes in code groups before the index.</p> <p>Predictors derived from clinical code groups use clinical coding data from 365 days before the index to 30 days before the index (this excludes episodes where no coding data would be available, because the coding process itself takes approximately one month).</p> <p>All groups included anywhere in the <code>group</code> column of all_other_codes are included, and each one becomes a new column with \"_before\" appended.</p> <p>Parameters:</p> Name Type Description Default <code>index_spells</code> <code>DataFrame</code> <p>A table containing <code>spell_id</code> as Pandas index and a column <code>episode_id</code> for the first episode in the index spell.</p> required <code>all_other_codes</code> <code>DataFrame</code> <p>A table of other episodes (and their clinical codes) relative to the index spell, output from counting.get_all_other_codes.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A table with one column per code group, counting the number of codes in that group that appeared in the year before the index.</p> Source code in <code>src\\pyhbr\\analysis\\acs.py</code> <pre><code>def get_code_features(index_spells: DataFrame, all_other_codes: DataFrame) -&gt; DataFrame:\n    \"\"\"Get counts of previous clinical codes in code groups before the index.\n\n    Predictors derived from clinical code groups use clinical coding data from 365\n    days before the index to 30 days before the index (this excludes episodes where\n    no coding data would be available, because the coding process itself takes\n    approximately one month).\n\n    All groups included anywhere in the `group` column of all_other_codes are\n    included, and each one becomes a new column with \"_before\" appended.\n\n    Args:\n        index_spells: A table containing `spell_id` as Pandas index and a\n            column `episode_id` for the first episode in the index spell.\n        all_other_codes: A table of other episodes (and their clinical codes)\n            relative to the index spell, output from counting.get_all_other_codes.\n\n    Returns:\n        A table with one column per code group, counting the number of codes\n            in that group that appeared in the year before the index.\n    \"\"\"\n    code_groups = all_other_codes[\"group\"].unique()\n    max_position = 999  # Allow any primary/secondary position\n    exclude_index_spell = False\n    max_before = dt.timedelta(days=365)\n    min_before = dt.timedelta(days=30)\n\n    # Get the episodes that occurred in the previous year (for clinical code features)\n    previous_year = counting.get_time_window(all_other_codes, -max_before, -min_before)\n\n    code_features = {}\n    for group in code_groups:\n        group_episodes = filter_by_code_groups(\n            previous_year,\n            group,\n            max_position,\n            exclude_index_spell,\n        )\n        code_features[group + \"_before\"] = counting.count_code_groups(\n            index_spells, group_episodes\n        )\n\n    return DataFrame(code_features)\n</code></pre>"},{"location":"reference/#pyhbr.analysis.acs.get_index_attributes","title":"<code>get_index_attributes(swd_index_spells, primary_care_attributes)</code>","text":"<p>Link the primary care patient data to the index spells</p> <p>Parameters:</p> Name Type Description Default <code>swd_index_spells</code> <code>DataFrame</code> <p>Index_spells linked to a a recent, valid patient attributes row. Contains the columns <code>patient_id</code> and <code>date</code> for linking, and has Pandas index <code>spell_id</code>.</p> required <code>primary_care_attributes</code> <code>DataFrame</code> <p>The full attributes table.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The table of index-spell patient attributes, indexed by <code>spell_id</code>.</p> Source code in <code>src\\pyhbr\\analysis\\acs.py</code> <pre><code>def get_index_attributes(\n    swd_index_spells: DataFrame, primary_care_attributes: DataFrame\n) -&gt; DataFrame:\n    \"\"\"Link the primary care patient data to the index spells\n\n    Args:\n        swd_index_spells: Index_spells linked to a a recent, valid\n            patient attributes row. Contains the columns `patient_id` and\n            `date` for linking, and has Pandas index `spell_id`.\n        primary_care_attributes: The full attributes table.\n\n    Returns:\n        The table of index-spell patient attributes, indexed by `spell_id`.\n    \"\"\"\n\n    return (\n        (\n            swd_index_spells[[\"patient_id\", \"date\"]]\n            .reset_index()\n            .merge(\n                primary_care_attributes,\n                how=\"left\",\n                on=[\"patient_id\", \"date\"],\n            )\n        )\n        .set_index(\"spell_id\")\n        .drop(columns=[\"patient_id\", \"date\"])\n    )\n</code></pre>"},{"location":"reference/#pyhbr.analysis.acs.get_index_spells","title":"<code>get_index_spells(episodes, codes, acs_group, pci_group, stemi_group, nstemi_group, complex_pci_group)</code>","text":"<p>Get the index spells for ACS/PCI patients</p> <p>Index spells are defined by the contents of the first episode of the spell (i.e. the cause of admission to hospital). Spells are considered an index event if either of the following hold:</p> <ul> <li>The primary diagnosis of the first episode contains an   ACS ICD-10 code. This is to ensure that only episodes where the   main diagnosis of the episode is ACS are considered, and not   cases where a secondary ACS is present that could refer to a   historical event.</li> <li>There is a PCI procedure in any primary or secondary position   in the first episode of the spell. It is assumed that a procedure   is only coded in secondary positions if it did occur in that   episode.</li> </ul> <p>A prerequisite for spell to be an index spell is that it contains episodes present in both the episodes and codes tables. The episodes table contains start-time/spell information, and the codes table contains information about what diagnoses/procedures occurred in each episode.</p> <p>The table returned contains one row per index spell (and is indexed by spell id). It also contains other information about the index spell, which is derived from the first episode of the spell.</p> <p>Parameters:</p> Name Type Description Default <code>episodes</code> <code>DataFrame</code> <p>All patient episodes. Must contain <code>episode_id</code>, <code>spell_id</code> and <code>episode_start</code>, <code>age</code> and <code>gender</code>.</p> required <code>codes</code> <code>DataFrame</code> <p>All diagnosis/procedure codes by episode. Must contain <code>episode_id</code>, <code>position</code> (indexed from 1 which is the primary code, &gt;1 are secondary codes), and <code>group</code> (expected to contain the value of the acs_group and pci_group arguments).</p> required <code>acs_group</code> <code>str</code> <p>The name of the ICD-10 code group used to define ACS.</p> required <code>pci_group</code> <code>str | None</code> <p>The name of the OPCS-4 code group used to define PCI. Pass None to not use PCI as an inclusion criterion for index events. In this case, the pci_index column is omitted, and only ACS primary diagnoses are allowed.</p> required <code>stemi_group</code> <code>str</code> <p>The name of the ICD-10 code group used to identify STEMI MI</p> required <code>nstemi_group</code> <code>str</code> <p>The name of the ICD-10 code group used to identify NSTEMI MI</p> required <code>complex_pci_group</code> <code>str | None</code> <p>The name of the OPCS-4 code group used to define complex PCI (in any primary/secondary position)</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A table of index spells and associated information about the first episode of the spell.</p> Source code in <code>src\\pyhbr\\analysis\\acs.py</code> <pre><code>def get_index_spells(\n    episodes: DataFrame,\n    codes: DataFrame,\n    acs_group: str,\n    pci_group: str | None,\n    stemi_group: str,\n    nstemi_group: str,\n    complex_pci_group: str | None,\n) -&gt; DataFrame:\n    \"\"\"Get the index spells for ACS/PCI patients\n\n    Index spells are defined by the contents of the first episode of\n    the spell (i.e. the cause of admission to hospital). Spells are\n    considered an index event if either of the following hold:\n\n    * The primary diagnosis of the first episode contains an\n      ACS ICD-10 code. This is to ensure that only episodes where the\n      main diagnosis of the episode is ACS are considered, and not\n      cases where a secondary ACS is present that could refer to a\n      historical event.\n    * There is a PCI procedure in any primary or secondary position\n      in the first episode of the spell. It is assumed that a procedure\n      is only coded in secondary positions if it did occur in that\n      episode.\n\n    A prerequisite for spell to be an index spell is that it contains\n    episodes present in both the episodes and codes tables. The episodes table\n    contains start-time/spell information, and the codes table contains\n    information about what diagnoses/procedures occurred in each episode.\n\n    The table returned contains one row per index spell (and is indexed by\n    spell id). It also contains other information about the index spell,\n    which is derived from the first episode of the spell.\n\n    Args:\n        episodes: All patient episodes. Must contain `episode_id`, `spell_id`\n            and `episode_start`, `age` and `gender`.\n        codes: All diagnosis/procedure codes by episode. Must contain\n            `episode_id`, `position` (indexed from 1 which is the primary\n            code, &gt;1 are secondary codes), and `group` (expected to contain\n            the value of the acs_group and pci_group arguments).\n        acs_group: The name of the ICD-10 code group used to define ACS.\n        pci_group: The name of the OPCS-4 code group used to define PCI. Pass None\n            to not use PCI as an inclusion criterion for index events. In this\n            case, the pci_index column is omitted, and only ACS primary diagnoses\n            are allowed.\n        stemi_group: The name of the ICD-10 code group used to identify STEMI MI\n        nstemi_group: The name of the ICD-10 code group used to identify NSTEMI MI\n        complex_pci_group: The name of the OPCS-4 code group used to define complex\n            PCI (in any primary/secondary position)\n\n    Returns:\n        A table of index spells and associated information about the\n            first episode of the spell.\n    \"\"\"\n\n    # Index spells are defined by the contents of the first episode in the\n    # spell (to capture the cause of admission to hospital).\n    first_episodes = episodes.sort_values(\"episode_start\").groupby(\"spell_id\").head(1)\n\n    # In the codes dataframe, if one code is in multiple groups, it gets multiple\n    # (one per code group). Concatenate the code groups to reduce to one row per\n    # code, then use str.contains() later to identify code groups\n    reduced_codes = codes.copy()\n    non_group_cols = [c for c in codes.columns if c != \"group\"]\n    reduced_codes[\"group\"] = codes.groupby(non_group_cols)[\"group\"].transform(\n        lambda x: \",\".join(x)\n    )\n    reduced_codes = reduced_codes.drop_duplicates()\n\n    # Join the diagnosis/procedure codes. The inner join reduces to episodes which\n    # have codes in any group, which is a superset of the index episodes -- if an\n    # episode has no codes in any code group, it cannot be an index event.\n    first_episodes_with_codes = first_episodes.merge(\n        reduced_codes, how=\"inner\", on=\"episode_id\"\n    )\n\n    # ACS matches based on a primary diagnosis of ACS (this is to rule out\n    # cases where patient history may contain ACS recorded as a secondary\n    # diagnosis).\n    acs_match = (first_episodes_with_codes[\"group\"].str.contains(acs_group)) &amp; (\n        first_episodes_with_codes[\"position\"] == 1\n    )\n\n    # A PCI match is allowed anywhere in the procedures list, but must still\n    # be present in the first episode of the index spell.\n    if pci_group is not None:\n        pci_match = first_episodes_with_codes[\"group\"].str.contains(pci_group)\n    else:\n        pci_match = False\n\n    # Get all the episodes matching the ACS or PCI condition (multiple rows\n    # per episode)\n    matching_episodes = first_episodes_with_codes[acs_match | pci_match]\n    matching_episodes.set_index(\"episode_id\", drop=True, inplace=True)\n\n    index_spells = DataFrame()\n\n    # Reduce to one row per episode, and store a flag for whether the ACS\n    # or PCI condition was present. If PCI is none, there is no need for these\n    # columns because all rows are ACS index events\n    if pci_group is not None:\n        index_spells[\"pci_index\"] = (\n            matching_episodes[\"group\"].str.contains(pci_group).groupby(\"episode_id\").any()\n        )\n        index_spells[\"acs_index\"] = (\n            matching_episodes[\"group\"].str.contains(acs_group).groupby(\"episode_id\").any()\n        )\n\n    # The stemi/nstemi columns are always needed to distinguish the type of ACS. If \n    # both are false, the result is unstable angina\n    index_spells[\"stemi_index\"] = (\n        matching_episodes[\"group\"].str.contains(stemi_group).groupby(\"episode_id\").any()\n    )   \n    index_spells[\"nstemi_index\"] = (\n        matching_episodes[\"group\"]\n        .str.contains(nstemi_group)\n        .groupby(\"episode_id\")\n        .any()\n    )\n\n    # Check if the PCI is complex\n    if complex_pci_group is not None:\n        index_spells[\"complex_pci_index\"] = (\n            matching_episodes[\"group\"]\n            .str.contains(complex_pci_group)\n            .groupby(\"episode_id\")\n            .any()\n        )    \n\n    # Join some useful information about the episode\n    index_spells = (\n        index_spells.merge(\n            episodes[[\"patient_id\", \"episode_start\", \"spell_id\", \"age\", \"gender\"]],\n            how=\"left\",\n            on=\"episode_id\",\n        )\n        .rename(columns={\"episode_start\": \"spell_start\"})\n        .reset_index(\"episode_id\")\n        .set_index(\"spell_id\")\n    )\n\n    # Convert the age column to a float. This should probably\n    # be done upstream\n    index_spells[\"age\"] = index_spells[\"age\"].astype(float)\n\n    return index_spells\n</code></pre>"},{"location":"reference/#pyhbr.analysis.acs.get_management","title":"<code>get_management(index_spells, all_other_codes, min_after, max_after, pci_group, cabg_group)</code>","text":"<p>Get the management type for each index event</p> <p>The result is a category series containing \"PCI\" if a PCI was performed, \"CABG\" if CABG was performed, or \"Conservative\" if neither were performed.</p> <p>Parameters:</p> Name Type Description Default <code>index_spells</code> <code>DataFrame</code> required <code>all_other_codes</code> <code>DataFrame</code> <p>description</p> required <code>min_after</code> <code>timedelta</code> <p>The start of the window after the index to look for management</p> required <code>max_after</code> <code>timedelta</code> <p>The end of the window after the index which defines management</p> required <code>pci_group</code> <code>str</code> <p>The name of the code group defining PCI management</p> required <code>cabg_management</code> <p>The name of the code group defining CABG management</p> required <p>Returns:</p> Type Description <code>Series</code> <p>A category series containing \"PCI\", \"CABG\", or \"Conservative\"</p> Source code in <code>src\\pyhbr\\analysis\\acs.py</code> <pre><code>def get_management(\n    index_spells: DataFrame,\n    all_other_codes: DataFrame,\n    min_after: dt.timedelta,\n    max_after: dt.timedelta,\n    pci_group: str,\n    cabg_group: str,\n) -&gt; Series:\n    \"\"\"Get the management type for each index event\n\n    The result is a category series containing \"PCI\" if a PCI was performed, \"CABG\"\n    if CABG was performed, or \"Conservative\" if neither were performed.\n\n    Args:\n        index_spells:\n        all_other_codes (DataFrame): _description_\n        min_after: The start of the window after the index to look for management\n        max_after: The end of the window after the index which defines management\n        pci_group: The name of the code group defining PCI management\n        cabg_management: The name of the code group defining CABG management\n\n    Returns:\n        A category series containing \"PCI\", \"CABG\", or \"Conservative\"\n    \"\"\"\n\n    management_window = counting.get_time_window(all_other_codes, min_after, max_after)\n\n    # Ensure that rows are only kept if they are from the same spell (management\n    # must occur before a hospital discharge and readmission)\n    same_spell_management_window = management_window[\n        management_window[\"index_spell_id\"].eq(management_window[\"other_spell_id\"])\n    ]\n\n    def check_management_type(g):\n        if g.eq(cabg_group).any():\n            return \"CABG\"\n        elif g.eq(pci_group).any():\n            return \"PCI\"\n        else:\n            return \"Conservative\"\n\n    return (\n        same_spell_management_window.groupby(\"index_spell_id\")[[\"group\"]]\n        .agg(check_management_type)\n        .astype(\"category\")\n    )\n</code></pre>"},{"location":"reference/#pyhbr.analysis.acs.get_outcomes","title":"<code>get_outcomes(index_spells, all_other_codes, date_of_death, cause_of_death, non_fatal_group, fatal_group)</code>","text":"<p>Get non-fatal and fatal outcomes defined by code groups</p> <p>Parameters:</p> Name Type Description Default <code>index_spells</code> <code>DataFrame</code> <p>A table containing <code>spell_id</code> as Pandas index and a column <code>episode_id</code> for the first episode in the index spell.</p> required <code>all_other_codes</code> <code>DataFrame</code> <p>A table of other episodes (and their clinical codes) relative to the index spell, output from counting.get_all_other_codes.</p> required <code>date_of_death</code> <code>DataFrame</code> <p>Contains a column date_of_death, with Pandas index <code>patient_id</code></p> required <code>cause_of_death</code> <code>DataFrame</code> <p>Contains columns <code>patient_id</code>, <code>code</code> (ICD-10) for cause of death, <code>position</code> of the code, and <code>group</code>.</p> required <code>non_fatal_group</code> <code>str</code> <p>The name of the ICD-10 group defining the non-fatal outcome (the primary diagnosis of subsequent episodes are checked for codes in this group)</p> required <code>fatal_group</code> <code>str</code> <p>The name of the ICD-10 group defining the fatal outcome (the primary diagnosis in the cause-of-death is checked for codes in this group).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe, indexed by <code>spell_id</code> (i.e. the index spell), with columns <code>all</code> (which counts the total fatal and non-fatal outcomes), and <code>fatal</code> (which just contains the fatal outcome)</p> Source code in <code>src\\pyhbr\\analysis\\acs.py</code> <pre><code>def get_outcomes(\n    index_spells: DataFrame,\n    all_other_codes: DataFrame,\n    date_of_death: DataFrame,\n    cause_of_death: DataFrame,\n    non_fatal_group: str,\n    fatal_group: str,\n) -&gt; DataFrame:\n    \"\"\"Get non-fatal and fatal outcomes defined by code groups\n\n    Args:\n        index_spells: A table containing `spell_id` as Pandas index and a\n            column `episode_id` for the first episode in the index spell.\n        all_other_codes: A table of other episodes (and their clinical codes)\n            relative to the index spell, output from counting.get_all_other_codes.\n        date_of_death: Contains a column date_of_death, with Pandas index\n            `patient_id`\n        cause_of_death: Contains columns `patient_id`, `code` (ICD-10) for\n            cause of death, `position` of the code, and `group`.\n        non_fatal_group: The name of the ICD-10 group defining the non-fatal\n            outcome (the primary diagnosis of subsequent episodes are checked\n            for codes in this group)\n        fatal_group: The name of the ICD-10 group defining the fatal outcome\n            (the primary diagnosis in the cause-of-death is checked for codes\n            in this group).\n\n    Returns:\n        A dataframe, indexed by `spell_id` (i.e. the index spell), with columns\n            `all` (which counts the total fatal and non-fatal outcomes),\n            and `fatal` (which just contains the fatal outcome)\n    \"\"\"\n\n    # Follow-up time for fatal and non-fatal events\n    max_after = dt.timedelta(days=365)\n\n    # Properties of non-fatal events\n    primary_only = True\n    exclude_index_spell = False\n    first_episode_only = False\n    min_after = dt.timedelta(hours=48)\n\n    # Work out fatal outcome\n    fatal = get_fatal_outcome(\n        index_spells, date_of_death, cause_of_death, fatal_group, max_after\n    )\n\n    # Get the episodes (and all their codes) in the follow-up window\n    following_year = counting.get_time_window(all_other_codes, min_after, max_after)\n\n    # Get non-fatal outcome\n    outcome_episodes = filter_by_code_groups(\n        following_year,\n        [non_fatal_group],\n        primary_only,\n        exclude_index_spell,\n        first_episode_only,\n    )\n    non_fatal = counting.count_code_groups(index_spells, outcome_episodes)\n\n    return DataFrame({\"all\": non_fatal + fatal, \"fatal\": fatal})\n</code></pre>"},{"location":"reference/#pyhbr.analysis.acs.get_secondary_care_prescriptions_features","title":"<code>get_secondary_care_prescriptions_features(prescriptions, index_spells, episodes)</code>","text":"<p>Get dummy feature columns for OAC and NSAID medications on admission</p> <p>Parameters:</p> Name Type Description Default <code>prescriptions</code> <code>DataFrame</code> <p>The table of secondary care prescriptions, containing a <code>group</code> column and <code>spell_id</code>.</p> required <code>index_spells</code> <code>DataFrame</code> <p>The index spells, which must be indexed by <code>spell_id</code></p> required <code>episodes</code> <code>DataFrame</code> <p>The episodes table containing <code>admission</code> and <code>discharge</code>, for linking prescriptions to spells.</p> required Source code in <code>src\\pyhbr\\analysis\\acs.py</code> <pre><code>def get_secondary_care_prescriptions_features(\n    prescriptions: DataFrame, index_spells: DataFrame, episodes: DataFrame\n) -&gt; DataFrame:\n    \"\"\"Get dummy feature columns for OAC and NSAID medications on admission\n\n    Args:\n        prescriptions: The table of secondary care prescriptions, containing\n            a `group` column and `spell_id`.\n        index_spells: The index spells, which must be indexed by `spell_id`\n        episodes: The episodes table containing `admission` and `discharge`,\n            for linking prescriptions to spells.\n    \"\"\"\n\n    # Get all the data required\n    df = (\n        index_spells.reset_index(\"spell_id\")\n        .merge(prescriptions, on=\"patient_id\", how=\"left\")\n        .merge(episodes[[\"admission\", \"discharge\"]], on=\"episode_id\", how=\"left\")\n    )\n\n    # Keep only prescriptions ordered between admission and discharge\n    # marked as present on admission\n    within_spell = (df[\"order_date\"] &gt;= df[\"admission\"]) &amp; (\n        df[\"order_date\"] &lt;= df[\"discharge\"]\n    )\n\n    # Filter and create dummy variables for on-admission medication\n    dummies = (\n        pd.get_dummies(\n            df[within_spell &amp; df[\"on_admission\"]].set_index(\"spell_id\")[\"group\"]\n        )\n        .groupby(\"spell_id\")\n        .max()\n        .astype(int)\n    )\n\n    # Join back onto index events and set missing entries to zero\n    return index_spells[[]].merge(dummies, how=\"left\", on=\"spell_id\").fillna(0)\n</code></pre>"},{"location":"reference/#pyhbr.analysis.acs.get_survival_data","title":"<code>get_survival_data(index_spells, fatal, non_fatal, max_after)</code>","text":"<p>Get survival data from fatal and non-fatal outcomes</p> <p>Parameters:</p> Name Type Description Default <code>index_spells</code> <code>DataFrame</code> <p>The index spells, indexed by <code>spell_id</code></p> required <code>fatal</code> <code>DataFrame</code> <p>The table of fatal outcomes, containing a <code>survival_time</code> column</p> required <code>non_fatal</code> <code>DataFrame</code> <p>The table of non-fatal outcomes, containing a <code>time_to_other_episode</code> column</p> required <code>max_after</code> <code>timedelta</code> <p>The right censor time. This is the maximum time for data contained in the fatal and non_fatal tables; any index spells with no events in either table will be right-censored with this time.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The survival data containing both fatal and non-fatal events. The survival time is the <code>time_to_event</code> column, the <code>fatal</code> column contains a flag indicating whether the event was fatal, and the <code>right_censor</code> column indicates whether the survival time is censored. The <code>code</code> and <code>docs</code> column provide information about the type of event for non-censored data (NA otherwise).</p> Source code in <code>src\\pyhbr\\analysis\\acs.py</code> <pre><code>def get_survival_data(\n    index_spells: DataFrame,\n    fatal: DataFrame,\n    non_fatal: DataFrame,\n    max_after: dt.timedelta,\n) -&gt; DataFrame:\n    \"\"\"Get survival data from fatal and non-fatal outcomes\n\n    Args:\n        index_spells: The index spells, indexed by `spell_id`\n        fatal: The table of fatal outcomes, containing a `survival_time` column\n        non_fatal: The table of non-fatal outcomes, containing a `time_to_other_episode` column\n        max_after: The right censor time. This is the maximum time for data contained in the\n            fatal and non_fatal tables; any index spells with no events in either table\n            will be right-censored with this time.\n\n    Returns:\n        The survival data containing both fatal and non-fatal events. The survival time is the\n            `time_to_event` column, the `fatal` column contains a flag indicating whether the\n            event was fatal, and the `right_censor` column indicates whether the survival time\n            is censored. The `code` and `docs` column provide information about the type of\n            event for non-censored data (NA otherwise).\n    \"\"\"\n    # Get bleeding survival analysis data (for both fatal\n    # and non-fatal bleeding). First, combine the fatal\n    # and non-fatal data\n    cols_to_keep = [\"index_spell_id\", \"code\", \"docs\", \"time_to_event\"]\n    non_fatal_survival = non_fatal.rename(\n        columns={\"time_to_other_episode\": \"time_to_event\"}\n    )[cols_to_keep]\n    non_fatal_survival[\"fatal\"] = False\n    fatal_survival = fatal.rename(columns={\"survival_time\": \"time_to_event\"})[\n        cols_to_keep\n    ]\n    fatal_survival[\"fatal\"] = True\n    survival = pd.concat([fatal_survival, non_fatal_survival])\n\n    # Take only the first event for each index spell\n    first_event = (\n        survival.sort_values(\"time_to_event\")\n        .groupby(\"index_spell_id\")\n        .head(1)\n        .set_index(\"index_spell_id\")\n    )\n    first_event[\"right_censor\"] = False\n    with pd.option_context(\"future.no_silent_downcasting\", True):\n        with_censor = (\n            index_spells[[]]\n            .merge(first_event, left_index=True, right_index=True, how=\"left\")\n            .fillna({\"fatal\": False, \"time_to_event\": max_after, \"right_censor\": True})\n            .infer_objects(copy=False)\n        )\n    return with_censor\n</code></pre>"},{"location":"reference/#pyhbr.analysis.acs.get_therapy","title":"<code>get_therapy(index_spells, primary_care_prescriptions)</code>","text":"<p>Get therapy (DAPT, etc.) recorded in primary care prescriptions in 60 days after index</p> <p>Parameters:</p> Name Type Description Default <code>index_spells</code> <code>DataFrame</code> <p>Index spells, containing <code>spell_id</code></p> required <code>primary_care_prescriptions</code> <code>DataFrame</code> <p>Contains a column <code>name</code> with the prescription and <code>date</code> when the prescription was recorded.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with a column <code>therapy</code> indexed by <code>spell_id</code></p> Source code in <code>src\\pyhbr\\analysis\\acs.py</code> <pre><code>def get_therapy(index_spells: DataFrame, primary_care_prescriptions: DataFrame) -&gt; DataFrame:\n    \"\"\"Get therapy (DAPT, etc.) recorded in primary care prescriptions in 60 days after index\n\n    Args:\n        index_spells: Index spells, containing `spell_id`\n        primary_care_prescriptions: Contains a column `name` with the prescription\n            and `date` when the prescription was recorded.\n\n    Returns:\n        DataFrame with a column `therapy` indexed by `spell_id`\n    \"\"\"\n\n    # Fetch a particular table or item from raw_data\n    df = primary_care_prescriptions.copy()\n\n\n    def map_medicine(x):\n        if x is None:\n            return np.nan\n        medicines = [\"warfarin\", \"ticagrelor\", \"prasugrel\", \"clopidogrel\", \"aspirin\"]\n        for m in medicines:\n            if m in x.lower():\n                return m\n        return np.nan\n\n\n    df[\"medicine\"] = df[\"name\"].apply(map_medicine)\n\n    # Join primary care prescriptions onto index spells\n    df = index_spells.reset_index().merge(\n        df, on=\"patient_id\", how=\"left\"\n    )\n\n    # Filter to only prescriptions seen in the following month\n    df = df[\n        (df[\"spell_start\"] - df[\"date\"] &lt; dt.timedelta(days=0))\n        &amp; (df[\"date\"] - df[\"spell_start\"] &lt; dt.timedelta(days=60))\n        &amp; ~df[\"medicine\"].isna()\n    ]\n\n    def map_therapy(x):\n\n        aspirin = x[\"medicine\"].eq(\"aspirin\").any()\n        oac = x[\"medicine\"].eq(\"warfarin\").any()\n        p2y12 = x[\"medicine\"].isin([\"ticagrelor\", \"prasugrel\", \"clopidogrel\"]).any()\n\n        if aspirin &amp; p2y12 &amp; oac:\n            return \"Triple\"\n        elif aspirin &amp; x[\"medicine\"].eq(\"ticagrelor\").any():\n            return \"DAPT-AT\"\n        elif aspirin &amp; x[\"medicine\"].eq(\"prasugrel\").any():\n            return \"DAPT-AP\"\n        elif aspirin &amp; x[\"medicine\"].eq(\"clopidogrel\").any():\n            return \"DAPT-AC\"\n        elif aspirin:\n            return \"Single\"\n        else:\n            return np.nan\n\n    # Get the type of therapy seen after the index spell\n    therapy = df.groupby(\"spell_id\")[[\"medicine\"]].apply(map_therapy).rename(\"therapy\")\n\n    # Join back onto the index spells to include cases where no\n    # therapy was seen\n    return index_spells[[]].merge(therapy, on=\"spell_id\", how=\"left\")\n</code></pre>"},{"location":"reference/#pyhbr.analysis.acs.identify_fatal_outcome","title":"<code>identify_fatal_outcome(index_spells, date_of_death, cause_of_death, outcome_group, max_position, max_after)</code>","text":"<p>Get fatal outcomes defined by a diagnosis code in a code group</p> <p>Parameters:</p> Name Type Description Default <code>index_spells</code> <code>DataFrame</code> <p>A table containing <code>spell_id</code> as Pandas index and a column <code>episode_id</code> for the first episode in the index spell.</p> required <code>date_of_death</code> <code>DataFrame</code> <p>Contains a column date_of_death, with Pandas index <code>patient_id</code></p> required <code>cause_of_death</code> <code>DataFrame</code> <p>Contains columns <code>patient_id</code>, <code>code</code> (ICD-10) for cause of death, <code>position</code> of the code, and <code>group</code>.</p> required <code>outcome_group</code> <code>str</code> <p>The name of the ICD-10 code group which defines the fatal outcome.</p> required <code>max_position</code> <code>int</code> <p>The maximum primary/secondary cause of death that will be checked for the code group.</p> required <code>max_after</code> <code>timedelta</code> <p>The maximum follow-up period after the index for valid outcomes.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>A series of boolean containing whether a fatal outcome occurred in the follow-up period.</p> Source code in <code>src\\pyhbr\\analysis\\acs.py</code> <pre><code>def identify_fatal_outcome(\n    index_spells: DataFrame,\n    date_of_death: DataFrame,\n    cause_of_death: DataFrame,\n    outcome_group: str,\n    max_position: int,\n    max_after: dt.timedelta,\n) -&gt; Series:\n    \"\"\"Get fatal outcomes defined by a diagnosis code in a code group\n\n    Args:\n        index_spells: A table containing `spell_id` as Pandas index and a\n            column `episode_id` for the first episode in the index spell.\n        date_of_death: Contains a column date_of_death, with Pandas index\n            `patient_id`\n        cause_of_death: Contains columns `patient_id`, `code` (ICD-10) for\n            cause of death, `position` of the code, and `group`.\n        outcome_group: The name of the ICD-10 code group which defines the fatal\n            outcome.\n        max_position: The maximum primary/secondary cause of death that will be\n            checked for the code group.\n        max_after: The maximum follow-up period after the index for valid outcomes.\n\n    Returns:\n        A series of boolean containing whether a fatal outcome occurred in the follow-up\n            period.\n    \"\"\"\n\n    # Inner join to get a table of index patients with death records\n    mortality_after_index = (\n        index_spells.reset_index()\n        .merge(date_of_death, on=\"patient_id\", how=\"inner\")\n        .merge(cause_of_death, on=\"patient_id\", how=\"inner\")\n    )\n    mortality_after_index[\"survival_time\"] = (\n        mortality_after_index[\"date_of_death\"] - mortality_after_index[\"spell_start\"]\n    )\n\n    # Reduce to only the fatal outcomes that meet the time window and\n    # code inclusion criteria\n    df = mortality_after_index[\n        (mortality_after_index[\"survival_time\"] &lt; max_after)\n        &amp; (mortality_after_index[\"position\"] &lt;= max_position)\n        &amp; (mortality_after_index[\"group\"] == outcome_group)\n    ]\n\n    # Rename the id columns to be compatible with counting.count_code_groups\n    # and select columns of interest\n    return df.rename(columns={\"spell_id\": \"index_spell_id\"})[\n        [\"index_spell_id\", \"survival_time\", \"code\", \"position\", \"docs\", \"group\"]\n    ]\n</code></pre>"},{"location":"reference/#pyhbr.analysis.acs.link_attribute_period_to_index","title":"<code>link_attribute_period_to_index(index_spells, primary_care_attributes)</code>","text":"<p>Link primary care attributes to index spells by attribute date</p> <p>The date column of an attributes row indicates that the attribute was valid at the end of the interval (date, date + 1month). It is important that no attribute is used in modelling that could have occurred after the index event, meaning that date + 1month &lt; spell_start must hold for any attribute used as a predictor. On the other hand, data substantially before the index event should not be used. The valid window is controlled by imposing:</p> <pre><code>date &lt; spell_start - attribute_valid_window\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>index_spells</code> <code>DataFrame</code> <p>The index spell table, containing a <code>spell_start</code> column and <code>patient_id</code></p> required <code>primary_care_attributes</code> <code>DataFrame</code> <p>The patient attributes table, containing <code>date</code> and <code>patient_id</code></p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The index_spells table with a <code>date</code> column added to link the attributes (along with <code>patient_id</code>). This may be NaT if  there is no valid attribute for this index event.</p> Source code in <code>src\\pyhbr\\analysis\\acs.py</code> <pre><code>def link_attribute_period_to_index(\n    index_spells: DataFrame, primary_care_attributes: DataFrame\n) -&gt; DataFrame:\n    \"\"\"Link primary care attributes to index spells by attribute date\n\n    The date column of an attributes row indicates that\n    the attribute was valid at the end of the interval\n    (date, date + 1month). It is important\n    that no attribute is used in modelling that could have occurred\n    after the index event, meaning that date + 1month &lt; spell_start\n    must hold for any attribute used as a predictor. On the other hand,\n    data substantially before the index event should not be used. The\n    valid window is controlled by imposing:\n\n        date &lt; spell_start - attribute_valid_window\n\n    Args:\n        index_spells: The index spell table, containing a `spell_start`\n            column and `patient_id`\n        primary_care_attributes: The patient attributes table, containing\n            `date` and `patient_id`\n\n    Returns:\n        The index_spells table with a `date` column added to link the\n            attributes (along with `patient_id`). This may be NaT if \n            there is no valid attribute for this index event.\n    \"\"\"\n\n    # Define a window before the index event where SWD attributes will be considered valid.\n    # 41 days is used to ensure that a full month is definitely captured. This\n    # ensures that attribute data that is fairly recent is used as predictors.\n    attribute_valid_window = dt.timedelta(days=60)\n\n    # Add all the patient's attributes onto each index spell\n    df = index_spells.reset_index().merge(\n        primary_care_attributes[[\"patient_id\", \"date\"]],\n        how=\"left\",\n        on=\"patient_id\",\n    )\n\n    # Only keep attributes that are from strictly before the index spell\n    # (note date represents the start of the month that attributes\n    # apply to)\n    attr_before_index = df[(df[\"date\"] + dt.timedelta(days=31)) &lt; df[\"spell_start\"]]\n\n    # Keep only the most recent attribute before the index spell\n    most_recent = attr_before_index.sort_values(\"date\").groupby(\"spell_id\").tail(1)\n\n    # Exclude attributes that occurred outside the attribute_value_window before the index\n    swd_index_spells = most_recent[\n        most_recent[\"date\"] &gt; (most_recent[\"spell_start\"] - attribute_valid_window)\n    ]\n\n    return index_spells.merge(\n        swd_index_spells[[\"spell_id\", \"date\"]].set_index(\"spell_id\"),\n        how=\"left\",\n        on=\"spell_id\",\n    )\n</code></pre>"},{"location":"reference/#pyhbr.analysis.acs.prescriptions_before_index","title":"<code>prescriptions_before_index(swd_index_spells, primary_care_prescriptions)</code>","text":"<p>Get the number of primary care prescriptions before each index spell</p> <p>Parameters:</p> Name Type Description Default <code>index_spells</code> <p>Must have Pandas index <code>spell_id</code></p> required <code>primary_care_prescriptions</code> <code>DataFrame</code> <p>Must contain a <code>name</code> column that contains a string containing the medicine name somewhere (any case), a <code>date</code> column with the prescription date, and a <code>patient_id</code> column.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A table indexed by <code>spell_id</code> that contains one column for each prescription type, prefexed with \"prior_\"</p> Source code in <code>src\\pyhbr\\analysis\\acs.py</code> <pre><code>def prescriptions_before_index(\n    swd_index_spells: DataFrame, primary_care_prescriptions: DataFrame\n) -&gt; DataFrame:\n    \"\"\"Get the number of primary care prescriptions before each index spell\n\n    Args:\n        index_spells: Must have Pandas index `spell_id`\n        primary_care_prescriptions: Must contain a `name` column\n            that contains a string containing the medicine name\n            somewhere (any case), a `date` column with the\n            prescription date, and a `patient_id` column.\n\n    Returns:\n        A table indexed by `spell_id` that contains one column\n            for each prescription type, prefexed with \"prior_\"\n    \"\"\"\n\n    df = primary_care_prescriptions\n\n    # Filter for relevant prescriptions\n    df = from_hic.filter_by_medicine(df)\n\n    # Drop rows where the prescription date is not known\n    df = df[~df[\"date\"].isna()]\n\n    # Join the prescriptions to the index spells\n    df = (\n        swd_index_spells[[\"spell_start\", \"patient_id\"]]\n        .reset_index()\n        .merge(df, how=\"left\", on=\"patient_id\")\n    )\n    df[\"time_to_index_spell\"] = df[\"spell_start\"] - df[\"date\"]\n\n    # Only keep prescriptions occurring in the year before the index event\n    min_before = dt.timedelta(days=0)\n    max_before = dt.timedelta(days=365)\n    events_before_index = counting.get_time_window(\n        df, -max_before, -min_before, \"time_to_index_spell\"\n    )\n\n    # Pivot each row (each prescription) to one column per\n    # prescription group.\n    all_counts = counting.count_events(\n        swd_index_spells, events_before_index, \"group\"\n    ).add_prefix(\"prior_\")\n\n    return all_counts\n</code></pre>"},{"location":"reference/#pyhbr.analysis.acs.remove_features","title":"<code>remove_features(index_attributes, max_missingness, const_threshold)</code>","text":"<p>Reduce to just the columns meeting minimum missingness and variability criteria.</p> <p>Parameters:</p> Name Type Description Default <code>index_attributes</code> <code>DataFrame</code> <p>The table of primary care attributes for the index spells</p> required <code>max_missingness</code> <p>The maximum allowed missingness in a column before a column is removed as a feature.</p> required <code>const_threshold</code> <p>The maximum allowed constant-value proportion (NA + most common non-NA value) before a column is removed as a feature</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A table containing the features that remain, which contain sufficient non-missing values and sufficient variance.</p> Source code in <code>src\\pyhbr\\analysis\\acs.py</code> <pre><code>def remove_features(\n    index_attributes: DataFrame, max_missingness, const_threshold\n) -&gt; DataFrame:\n    \"\"\"Reduce to just the columns meeting minimum missingness and variability criteria.\n\n    Args:\n        index_attributes: The table of primary care attributes for the index spells\n        max_missingness: The maximum allowed missingness in a column before a column\n            is removed as a feature.\n        const_threshold: The maximum allowed constant-value proportion (NA + most\n            common non-NA value) before a column is removed as a feature\n\n    Returns:\n        A table containing the features that remain, which contain sufficient\n            non-missing values and sufficient variance.\n    \"\"\"\n    missingness = describe.proportion_missingness(index_attributes)\n    nearly_constant = describe.nearly_constant(index_attributes, const_threshold)\n    to_keep = (missingness &lt; max_missingness) &amp; ~nearly_constant\n    return index_attributes.loc[:, to_keep]\n</code></pre>"},{"location":"reference/#pyhbr.analysis.arc_hbr","title":"<code>arc_hbr</code>","text":"<p>Calculation of the ARC HBR score</p>"},{"location":"reference/#pyhbr.analysis.arc_hbr.all_index_spell_episodes","title":"<code>all_index_spell_episodes(index_episodes, episodes)</code>","text":"<p>Get all the other episodes in the index spell</p> <p>This is a dataframe of index spells (defined as the spell containing an episode in index_episodes), along with all the episodes in that spell (including the index episode itself). This is useful for performing operations at index-spell granularity</p> <p>Parameters:</p> Name Type Description Default <code>index_episodes</code> <code>DataFrame</code> <p>Must contain Pandas index <code>episode_id</code></p> required <code>episodes</code> <code>DataFrame</code> <p>Must contain Pandas index <code>episode_id</code> and have a columne <code>spell_id</code>.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe with a column <code>spell_id</code> for index spells, and <code>episode_id</code> for all episodes in that spell. A column <code>index_episode</code> shows which of the episodes is the first episode in the spell.</p> Source code in <code>src\\pyhbr\\analysis\\arc_hbr.py</code> <pre><code>def all_index_spell_episodes(\n    index_episodes: DataFrame, episodes: DataFrame\n) -&gt; DataFrame:\n    \"\"\"Get all the other episodes in the index spell\n\n    This is a dataframe of index spells (defined as the spell containing\n    an episode in index_episodes), along with all the episodes in that\n    spell (including the index episode itself). This is useful for\n    performing operations at index-spell granularity\n\n    Args:\n        index_episodes: Must contain Pandas index `episode_id`\n        episodes: Must contain Pandas index `episode_id` and have a columne\n            `spell_id`.\n\n    Returns:\n        A dataframe with a column `spell_id` for index spells, and `episode_id`\n            for all episodes in that spell. A column `index_episode` shows which\n            of the episodes is the first episode in the spell.\n    \"\"\"\n    index_spells = (\n        index_episodes[[]]\n        .merge(episodes[\"spell_id\"], how=\"left\", on=\"episode_id\")\n        .set_index(\"spell_id\")\n    )\n    return index_spells.merge(episodes.reset_index(), how=\"left\", on=\"spell_id\")[\n        [\"episode_id\", \"spell_id\"]\n    ]\n</code></pre>"},{"location":"reference/#pyhbr.analysis.arc_hbr.arc_hbr_age","title":"<code>arc_hbr_age(has_age)</code>","text":"<p>Calculate the age ARC-HBR criterion</p> <p>Calculate the age ARC HBR criterion (0.5 points if &gt; 75 at index, 0 otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>has_age</code> <code>DataFrame</code> <p>Dataframe which has a column <code>age</code></p> required <p>Returns:</p> Type Description <code>Series</code> <p>A series of values 0.5 (if age &gt; 75 at index) or 0 otherwise, indexed by input dataframe index.</p> Source code in <code>src\\pyhbr\\analysis\\arc_hbr.py</code> <pre><code>def arc_hbr_age(has_age: DataFrame) -&gt; Series:\n    \"\"\"Calculate the age ARC-HBR criterion\n\n    Calculate the age ARC HBR criterion (0.5 points if &gt; 75 at index, 0 otherwise.\n\n    Args:\n        has_age: Dataframe which has a column `age`\n\n    Returns:\n        A series of values 0.5 (if age &gt; 75 at index) or 0 otherwise, indexed\n            by input dataframe index.\n    \"\"\"\n    return Series(np.where(has_age[\"age\"] &gt; 75, 0.5, 0), index=has_age.index)\n</code></pre>"},{"location":"reference/#pyhbr.analysis.arc_hbr.arc_hbr_anaemia","title":"<code>arc_hbr_anaemia(has_index_hb_and_gender)</code>","text":"<p>Calculate the ARC HBR anaemia (low Hb) criterion</p> <p>Calculates anaemia based on the worst (lowest) index Hb measurement and gender currently. Should be modified to take most recent Hb value or clinical code.</p> <p>Parameters:</p> Name Type Description Default <code>has_index_hb_and_gender</code> <code>DataFrame</code> <p>Dataframe having the column <code>index_hb</code> containing the Hb measurement (g/dL) at the index event, or NaN if no Hb measurement was made. Also contains <code>gender</code> (categorical with categories \"male\", \"female\", and \"unknown\").</p> required <p>Returns:</p> Type Description <code>Series</code> <p>A series containing the HBR score for the index episode.</p> Source code in <code>src\\pyhbr\\analysis\\arc_hbr.py</code> <pre><code>def arc_hbr_anaemia(has_index_hb_and_gender: DataFrame) -&gt; Series:\n    \"\"\"Calculate the ARC HBR anaemia (low Hb) criterion\n\n    Calculates anaemia based on the worst (lowest) index Hb measurement\n    and gender currently. Should be modified to take most recent Hb value\n    or clinical code.\n\n    Args:\n        has_index_hb_and_gender: Dataframe having the column `index_hb` containing the\n            Hb measurement (g/dL) at the index event, or NaN if no Hb measurement\n            was made. Also contains `gender` (categorical with categories \"male\",\n            \"female\", and \"unknown\").\n\n    Returns:\n        A series containing the HBR score for the index episode.\n    \"\"\"\n\n    df = has_index_hb_and_gender\n\n    # Evaluated in order\n    arc_score_conditions = [\n        df[\"hb\"] &lt; 11.0,  # Major for any gender\n        df[\"hb\"] &lt; 11.9,  # Minor for any gender\n        (df[\"hb\"] &lt; 12.9) &amp; (df[\"gender\"] == \"male\"),  # Minor for male\n        df[\"hb\"] &gt;= 12.9,  # None for any gender\n    ]\n    arc_scores = [1.0, 0.5, 0.5, 0.0]\n\n    # Default is used to fill missing Hb score with 0.0 for now. TODO: replace with\n    # fall-back to recent Hb, or codes.\n    return Series(\n        np.select(arc_score_conditions, arc_scores, default=0.0),\n        index=df.index,\n    )\n</code></pre>"},{"location":"reference/#pyhbr.analysis.arc_hbr.arc_hbr_cancer","title":"<code>arc_hbr_cancer(has_prior_cancer)</code>","text":"<p>Calculate the cancer ARC HBR criterion</p> <p>This function takes a dataframe with a column prior_cancer with a count of the cancer diagnoses in the previous year.</p> <p>Parameters:</p> Name Type Description Default <code>has_prior_cancer</code> <code>DataFrame</code> <p>Has a column <code>prior_cancer</code> with a count of the number of cancer diagnoses occurring in the year before the index event.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>The ARC HBR cancer criterion (0.0, 1.0)</p> Source code in <code>src\\pyhbr\\analysis\\arc_hbr.py</code> <pre><code>def arc_hbr_cancer(has_prior_cancer: DataFrame) -&gt; Series:\n    \"\"\"Calculate the cancer ARC HBR criterion\n\n    This function takes a dataframe with a column prior_cancer\n    with a count of the cancer diagnoses in the previous year.\n\n    Args:\n        has_prior_cancer: Has a column `prior_cancer` with a count\n            of the number of cancer diagnoses occurring in the\n            year before the index event.\n\n    Returns:\n        The ARC HBR cancer criterion (0.0, 1.0)\n    \"\"\"\n    return Series(\n        np.where(has_prior_cancer[\"cancer_before\"] &gt; 0, 1.0, 0),\n        index=has_prior_cancer.index,\n    )\n</code></pre>"},{"location":"reference/#pyhbr.analysis.arc_hbr.arc_hbr_cirrhosis_ptl_hyp","title":"<code>arc_hbr_cirrhosis_ptl_hyp(has_prior_cirrhosis)</code>","text":"<p>Calculate the liver cirrhosis with portal hypertension ARC HBR criterion</p> <p>This function takes a dataframe with two columns prior_cirrhosis and prior_portal_hyp, which count the number of diagnosis of liver cirrhosis and portal hypertension seen in the previous year.</p> <p>Parameters:</p> Name Type Description Default <code>has_prior_cirrhosis</code> <code>DataFrame</code> <p>Has columns <code>prior_cirrhosis</code> and <code>prior_portal_hyp</code>.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>The ARC HBR criterion (0.0, 1.0)</p> Source code in <code>src\\pyhbr\\analysis\\arc_hbr.py</code> <pre><code>def arc_hbr_cirrhosis_ptl_hyp(has_prior_cirrhosis: DataFrame) -&gt; Series:\n    \"\"\"Calculate the liver cirrhosis with portal hypertension ARC HBR criterion\n\n    This function takes a dataframe with two columns prior_cirrhosis\n    and prior_portal_hyp, which count the number of diagnosis of\n    liver cirrhosis and portal hypertension seen in the previous\n    year.\n\n    Args:\n        has_prior_cirrhosis: Has columns `prior_cirrhosis` and\n            `prior_portal_hyp`.\n\n    Returns:\n        The ARC HBR criterion (0.0, 1.0)\n    \"\"\"\n    cirrhosis = has_prior_cirrhosis[\"liver_cirrhosis_before\"] &gt; 0\n    portal_hyp = has_prior_cirrhosis[\"portal_hypertension_before\"] &gt; 0\n\n    return Series(\n        np.where(cirrhosis &amp; portal_hyp, 1.0, 0),\n        index=has_prior_cirrhosis.index,\n    )\n</code></pre>"},{"location":"reference/#pyhbr.analysis.arc_hbr.arc_hbr_ckd","title":"<code>arc_hbr_ckd(has_index_egfr)</code>","text":"<p>Calculate the ARC HBR chronic kidney disease (CKD) criterion</p> <p>The ARC HBR CKD criterion is calculated based on the eGFR as follows:</p> eGFR Score eGFR &lt; 30 mL/min 1.0 30 mL/min \\&lt;= eGFR &lt; 60 mL/min 0.5 eGFR &gt;= 60 mL/min 0.0 <p>If the eGFR is NaN, set score to zero (TODO: fall back to ICD-10 codes in this case)</p> <p>Parameters:</p> Name Type Description Default <code>has_index_egfr</code> <code>DataFrame</code> <p>Dataframe having the column <code>index_egfr</code> (in units of mL/min) with the eGFR measurement at index, or NaN which means no eGFR measurement was found at the index.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>A series containing the CKD ARC criterion, based on the eGFR at index.</p> Source code in <code>src\\pyhbr\\analysis\\arc_hbr.py</code> <pre><code>def arc_hbr_ckd(has_index_egfr: DataFrame) -&gt; Series:\n    \"\"\"Calculate the ARC HBR chronic kidney disease (CKD) criterion\n\n    The ARC HBR CKD criterion is calculated based on the eGFR as\n    follows:\n\n    | eGFR                           | Score |\n    |--------------------------------|-------|\n    | eGFR &lt; 30 mL/min               | 1.0   |\n    | 30 mL/min \\&lt;= eGFR &lt; 60 mL/min | 0.5   |\n    | eGFR &gt;= 60 mL/min              | 0.0   |\n\n    If the eGFR is NaN, set score to zero (TODO: fall back to ICD-10\n    codes in this case)\n\n    Args:\n        has_index_egfr: Dataframe having the column `index_egfr` (in units of mL/min)\n            with the eGFR measurement at index, or NaN which means no eGFR\n            measurement was found at the index.\n\n    Returns:\n        A series containing the CKD ARC criterion, based on the eGFR at\n            index.\n    \"\"\"\n\n    # Replace NaN values for now with 100 (meaning score 0.0)\n    df = has_index_egfr[\"egfr\"].fillna(90)\n\n    # Using a high upper limit to catch any high eGFR values. In practice,\n    # the highest value is 90 (which comes from the string \"&gt;90\" in the database).\n    return cut(df, [0, 30, 60, 10000], right=False, labels=[1.0, 0.5, 0.0])\n</code></pre>"},{"location":"reference/#pyhbr.analysis.arc_hbr.arc_hbr_ischaemic_stroke_ich","title":"<code>arc_hbr_ischaemic_stroke_ich(has_prior_ischaemic_stroke)</code>","text":"<p>Calculate the ischaemic stroke/intracranial haemorrhage ARC HBR criterion</p> <p>This function takes a dataframe with two columns prior_bavm_ich and prior_portal_hyp, which count the number of diagnosis of liver cirrhosis and portal hypertension seen in the previous year.</p> <p>If bAVM/ICH is present, 1.0 is added to the score. Else, if ischaemic stroke is present, add 0.5. Otherwise add 0.</p> <p>Parameters:</p> Name Type Description Default <code>has_prior_ischaemic_stroke</code> <code>DataFrame</code> <p>Has a column <code>prior_ischaemic_stroke</code> containing the number of any-severity ischaemic strokes in the previous year, and a column <code>prior_bavm_ich</code> containing a count of any diagnosis of brain arteriovenous malformation or intracranial haemorrhage.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>The ARC HBR criterion (0.0, 1.0)</p> Source code in <code>src\\pyhbr\\analysis\\arc_hbr.py</code> <pre><code>def arc_hbr_ischaemic_stroke_ich(has_prior_ischaemic_stroke: DataFrame) -&gt; Series:\n    \"\"\"Calculate the ischaemic stroke/intracranial haemorrhage ARC HBR criterion\n\n    This function takes a dataframe with two columns prior_bavm_ich\n    and prior_portal_hyp, which count the number of diagnosis of\n    liver cirrhosis and portal hypertension seen in the previous\n    year.\n\n    If bAVM/ICH is present, 1.0 is added to the score. Else, if\n    ischaemic stroke is present, add 0.5. Otherwise add 0.\n\n    Args:\n        has_prior_ischaemic_stroke: Has a column `prior_ischaemic_stroke` containing\n            the number of any-severity ischaemic strokes in the previous\n            year, and a column `prior_bavm_ich` containing a count of\n            any diagnosis of brain arteriovenous malformation or\n            intracranial haemorrhage.\n\n\n    Returns:\n        The ARC HBR criterion (0.0, 1.0)\n    \"\"\"\n    ischaemic_stroke = has_prior_ischaemic_stroke[\"ischaemic_stroke_before\"] &gt; 0\n    bavm_ich = (has_prior_ischaemic_stroke[\"bavm_before\"] + has_prior_ischaemic_stroke[\"ich_before\"]) &gt; 0\n\n    score_one = np.where(bavm_ich, 1, 0)\n    score_half = np.where(ischaemic_stroke, 0.5, 0)\n    score_zero = np.zeros(len(has_prior_ischaemic_stroke))\n\n    return Series(\n        np.maximum(score_one, score_half, score_zero),\n        index=has_prior_ischaemic_stroke.index,\n    )\n</code></pre>"},{"location":"reference/#pyhbr.analysis.arc_hbr.arc_hbr_medicine","title":"<code>arc_hbr_medicine(index_spells, episodes, prescriptions, medicine_group, arc_score)</code>","text":"<p>Calculate the oral-anticoagulant/NSAID ARC HBR criterion</p> <p>Pass the list of medicines which qualifies for the OAC ARC criterion, along with the ARC score; or pass the same data for the NSAID criterion.</p> <p>The score is added if a prescription of the medicine is seen at any time during the patient spell.</p> <p>Notes on the OAC and NSAID criteria:</p> <p>1.0 point if an one of the OACs \"warfarin\", \"apixaban\", \"rivaroxaban\", \"edoxaban\", \"dabigatran\", is present in the index spell (meaning the index episode, or any other episode in the spell).</p> <p>1.0 point is added if an one of the following NSAIDs is present on admission:</p> <ul> <li>Ibuprofen</li> <li>Naproxen</li> <li>Diclofenac</li> <li>Celecoxib</li> <li>Mefenamic acid</li> <li>Etoricoxib</li> <li>Indomethacin</li> </ul> <p>Note</p> <p>The on admission flag could be used to imply expected chronic/extended use, but this is not included as it filters out all OAC prescriptions in the HIC data.</p> <p>Parameters:</p> Name Type Description Default <code>index_spells</code> <code>DataFrame</code> <p>Index <code>spell_id</code> is used to narrow prescriptions.</p> required <code>prescriptions</code> <code>DataFrame</code> <p>Contains <code>name</code> (of medicine).</p> required <p>Returns:</p> Type Description <code>Series</code> <p>The ARC score for each index spell</p> Source code in <code>src\\pyhbr\\analysis\\arc_hbr.py</code> <pre><code>def arc_hbr_medicine(\n    index_spells: DataFrame,\n    episodes: DataFrame,\n    prescriptions: DataFrame,\n    medicine_group: str,\n    arc_score: float,\n) -&gt; Series:\n    \"\"\"Calculate the oral-anticoagulant/NSAID ARC HBR criterion\n\n    Pass the list of medicines which qualifies for the OAC\n    ARC criterion, along with the ARC score; or pass the same\n    data for the NSAID criterion.\n\n    The score is added if a prescription of the medicine is seen\n    at any time during the patient spell.\n\n    Notes on the OAC and NSAID criteria:\n\n    1.0 point if an one of the OACs \"warfarin\", \"apixaban\",\n    \"rivaroxaban\", \"edoxaban\", \"dabigatran\", is present\n    in the index spell (meaning the index episode, or any\n    other episode in the spell).\n\n    1.0 point is added if an one of the following NSAIDs is present\n    on admission:\n\n    * Ibuprofen\n    * Naproxen\n    * Diclofenac\n    * Celecoxib\n    * Mefenamic acid\n    * Etoricoxib\n    * Indomethacin\n\n    !!! note\n        The on admission flag could be used to imply expected\n        chronic/extended use, but this is not included as it filters\n        out all OAC prescriptions in the HIC data.\n\n    Args:\n        index_spells: Index `spell_id` is used to narrow prescriptions.\n        prescriptions: Contains `name` (of medicine).\n\n    Returns:\n        The ARC score for each index spell\n    \"\"\"\n\n    # Get all the data required\n    df = (\n        index_spells.reset_index(\"spell_id\")\n        .merge(prescriptions, on=\"patient_id\", how=\"left\")\n        .merge(episodes[[\"admission\", \"discharge\"]], on=\"episode_id\", how=\"left\")\n    )\n\n    # Filter by prescription name and only keep only prescriptions ordered between\n    # admission and discharge\n    correct_prescription = df[\"group\"] == medicine_group\n    within_spell = (df[\"order_date\"] &gt;= df[\"admission\"]) &amp; (\n        df[\"order_date\"] &lt;= df[\"discharge\"]\n    )\n\n    # Populate the rows of df with the score\n    df[\"arc_score\"] = 0.0\n    df.loc[correct_prescription &amp; within_spell, \"arc_score\"] = 1.0\n\n    # Group by the index spell id and get the max score\n    return df.groupby(\"spell_id\").max(\"arc_score\")[\"arc_score\"]\n</code></pre>"},{"location":"reference/#pyhbr.analysis.arc_hbr.arc_hbr_nsaid","title":"<code>arc_hbr_nsaid(index_episodes, prescriptions)</code>","text":"<p>Calculate the non-steroidal anti-inflamatory drug (NSAID) ARC HBR criterion</p> <p>1.0 point is added if an one of the following NSAIDs is present on admission:</p> <ul> <li>Ibuprofen</li> <li>Naproxen</li> <li>Diclofenac</li> <li>Celecoxib</li> <li>Mefenamic acid</li> <li>Etoricoxib</li> <li>Indomethacin</li> </ul> <p>Parameters:</p> Name Type Description Default <code>index_episodes</code> <code>DataFrame</code> <p>Index <code>episode_id</code> is used to narrow prescriptions.</p> required <code>prescriptions</code> <code>DataFrame</code> <p>Contains <code>name</code> (of medicine).</p> required <p>Returns:</p> Type Description <code>Series</code> <p>The OAC ARC score for each index event.</p> Source code in <code>src\\pyhbr\\analysis\\arc_hbr.py</code> <pre><code>def arc_hbr_nsaid(index_episodes: DataFrame, prescriptions: DataFrame) -&gt; Series:\n    \"\"\"Calculate the non-steroidal anti-inflamatory drug (NSAID) ARC HBR criterion\n\n    1.0 point is added if an one of the following NSAIDs is present\n    on admission:\n\n    * Ibuprofen\n    * Naproxen\n    * Diclofenac\n    * Celecoxib\n    * Mefenamic acid\n    * Etoricoxib\n    * Indomethacin\n\n    Args:\n        index_episodes: Index `episode_id` is used to narrow prescriptions.\n        prescriptions: Contains `name` (of medicine).\n\n    Returns:\n        The OAC ARC score for each index event.\n    \"\"\"\n    df = index_episodes.merge(prescriptions, how=\"left\", on=\"episode_id\")\n    nsaid_criterion = ((df[\"group\"] == \"nsaid\") &amp; (df[\"on_admission\"] == True)).astype(\n        \"float\"\n    )\n    return nsaid_criterion.set_axis(index_episodes.index)\n</code></pre>"},{"location":"reference/#pyhbr.analysis.arc_hbr.arc_hbr_prior_bleeding","title":"<code>arc_hbr_prior_bleeding(has_prior_bleeding)</code>","text":"<p>Calculate the prior bleeding/transfusion ARC HBR criterion</p> <p>This function takes a dataframe with a column prior_bleeding_12 with a count of the prior bleeding events in the previous year.</p> <p>TODO: Input needs a separate column for bleeding in 6 months and bleeding in a year, so distinguish 0.5 from 1. Also need to add transfusion.</p> <p>Parameters:</p> Name Type Description Default <code>has_prior_bleeding</code> <code>DataFrame</code> <p>Has a column <code>prior_bleeding_12</code> with a count of the number of bleeds occurring one year before the index. Has <code>episode_id</code> as the index.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>The ARC HBR bleeding/transfusion criterion (0.0, 0.5, or 1.0)</p> Source code in <code>src\\pyhbr\\analysis\\arc_hbr.py</code> <pre><code>def arc_hbr_prior_bleeding(has_prior_bleeding: DataFrame) -&gt; Series:\n    \"\"\"Calculate the prior bleeding/transfusion ARC HBR criterion\n\n    This function takes a dataframe with a column prior_bleeding_12\n    with a count of the prior bleeding events in the previous year.\n\n    TODO: Input needs a separate column for bleeding in 6 months and\n    bleeding in a year, so distinguish 0.5 from 1. Also need to add\n    transfusion.\n\n    Args:\n        has_prior_bleeding: Has a column `prior_bleeding_12` with a count\n            of the number of bleeds occurring one year before the index.\n            Has `episode_id` as the index.\n\n    Returns:\n        The ARC HBR bleeding/transfusion criterion (0.0, 0.5, or 1.0)\n    \"\"\"\n    return Series(\n        np.where(has_prior_bleeding[\"bleeding_adaptt_before\"] &gt; 0, 0.5, 0),\n        index=has_prior_bleeding.index,\n    )\n</code></pre>"},{"location":"reference/#pyhbr.analysis.arc_hbr.arc_hbr_tcp","title":"<code>arc_hbr_tcp(has_index_platelets)</code>","text":"<p>Calculate the ARC HBR thrombocytopenia (low platelet count) criterion</p> <p>The score is 1.0 if platelet count &lt; 100e9/L, otherwise it is 0.0.</p> <p>Parameters:</p> Name Type Description Default <code>has_index_platelets</code> <code>DataFrame</code> <p>Has column <code>index_platelets</code>, which is the platelet count measurement in the index.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Series containing the ARC score</p> Source code in <code>src\\pyhbr\\analysis\\arc_hbr.py</code> <pre><code>def arc_hbr_tcp(has_index_platelets: DataFrame) -&gt; Series:\n    \"\"\"Calculate the ARC HBR thrombocytopenia (low platelet count) criterion\n\n    The score is 1.0 if platelet count &lt; 100e9/L, otherwise it is 0.0.\n\n    Args:\n        has_index_platelets: Has column `index_platelets`, which is the\n            platelet count measurement in the index.\n\n    Returns:\n        Series containing the ARC score\n    \"\"\"\n    return Series(\n        np.where(has_index_platelets[\"platelets\"] &lt; 100, 1.0, 0),\n        index=has_index_platelets.index,\n    )\n</code></pre>"},{"location":"reference/#pyhbr.analysis.arc_hbr.first_index_lab_result","title":"<code>first_index_lab_result(index_spells, lab_results, episodes)</code>","text":"<p>Get the (first) lab result associated to each index spell</p> <p>Get a table of the first lab result seen in the index admission (between the admission date and discharge date), with one column for each value of the <code>test_name</code> column in lab_results.</p> <p>The resulting table has all-NA rows for index spells where no lab results were seen, and cells contain NA if that lab result was missing from the index spell.</p> <p>Parameters:</p> Name Type Description Default <code>index_spells</code> <code>DataFrame</code> <p>Has an <code>spell_id</code> index and <code>patient_id</code> column.</p> required <code>lab_results</code> <code>DataFrame</code> <p>Has a <code>test_name</code> and a <code>result</code> column for the numerical test result, and a <code>sample_date</code> for when the sample for the test was collected.</p> required <code>episodes</code> <code>DataFrame</code> <p>Indexed by <code>episode_id</code>, and contains <code>admission</code> and <code>discharge</code> columns.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A table indexed by <code>spell_id</code> containing one column per unique test in <code>test_name</code> (the column name is the same as the value in the <code>test_name</code> column).</p> Source code in <code>src\\pyhbr\\analysis\\arc_hbr.py</code> <pre><code>def first_index_lab_result(\n    index_spells: DataFrame,\n    lab_results: DataFrame,\n    episodes: DataFrame,\n) -&gt; DataFrame:\n    \"\"\"Get the (first) lab result associated to each index spell\n\n    Get a table of the first lab result seen in the index admission (between\n    the admission date and discharge date), with one column for each\n    value of the `test_name` column in lab_results.\n\n    The resulting table has all-NA rows for index spells where no lab results\n    were seen, and cells contain NA if that lab result was missing from the\n    index spell.\n\n    Args:\n        index_spells: Has an `spell_id` index and `patient_id` column.\n        lab_results: Has a `test_name` and a `result` column for the\n            numerical test result, and a `sample_date` for when the sample\n            for the test was collected.\n        episodes: Indexed by `episode_id`, and contains `admission`\n            and `discharge` columns.\n\n    Returns:\n        A table indexed by `spell_id` containing one column per unique\n            test in `test_name` (the column name is the same as the value\n            in the `test_name` column).\n    \"\"\"\n\n    # Get the admission and discharge time for each index spell\n    admission_discharge = (\n        index_spells[[\"patient_id\", \"episode_id\"]]\n        .reset_index(\"spell_id\")\n        .merge(episodes[[\"admission\", \"discharge\"]], on=\"episode_id\", how=\"left\")\n    )\n\n    # For every index spell, join all the lab results for that patient\n    # and reduce to only those occurring within the admission/discharge\n    # time window\n    df = admission_discharge.merge(lab_results, on=\"patient_id\", how=\"left\")\n    within_spell = (df[\"sample_date\"] &gt; df[\"admission\"]) &amp; (\n        df[\"sample_date\"] &lt; df[\"discharge\"]\n    )\n    index_spell_labs = df[within_spell]\n    first_lab = (\n        index_spell_labs.sort_values(\"sample_date\")\n        .groupby([\"spell_id\", \"test_name\"])\n        .head(1)\n    )\n\n    # Convert to a wide format with one column per test, then right-join the\n    # index spells to catch cases where no lab results were present\n    wide = first_lab.pivot(index=\"spell_id\", columns=\"test_name\", values=\"result\").merge(\n        index_spells[[]], on=\"spell_id\", how=\"right\"\n    )\n\n    return wide\n</code></pre>"},{"location":"reference/#pyhbr.analysis.arc_hbr.plot_index_measurement_distribution","title":"<code>plot_index_measurement_distribution(features)</code>","text":"<p>Plot a histogram of measurement results at the index</p> <p>Parameters:</p> Name Type Description Default <code>index_episodes</code> <p>Must contain <code>index_hb</code>, <code>index_egfr</code>,</p> required Source code in <code>src\\pyhbr\\analysis\\arc_hbr.py</code> <pre><code>def plot_index_measurement_distribution(features: DataFrame):\n    \"\"\"Plot a histogram of measurement results at the index\n\n    Args:\n        index_episodes: Must contain `index_hb`, `index_egfr`,\n        and `index_platelets`. The index_hb column is multiplied\n        by 10 to get units g/L.\n    \"\"\"\n\n    # Make a plot showing the three lab results as histograms\n    df = features.copy()\n    df[\"index_hb\"] = 10 * df[\"index_hb\"]  # Convert from g/dL to g/L\n    df = (\n        df.filter(regex=\"^index_(egfr|hb|platelets)\")\n        .rename(\n            columns={\n                \"index_egfr\": \"eGFR (mL/min)\",\n                \"index_hb\": \"Hb (g/L)\",\n                \"index_platelets\": \"Plt (x10^9/L)\",\n            }\n        )\n        .melt(value_name=\"Test result at index episode\", var_name=\"Test\")\n    )\n    g = sns.displot(\n        df,\n        x=\"Test result at index episode\",\n        hue=\"Test\",\n    )\n    g.figure.subplots_adjust(top=0.95)\n    g.ax.set_title(\"Distribution of Laboratory Test Results in ACS/PCI index events\")\n</code></pre>"},{"location":"reference/#pyhbr.analysis.calibration","title":"<code>calibration</code>","text":"<p>Calibration plots</p> <p>A calibration plot is a comparison of the proportion p of events that occur in the subset of those with predicted probability p'. Ideally, p = p' meaning that of the cases predicted to occur with probability p', p of them do occur. Calibration is presented as a plot of p against 'p'.</p> <p>The stability of the calibration can be investigated, by plotting p against p' for multiple bootstrapped models (see stability.py).</p>"},{"location":"reference/#pyhbr.analysis.calibration.draw_calibration_confidence","title":"<code>draw_calibration_confidence(ax, calibration)</code>","text":"<p>Draw a single model's calibration curve with confidence intervals</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>The axes on which to draw the plot</p> required <code>calibration</code> <code>DataFrame</code> <p>The model's calibration data</p> required Source code in <code>src\\pyhbr\\analysis\\calibration.py</code> <pre><code>def draw_calibration_confidence(ax: Axes, calibration: DataFrame):\n    \"\"\"Draw a single model's calibration curve with confidence intervals\n\n    Args:\n        ax: The axes on which to draw the plot\n        calibration: The model's calibration data\n    \"\"\"\n    c = calibration\n\n    make_error_boxes(ax, c)\n\n    ax.set_xscale(\"log\")\n    ax.set_yscale(\"log\")\n    ax.xaxis.set_major_formatter(mtick.PercentFormatter())\n    ax.yaxis.set_major_formatter(mtick.PercentFormatter())\n    ax.set_ylabel(\"Estimated Prevalence\")\n    ax.set_xlabel(\"Model-Estimated Risks\")\n    ax.set_title(\"Accuracy of Risk Estimates\")\n\n    # Get the minimum and maximum for the x range\n    min_x = 100 * (c[\"bin_center\"]).min()\n    max_x = 100 * (c[\"bin_center\"]).max()\n\n    # Generate a dense straight line (smooth curve on log scale)\n    coords = np.linspace(min_x, max_x, num=50)\n\n    ax.plot(coords, coords, c=\"k\")\n</code></pre>"},{"location":"reference/#pyhbr.analysis.calibration.get_average_calibration_error","title":"<code>get_average_calibration_error(probs, y_test, n_bins)</code>","text":"<p>This is the weighted average discrepancy between the predicted risk and the observed proportions on the calibration curve.</p> <p>See \"https://towardsdatascience.com/expected-calibration-error-ece-a-step- by-step-visual-explanation-with-python-code-c3e9aa12937d\" for a good explanation.</p> <p>The formula for estimated calibration error (ece) is:</p> <p>ece = Sum over bins [samples_in_bin / N] * | P_observed - P_pred |,</p> <p>where P_observed is the empirical proportion of positive samples in the bin, and P_pred is the predicted probability for that bin. The results are weighted by the number of samples in the bin (because some probabilities are predicted more frequently than others).</p> <p>The result is interpreted as an absolute error: i.e. a value of 0.1 means that the calibration is out on average by 10%. It may be better to modify the formula to compute an average relative error.</p> <p>Testing: not yet tested.</p> Source code in <code>src\\pyhbr\\analysis\\calibration.py</code> <pre><code>def get_average_calibration_error(probs, y_test, n_bins):\n    \"\"\"\n    This is the weighted average discrepancy between the predicted risk and the\n    observed proportions on the calibration curve.\n\n    See \"https://towardsdatascience.com/expected-calibration-error-ece-a-step-\n    by-step-visual-explanation-with-python-code-c3e9aa12937d\" for a good\n    explanation.\n\n    The formula for estimated calibration error (ece) is:\n\n       ece = Sum over bins [samples_in_bin / N] * | P_observed - P_pred |,\n\n    where P_observed is the empirical proportion of positive samples in the\n    bin, and P_pred is the predicted probability for that bin. The results are\n    weighted by the number of samples in the bin (because some probabilities are\n    predicted more frequently than others).\n\n    The result is interpreted as an absolute error: i.e. a value of 0.1 means\n    that the calibration is out on average by 10%. It may be better to modify the\n    formula to compute an average relative error.\n\n    Testing: not yet tested.\n    \"\"\"\n\n    # There is one estimated calibration error for each model (the model under\n    # test and all the bootstrap models). These will be averaged at the end\n    estimated_calibration_errors = []\n\n    # The total number of samples is the number of rows in the probs array. This\n    # is used with the number of samples in the bins to weight the probability\n    # error\n    N = probs.shape[0]\n\n    bin_edges = np.linspace(0, 1, n_bins + 1)\n    for n in range(probs.shape[1]):\n\n        prob_true, prob_pred = calibration_curve(y_test, probs[:, n], n_bins=n_bins)\n\n        # For each prob_pred, need to count the number of samples in that lie in\n        # the bin centered at prob_pred.\n        bin_width = 1 / n_bins\n        count_in_bins = []\n        for prob in prob_pred:\n            bin_start = prob - bin_width / 2\n            bin_end = prob + bin_width / 2\n            count = ((bin_start &lt;= probs[:, n]) &amp; (probs[:, n] &lt; bin_end)).sum()\n            count_in_bins.append(count)\n        count_in_bins = np.array(count_in_bins)\n\n        error = np.sum(count_in_bins * np.abs(prob_true - prob_pred)) / N\n        estimated_calibration_errors.append(error)\n\n    return np.mean(estimated_calibration_errors)\n</code></pre>"},{"location":"reference/#pyhbr.analysis.calibration.get_calibration","title":"<code>get_calibration(probs, y_test, n_bins)</code>","text":"<p>Calculate the calibration of the fitted models</p> <p>Warning</p> <p>This function is deprecated. Use the variable bin width calibration function instead.</p> <p>Get the calibration curves for all models (whose probability predictions for the positive class are columns of probs) based on the outcomes in y_test. Rows of y_test correspond to rows of probs. The result is a list of pairs, one for each model (column of probs). Each pair contains the vector of x- and y-coordinates of the calibration curve.</p> <p>Parameters:</p> Name Type Description Default <code>probs</code> <code>DataFrame</code> <p>The dataframe of probabilities predicted by the model. The first column is the model-under-test (fitted on the training data) and the other columns are from the fits on the training data resamples.</p> required <code>y_test</code> <code>Series</code> <p>The outcomes corresponding to the predicted probabilities.</p> required <code>n_bins</code> <code>int</code> <p>The number of bins to group probability predictions into, for the purpose of averaging the observed frequency of outcome in the test set.</p> required <p>Returns:</p> Type Description <code>list[DataFrame]</code> <p>A list of DataFrames containing the calibration curves. Each DataFrame contains the columns <code>predicted</code> and <code>observed</code>.</p> Source code in <code>src\\pyhbr\\analysis\\calibration.py</code> <pre><code>def get_calibration(probs: DataFrame, y_test: Series, n_bins: int) -&gt; list[DataFrame]:\n    \"\"\"Calculate the calibration of the fitted models\n\n    !!! warning\n\n        This function is deprecated. Use the variable bin width calibration\n        function instead.\n\n    Get the calibration curves for all models (whose probability\n    predictions for the positive class are columns of probs) based\n    on the outcomes in y_test. Rows of y_test correspond to rows of\n    probs. The result is a list of pairs, one for each model (column\n    of probs). Each pair contains the vector of x- and y-coordinates\n    of the calibration curve.\n\n    Args:\n        probs: The dataframe of probabilities predicted by the model.\n            The first column is the model-under-test (fitted on the training\n            data) and the other columns are from the fits on the training\n            data resamples.\n        y_test: The outcomes corresponding to the predicted probabilities.\n        n_bins: The number of bins to group probability predictions into, for\n            the purpose of averaging the observed frequency of outcome in the\n            test set.\n\n    Returns:\n        A list of DataFrames containing the calibration curves. Each DataFrame\n            contains the columns `predicted` and `observed`.\n\n    \"\"\"\n    curves = []\n    for column in probs.columns:\n        prob_true, prob_pred = calibration_curve(y_test, probs[column], n_bins=n_bins)\n        df = DataFrame({\"predicted\": prob_pred, \"observed\": prob_true})\n        curves.append(df)\n    return curves\n</code></pre>"},{"location":"reference/#pyhbr.analysis.calibration.get_prevalence","title":"<code>get_prevalence(y_test)</code>","text":"<p>Estimate the prevalence in a set of outcomes</p> <p>To calculate model calibration, patients are grouped together into similar-risk groups. The prevalence of the outcome in each group is then compared to the predicted risk.</p> <p>The true risk of the outcome within each group is not known, but it is known what outcome occurred.</p> <p>One possible assumption is that the patients in each group all have the same risk, p. In this case, the outcomes from the group follow a Bernoulli distribution. The population parameter p (where the popopulation is all patients receiving risk predictions in this group) can be estimated simply using \\(\\hat{p} = N_\\text{outcome}/N_\\text{group_size}\\). Using a simple approach to calculate the confidence interval on this estimate, assuming a large enough sample size for normally distributed estimate of the mean, gives a CI of:</p> \\[ \\hat{p} \\pm 1.96\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{N_\\text{group_size}}} \\] <p>(See this answer for details.)</p> <p>However, the assumption of uniform risk within the models groups-of-equal-risk-prediction may not be valid, because it assumes that the model is predicting reasonably accurate risks, and the model is the item under test.</p> <p>One argument is that, if the estimated prevalence matches the risk of the group closely, then this may give evidence that the models predicted risks are accurate -- the alternative would be that the real risks follow a different distribution, whose mean happens (coincidentally) to coincide with the predicted risk. Such a conclusion may be possible if the confidence interval for the estimated prevalence is narrow, and agrees with the predicted risk closely.</p> <p>Without further assumptions, there is nothing further that can be said about the distribution of patient risks within each group. As a result, good calibration is a necessary, but not sufficient, condition for accurate risk predictions in the model .</p> <p>Parameters:</p> Name Type Description Default <code>y_test</code> <code>Series</code> <p>The (binary) outcomes in a single risk group. The values are True/False (boolean)</p> required <p>Returns:</p> Type Description <p>A map containing the key \"prevalence\", for the estimated mean of the Bernoulli distribution, and \"lower\" and \"upper\" for the estimated confidence interval, assuming all patients in the risk group are drawn from a single Bernoulii distribution. The \"variance\" is the estimate of the sample variance of the estimated prevalence, and can be used to form an average of the accuracy uncertainties in each bin.</p> <p>Note that the assumption of a Bernoulli distribution is not necessarily accurate.</p> Source code in <code>src\\pyhbr\\analysis\\calibration.py</code> <pre><code>def get_prevalence(y_test: Series):\n    \"\"\"Estimate the prevalence in a set of outcomes\n\n    To calculate model calibration, patients are grouped\n    together into similar-risk groups. The prevalence of\n    the outcome in each group is then compared to the\n    predicted risk.\n\n    The true risk of the outcome within each group is not\n    known, but it is known what outcome occurred.\n\n    One possible assumption is that the patients in each\n    group all have the same risk, p. In this case, the\n    outcomes from the group follow a Bernoulli\n    distribution. The population parameter p (where the\n    popopulation is all patients receiving risk predictions\n    in this group) can be estimated simply using\n    $\\hat{p} = N_\\\\text{outcome}/N_\\\\text{group_size}$.\n    Using a simple approach to calculate the confidence\n    interval on this estimate, assuming a large enough\n    sample size for normally distributed estimate of the\n    mean, gives a CI of:\n\n    $$\n    \\hat{p} \\pm 1.96\\sqrt{\\\\frac{\\hat{p}(1-\\hat{p})}{N_\\\\text{group_size}}}\n    $$\n\n    (See [this answer](https://stats.stackexchange.com/a/156807)\n    for details.)\n\n    However, the assumption of uniform risk within the\n    models groups-of-equal-risk-prediction may not be valid,\n    because it assumes that the model is predicting\n    reasonably accurate risks, and the model is the item\n    under test.\n\n    One argument is that, if the estimated prevalence matches\n    the risk of the group closely, then this may give evidence\n    that the models predicted risks are accurate -- the alternative\n    would be that the real risks follow a different distribution, whose\n    mean happens (coincidentally) to coincide with the predicted\n    risk. Such a conclusion may be possible if the confidence\n    interval for the estimated prevalence is narrow, and agrees\n    with the predicted risk closely.\n\n    Without further assumptions, there is nothing further that\n    can be said about the distribution of patient risks within\n    each group. As a result, good calibration is a necessary,\n    but not sufficient, condition for accurate risk\n    predictions in the model .\n\n    Args:\n        y_test: The (binary) outcomes in a single risk group.\n            The values are True/False (boolean)\n\n    Returns:\n        A map containing the key \"prevalence\", for the estimated\n            mean of the Bernoulli distribution, and \"lower\"\n            and \"upper\" for the estimated confidence interval,\n            assuming all patients in the risk group are drawn\n            from a single Bernoulii distribution. The \"variance\"\n            is the estimate of the sample variance of the estimated\n            prevalence, and can be used to form an average of\n            the accuracy uncertainties in each bin.\n\n            Note that the assumption of a Bernoulli distribution\n            is not necessarily accurate.\n    \"\"\"\n    n_group_size = len(y_test)\n    p_hat = np.mean(y_test)\n    variance = (p_hat * (1 - p_hat)) / n_group_size # square of standard error of Bernoulli\n    half_width = 1.96 * np.sqrt(variance) # Estimate of 95% confidence interval\n    return {\n        \"prevalence\": p_hat,\n        \"lower\": p_hat - half_width,\n        \"upper\": p_hat + half_width,\n        \"variance\": variance\n    }\n</code></pre>"},{"location":"reference/#pyhbr.analysis.calibration.get_variable_width_calibration","title":"<code>get_variable_width_calibration(probs, y_test, n_bins)</code>","text":"<p>Get variable-bin-width calibration curves</p> <p>Model predictions are arranged in ascending order, and then risk ranges are selected so that an equal number of predictions falls in each group. This means bin widths will be more granular at points where many patients are predicted the same risk. The risk bins are shown on the x-axis of calibration plots.</p> <p>In each bin, the proportion of patient with an event are calculated. This value, which is a function of each bin, is plotted on the y-axis of the calibration plot, and is a measure of the prevalence of the outcome in each bin. In a well calibrated model, this prevalence should match the mean risk prediction in the bin (the bin center).</p> <p>Note that a well-calibrated model is not a sufficient condition for correctness of risk predictions. One way that the prevalence of the bin can match the bin risk is for all true risks to roughly match the bin risk P. However, other ways are possible, for example, a proportion P of patients in the bin could have 100% risk, and the other have zero risk.</p> <p>Parameters:</p> Name Type Description Default <code>probs</code> <code>DataFrame</code> <p>Each column is the predictions from one of the resampled models. The first column corresponds to the model-under-test.</p> required <code>y</code> <p>Contains the observed outcomes.</p> required <code>n_bins</code> <code>int</code> <p>The number of (variable-width) bins to include.</p> required <p>Returns:</p> Type Description <code>list[DataFrame]</code> <p>A list of dataframes, one for each calibration curve. The \"bin_center\" column contains the central bin width; the \"bin_half_width\" column contains the half-width of each equal-risk group. The \"est_prev\" column contains the mean number of events in that bin; and the \"est_prev_err\" contains the half-width of the 95% confidence interval (symmetrical above and below bin_prev).</p> Source code in <code>src\\pyhbr\\analysis\\calibration.py</code> <pre><code>def get_variable_width_calibration(\n    probs: DataFrame, y_test: Series, n_bins: int\n) -&gt; list[DataFrame]:\n    \"\"\"Get variable-bin-width calibration curves\n\n    Model predictions are arranged in ascending order, and then risk ranges\n    are selected so that an equal number of predictions falls in each group.\n    This means bin widths will be more granular at points where many patients\n    are predicted the same risk. The risk bins are shown on the x-axis of\n    calibration plots.\n\n    In each bin, the proportion of patient with an event are calculated. This\n    value, which is a function of each bin, is plotted on the y-axis of the\n    calibration plot, and is a measure of the prevalence of the outcome in\n    each bin. In a well calibrated model, this prevalence should match the\n    mean risk prediction in the bin (the bin center).\n\n    Note that a well-calibrated model is not a sufficient condition for\n    correctness of risk predictions. One way that the prevalence of the\n    bin can match the bin risk is for all true risks to roughly match\n    the bin risk P. However, other ways are possible, for example, a\n    proportion P of patients in the bin could have 100% risk, and the\n    other have zero risk.\n\n\n    Args:\n        probs: Each column is the predictions from one of the resampled\n            models. The first column corresponds to the model-under-test.\n        y: Contains the observed outcomes.\n        n_bins: The number of (variable-width) bins to include.\n\n    Returns:\n        A list of dataframes, one for each calibration curve. The\n            \"bin_center\" column contains the central bin width;\n            the \"bin_half_width\" column contains the half-width\n            of each equal-risk group. The \"est_prev\" column contains\n            the mean number of events in that bin;\n            and the \"est_prev_err\" contains the half-width of the 95%\n            confidence interval (symmetrical above and below bin_prev).\n    \"\"\"\n\n    # Make the list that will contain the output calibration information\n    calibration_dfs = []\n\n    n_cols = probs.shape[1]\n    for n in range(n_cols):\n\n        # Get the probabilities predicted by one of the resampled\n        # models (stored as a column in probs)\n        col = probs.iloc[:, n].sort_values()\n\n        # Bin the predictions into variable-width risk\n        # ranges with equal numbers in each bin\n        n_bins = 5\n        samples_per_bin = int(np.ceil(len(col) / n_bins))\n        bins = []\n        for start in range(0, len(col), samples_per_bin):\n            end = start + samples_per_bin\n            bins.append(col[start:end])\n\n        # Get the bin centres and bin widths\n        bin_center = []\n        bin_half_width = []\n        for b in bins:\n            upper = b.max()\n            lower = b.min()\n            bin_center.append((upper + lower) / 2)\n            bin_half_width.append((upper - lower) / 2)\n\n        # Get the event prevalence in the bin\n        # Get the confidence intervals for each bin\n        est_prev = []\n        est_prev_err = []\n        est_prev_variance = []\n        actual_samples_per_bin = []\n        num_events = []\n        for b in bins:\n\n            # Get the outcomes corresponding to the current\n            # bin (group of equal predicted risk)\n            equal_risk_group = y_test.loc[b.index]\n\n            actual_samples_per_bin.append(len(b))\n            num_events.append(equal_risk_group.sum())\n\n            prevalence_ci = get_prevalence(equal_risk_group)\n            est_prev_err.append((prevalence_ci[\"upper\"] - prevalence_ci[\"lower\"]) / 2)\n            est_prev.append(prevalence_ci[\"prevalence\"])\n            est_prev_variance.append(prevalence_ci[\"variance\"])\n\n        # Add the data to the calibration list\n        df = DataFrame(\n            {\n                \"bin_center\": bin_center,\n                \"bin_half_width\": bin_half_width,\n                \"est_prev\": est_prev,\n                \"est_prev_err\": est_prev_err,\n                \"est_prev_variance\": est_prev_variance,\n                \"samples_per_bin\": actual_samples_per_bin,\n                \"num_events\": num_events,\n            }\n        )\n        calibration_dfs.append(df)\n\n    return calibration_dfs\n</code></pre>"},{"location":"reference/#pyhbr.analysis.calibration.make_error_boxes","title":"<code>make_error_boxes(ax, calibration)</code>","text":"<p>Plot error boxes and error bars around points</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>The axis on which to plot the error boxes.</p> required <code>calibration</code> <code>DataFrame</code> <p>Dataframe containing one row per bin, showing how the predicted risk compares to the estimated prevalence.</p> required Source code in <code>src\\pyhbr\\analysis\\calibration.py</code> <pre><code>def make_error_boxes(ax: Axes, calibration: DataFrame):\n    \"\"\"Plot error boxes and error bars around points\n\n    Args:\n        ax: The axis on which to plot the error boxes.\n        calibration: Dataframe containing one row per\n            bin, showing how the predicted risk compares\n            to the estimated prevalence.\n    \"\"\"\n\n    alpha = 0.3\n\n    c = calibration\n    for n in range(len(c)):\n        num_events = c.loc[n, \"num_events\"]\n        samples_in_bin = c.loc[n, \"samples_per_bin\"]\n\n        est_prev = 100 * c.loc[n, \"est_prev\"]\n        est_prev_err = 100 * c.loc[n, \"est_prev_err\"]\n        risk = 100 * c.loc[n, \"bin_center\"]\n        bin_half_width = 100 * c.loc[n, \"bin_half_width\"]\n\n        margin = 1.0\n        x = risk - margin * bin_half_width\n        y = est_prev - margin * est_prev_err\n        width = 2 * margin * bin_half_width\n        height = 2 * margin * est_prev_err\n\n        rect = Rectangle(\n            (x, y), width, height,\n            label=f\"Risk {risk:.2f}%, {num_events}/{samples_in_bin} events\",\n            alpha=alpha,\n            facecolor=cm.jet(n/len(c))\n        )\n        ax.add_patch(rect)\n\n    ax.errorbar(\n        x=100 * c[\"bin_center\"],\n        y=100 * c[\"est_prev\"],\n        xerr=100 * c[\"bin_half_width\"],\n        yerr=100 * c[\"est_prev_err\"],\n        fmt=\"None\",\n    )\n\n    ax.legend()\n</code></pre>"},{"location":"reference/#pyhbr.analysis.calibration.plot_calibration_curves","title":"<code>plot_calibration_curves(ax, curves, title='Stability of Calibration')</code>","text":"<p>Plot calibration curves for the model under test and resampled models</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>The axes on which to plot the calibration curves</p> required <code>curves</code> <code>list[DataFrame]</code> <p>A list of DataFrames containing the calibration curve data</p> required <code>title</code> <p>Title to add to the plot.</p> <code>'Stability of Calibration'</code> Source code in <code>src\\pyhbr\\analysis\\calibration.py</code> <pre><code>def plot_calibration_curves(\n    ax: Axes,\n    curves: list[DataFrame],\n    title=\"Stability of Calibration\",\n):\n    \"\"\"Plot calibration curves for the model under test and resampled models\n\n    Args:\n        ax: The axes on which to plot the calibration curves\n        curves: A list of DataFrames containing the calibration curve data\n        title: Title to add to the plot.\n    \"\"\"\n    mut_curve = curves[0]  # model-under-test\n    ax.plot(\n        100 * mut_curve[\"bin_center\"],\n        100 * mut_curve[\"est_prev\"],\n        label=\"Model-under-test\",\n        c=\"r\",\n    )\n    for curve in curves[1:]:\n        ax.plot(\n            100*curve[\"bin_center\"],\n            100*curve[\"est_prev\"],\n            label=\"Resample\",\n            c=\"b\",\n            linewidth=0.3,\n            alpha=0.4,\n        )\n\n    # Get the minimum and maximum for the x range\n    min_x = 100 * (curves[0][\"bin_center\"]).min()\n    max_x = 100 * (curves[0][\"bin_center\"]).max()\n\n    # Generate a dense straight line (smooth curve on log scale)\n    coords = np.linspace(min_x, max_x, num=50)\n    ax.plot(coords, coords, c=\"k\")\n\n    ax.legend([\"Model-under-test\", \"Bootstrapped models\"])\n\n    ax.set_xscale(\"log\")\n    ax.set_yscale(\"log\")\n    ax.xaxis.set_major_formatter(mtick.PercentFormatter())\n    ax.yaxis.set_major_formatter(mtick.PercentFormatter())\n    ax.set_ylabel(\"Estimated Prevalence\")\n    ax.set_xlabel(\"Model-Estimated Risks\")\n    ax.set_title(title)\n</code></pre>"},{"location":"reference/#pyhbr.analysis.calibration.plot_prediction_distribution","title":"<code>plot_prediction_distribution(ax, probs, n_bins)</code>","text":"<p>Plot the distribution of predicted probabilities over the models as a bar chart, with error bars showing the standard deviation of each model height. All model predictions (columns of probs) are given equal weight in the average; column 0 (the model under test) is not singled out in any way.</p> <p>The function plots vertical error bars that are one standard deviation up and down (so 2*sd in total)</p> Source code in <code>src\\pyhbr\\analysis\\calibration.py</code> <pre><code>def plot_prediction_distribution(ax, probs, n_bins):\n    \"\"\"\n    Plot the distribution of predicted probabilities over the models as\n    a bar chart, with error bars showing the standard deviation of each\n    model height. All model predictions (columns of probs) are given equal\n    weight in the average; column 0 (the model under test) is not singled\n    out in any way.\n\n    The function plots vertical error bars that are one standard deviation\n    up and down (so 2*sd in total)\n    \"\"\"\n    bin_edges = np.linspace(0, 1, n_bins + 1)\n    freqs = []\n    for j in range(probs.shape[1]):\n        f, _ = np.histogram(probs[:, j], bins=bin_edges)\n        freqs.append(f)\n    means = np.mean(freqs, axis=0)\n    sds = np.std(freqs, axis=0)\n\n    bin_centers = (bin_edges[1:] + bin_edges[:-1]) / 2\n\n    # Compute the bin width to leave a gap between bars\n    # of 20%\n    bin_width = 0.80 / n_bins\n\n    ax.bar(bin_centers, height=means, width=bin_width, yerr=2 * sds)\n    # ax.set_title(\"Distribution of predicted probabilities\")\n    ax.set_xlabel(\"Mean predicted probability\")\n    ax.set_ylabel(\"Count\")\n</code></pre>"},{"location":"reference/#pyhbr.analysis.describe","title":"<code>describe</code>","text":""},{"location":"reference/#pyhbr.analysis.describe.column_prop","title":"<code>column_prop(bool_col)</code>","text":"<p>Return a string with the number of non-zero items in the columns and a percentage</p> Source code in <code>src\\pyhbr\\analysis\\describe.py</code> <pre><code>def column_prop(bool_col):\n    \"\"\"Return a string with the number of non-zero items in the columns\n    and a percentage\n    \"\"\"\n    count = bool_col.sum()\n    percent = 100 * count / len(bool_col)\n    return f\"{count} ({percent:.2f}%)\"\n</code></pre>"},{"location":"reference/#pyhbr.analysis.describe.get_column_rates","title":"<code>get_column_rates(data)</code>","text":"<p>Get the proportion of rows in each column that are non-zero</p> <p>Either pass the full table, or subset it based on a condition to get the rates for that subset.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>A table containing columns where the proportion of non-zero rows should be calculated.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>A Series (single column) with one row per column in the original data, containing the rate of non-zero items in each column. The Series is indexed by the names of the columns, with \"_rate\" appended.</p> Source code in <code>src\\pyhbr\\analysis\\describe.py</code> <pre><code>def get_column_rates(data: DataFrame) -&gt; Series:\n    \"\"\"Get the proportion of rows in each column that are non-zero\n\n    Either pass the full table, or subset it based\n    on a condition to get the rates for that subset.\n\n    Args:\n        data: A table containing columns where the proportion\n            of non-zero rows should be calculated.\n\n    Returns:\n        A Series (single column) with one row per column in the\n            original data, containing the rate of non-zero items\n            in each column. The Series is indexed by the names of\n            the columns, with \"_rate\" appended.\n    \"\"\"\n    return Series(\n        {name + \"_rate\": proportion_nonzero(col) for name, col in data.items()}\n    ).sort_values()\n</code></pre>"},{"location":"reference/#pyhbr.analysis.describe.get_outcome_prevalence","title":"<code>get_outcome_prevalence(outcomes)</code>","text":"<p>Get the prevalence of each outcome as a percentage.</p> <p>This function takes the outcomes dataframe used to define the y vector of the training/testing set and calculates the prevalence of each outcome in a form suitable for inclusion in a report.</p> <p>Parameters:</p> Name Type Description Default <code>outcomes</code> <code>DataFrame</code> <p>A dataframe with the columns \"fatal_{outcome}\", \"non_fatal_{outcome}\", and \"{outcome}\" (for the total), where {outcome} is \"bleeding\" or \"ischaemia\". Each row is an index spell, and the elements in the table are boolean (whether or not the outcome occurred).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A table with the prevalence of each outcome, and a multi-index containing the \"Outcome\" (\"Bleeding\" or \"Ischaemia\"), and the outcome \"Type\" (fatal, total, etc.)</p> Source code in <code>src\\pyhbr\\analysis\\describe.py</code> <pre><code>def get_outcome_prevalence(outcomes: DataFrame) -&gt; DataFrame:\n    \"\"\"Get the prevalence of each outcome as a percentage.\n\n    This function takes the outcomes dataframe used to define\n    the y vector of the training/testing set and calculates the\n    prevalence of each outcome in a form suitable for inclusion\n    in a report.\n\n    Args:\n        outcomes: A dataframe with the columns \"fatal_{outcome}\",\n            \"non_fatal_{outcome}\", and \"{outcome}\" (for the total),\n            where {outcome} is \"bleeding\" or \"ischaemia\". Each row\n            is an index spell, and the elements in the table are\n            boolean (whether or not the outcome occurred).\n\n    Returns:\n        A table with the prevalence of each outcome, and a multi-index\n            containing the \"Outcome\" (\"Bleeding\" or \"Ischaemia\"), and\n            the outcome \"Type\" (fatal, total, etc.)\n    \"\"\"\n    df = (\n        100\n        * outcomes.rename(\n            columns={\n                \"bleeding\": \"Bleeding.Total\",\n                \"non_fatal_bleeding\": \"Bleeding.Non-Fatal (BARC 2-4)\",\n                \"fatal_bleeding\": \"Bleeding.Fatal (BARC 5)\",\n                \"ischaemia\": \"Ischaemia.Total\",\n                \"non_fatal_ischaemia\": \"Ischaemia.Non-Fatal (MI/Stroke)\",\n                \"fatal_ischaemia\": \"Ischaemia.Fatal (CV Death)\",\n            }\n        )\n        .melt(value_name=\"Prevalence (%)\")\n        .groupby(\"variable\")\n        .sum()\n        / len(outcomes)\n    )\n    df = df.reset_index()\n    df[[\"Outcome\", \"Type\"]] = df[\"variable\"].str.split(\".\", expand=True)\n    return df.set_index([\"Outcome\", \"Type\"])[[\"Prevalence (%)\"]].apply(\n        lambda x: round(x, 2)\n    )\n</code></pre>"},{"location":"reference/#pyhbr.analysis.describe.get_summary_table","title":"<code>get_summary_table(models, high_risk_thresholds, config)</code>","text":"<p>Get a table of model metric comparison across different models</p> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>dict[str, Any]</code> <p>A map from model names to model data (containing the key \"fit_results\")</p> required <code>high_risk_thresholds</code> <code>dict[str, float]</code> <p>A dictionary containing the keys \"bleeding\" and \"ischaemia\" mapped to the thresholds used to determine whether a patient is at high risk from the models.</p> required <code>config</code> <code>dict[str, Any]</code> <p>The config file used as input to the results and report generator scripts. It must contain the keys \"outcomes\" and \"models\", which are dictionaries containing the outcome or model name and a sub-key \"abbr\" which contains a short name of the outcome/model.</p> required Source code in <code>src\\pyhbr\\analysis\\describe.py</code> <pre><code>def get_summary_table(\n    models: dict[str, Any],\n    high_risk_thresholds: dict[str, float],\n    config: dict[str, Any],\n):\n    \"\"\"Get a table of model metric comparison across different models\n\n    Args:\n        models: A map from model names to model data (containing the\n            key \"fit_results\")\n        high_risk_thresholds: A dictionary containing the keys\n            \"bleeding\" and \"ischaemia\" mapped to the thresholds\n            used to determine whether a patient is at high risk\n            from the models.\n        config: The config file used as input to the results and\n            report generator scripts. It must contain the keys\n            \"outcomes\" and \"models\", which are dictionaries\n            containing the outcome or model name and a sub-key\n            \"abbr\" which contains a short name of the outcome/model.\n    \"\"\"\n    model_names = []\n    instabilities = []\n    aucs = []\n    risk_accuracy = []\n    low_risk_reclass = []\n    high_risk_reclass = []\n    model_key = []  # For identifying the model later\n    outcome_key = []  # For identifying the outcome later\n    median_auc = []  # Numerical AUC for finding the best model\n\n    for model, model_data in models.items():\n        for outcome in [\"bleeding\", \"ischaemia\"]:\n\n            model_key.append(model)\n            outcome_key.append(outcome)\n\n            fit_results = model_data[\"fit_results\"]\n\n            # Abbreviated model name\n            model_abbr = config[\"models\"][model][\"abbr\"]\n            outcome_abbr = config[\"outcomes\"][outcome][\"abbr\"]\n            model_names.append(f\"{model_abbr}-{outcome_abbr}\")\n\n            probs = fit_results[\"probs\"]\n\n            # Get the summary instabilities\n            instability = stability.average_absolute_instability(probs[outcome])\n            instabilities.append(common.median_to_string(instability))\n\n            # Get the summary calibration accuracies\n            calibrations = fit_results[\"calibrations\"][outcome]\n\n            # Join together all the calibration data for the primary model\n            # and all the bootstrap models, to compare the bin center positions\n            # with the estimated prevalence for all bins.\n            all_calibrations = pd.concat(calibrations)\n\n            # Average relative error where prevalence is non-zero\n            accuracy_mean = 0\n            accuracy_variance = 0\n            count = 0\n            for n in range(len(all_calibrations)):\n                if all_calibrations[\"est_prev\"].iloc[n] &gt; 0:\n\n                    # This assumes that all risk predictions in the bin are at the bin center, with no\n                    # distribution (i.e. the result is normal with a distribution based on the sample\n                    # mean of the prevalence. For more accuracy, consider using the empirical distribution\n                    # of the risk predictions in the bin as the basis for this calculation.\n                    accuracy_mean += np.abs(\n                        all_calibrations[\"bin_center\"].iloc[n]\n                        - all_calibrations[\"est_prev\"].iloc[n]\n                    )\n\n                    # When adding normal distributions together, the variances sum.\n                    accuracy_variance += all_calibrations[\"est_prev_variance\"].iloc[n]\n\n                    count += 1\n            accuracy_mean /= count\n            accuracy_variance /= count\n\n            # Calculate a 95% confidence interval for the resulting mean of the accuracies,\n            # assuming all the distributions are normal.\n            ci_upper = accuracy_mean + 1.96 * np.sqrt(accuracy_variance)\n            ci_lower = accuracy_mean - 1.96 * np.sqrt(accuracy_variance)\n            risk_accuracy.append(\n                f\"{100*accuracy_mean:.2f}%, CI [{100*ci_lower:.2f}%, {100*ci_upper:.2f}%]\"\n            )\n\n            threshold = high_risk_thresholds[outcome]\n            y_test = model_data[\"y_test\"][outcome]\n            df = stability.get_reclass_probabilities(probs[outcome], y_test, threshold)\n            high_risk = (df[\"original_risk\"] &gt;= threshold).sum()\n            high_risk_and_unstable = (\n                (df[\"original_risk\"] &gt;= threshold) &amp; (df[\"unstable_prob\"] &gt;= 0.5)\n            ).sum()\n            high_risk_reclass.append(f\"{100 * high_risk_and_unstable / high_risk:.2f}%\")\n            low_risk = (df[\"original_risk\"] &lt; threshold).sum()\n            low_risk_and_unstable = (\n                (df[\"original_risk\"] &lt; threshold) &amp; (df[\"unstable_prob\"] &gt;= 0.5)\n            ).sum()\n            low_risk_reclass.append(f\"{100 * low_risk_and_unstable / low_risk:.2f}%\")\n\n            # Get the summary ROC AUCs\n            auc_data = fit_results[\"roc_aucs\"][outcome]\n            auc_spread = Series(\n                auc_data.resample_auc + [auc_data.model_under_test_auc]\n            ).quantile([0.025, 0.5, 0.975])\n            aucs.append(common.median_to_string(auc_spread, unit=\"\"))\n            median_auc.append(auc_spread[0.5])\n\n    return DataFrame(\n        {\n            \"Model\": model_names,\n            \"Spread of Instability\": instabilities,\n            \"H\u2192L\": high_risk_reclass,\n            \"L\u2192H\": low_risk_reclass,\n            \"Estimated Risk Uncertainty\": risk_accuracy,\n            \"ROC AUC\": aucs,\n            \"model_key\": model_key,\n            \"outcome_key\": outcome_key,\n            \"median_auc\": median_auc,\n        }\n    ).set_index(\"Model\", drop=True)\n</code></pre>"},{"location":"reference/#pyhbr.analysis.describe.nearly_constant","title":"<code>nearly_constant(data, threshold)</code>","text":"<p>Check which columns of the input table have low variation</p> <p>A column is considered low variance if the proportion of rows containing NA or the most common non-NA value exceeds threshold. For example, if NA and one other value together comprise 99% of the column, then it is considered to be low variance based on a threshold of 0.9.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The table to check for zero variance</p> required <code>threshold</code> <code>float</code> <p>The proportion of the column that must be NA or the most common value above which the column is considered low variance.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>A Series containing bool, indexed by the column name in the original data, containing whether the column has low variance.</p> Source code in <code>src\\pyhbr\\analysis\\describe.py</code> <pre><code>def nearly_constant(data: DataFrame, threshold: float) -&gt; Series:\n    \"\"\"Check which columns of the input table have low variation\n\n    A column is considered low variance if the proportion of rows\n    containing NA or the most common non-NA value exceeds threshold.\n    For example, if NA and one other value together comprise 99% of\n    the column, then it is considered to be low variance based on\n    a threshold of 0.9.\n\n    Args:\n        data: The table to check for zero variance\n        threshold: The proportion of the column that must be NA or\n            the most common value above which the column is considered\n            low variance.\n\n    Returns:\n        A Series containing bool, indexed by the column name\n            in the original data, containing whether the column\n            has low variance.\n    \"\"\"\n\n    def low_variance(column: Series) -&gt; bool:\n\n        if len(column) == 0:\n            # If the column has length zero, consider\n            # it low variance\n            return True\n\n        if len(column.dropna()) == 0:\n            # If the column is all-NA, it is low variance\n            # independently of the threshold\n            return True\n\n        # Else, if the proportion of NA and the most common\n        # non-NA value is higher than threshold, the column\n        # is low variance\n        na_count = column.isna().sum()\n        counts = column.value_counts()\n        most_common_value_count = counts.iloc[0]\n        if (na_count + most_common_value_count) / len(column) &gt; threshold:\n            return True\n\n        return False\n\n    return data.apply(low_variance).rename(\"nearly_constant\")\n</code></pre>"},{"location":"reference/#pyhbr.analysis.describe.plot_arc_hbr_survival","title":"<code>plot_arc_hbr_survival(ax, data)</code>","text":"<p>Plot survival curves for bleeding by ARC HBR score.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <p>List of two axes objects</p> required <code>data</code> <p>A loaded data file</p> required <code>config</code> <p>The analysis config (from yaml)</p> required Source code in <code>src\\pyhbr\\analysis\\describe.py</code> <pre><code>def plot_arc_hbr_survival(ax, data, ):\n    \"\"\"Plot survival curves for bleeding by ARC HBR score.\n\n    Args:\n        ax: List of two axes objects\n        data: A loaded data file\n        config: The analysis config (from yaml)\n    \"\"\"\n\n    # Get bleeding survival data\n    survival = data[\"bleeding_survival\"]\n    features_index = data[\"features_index\"]\n    arc_hbr_score = data[\"arc_hbr_score\"]\n    arc_hbr_score[\"score\"] = pd.cut(\n        data[\"arc_hbr_score\"][\"total_score\"],\n        [0, 1, 2, 100],\n        labels=[\"Score = 0\", \"0 &lt; Score &lt;= 1\", \"Score &gt; 1\"],\n        right=False,\n    )\n\n    def masked_survival(survival, mask):\n        masked_survival = survival[mask]\n        status = ~masked_survival[\"right_censor\"]\n        survival_in_days = masked_survival[\"time_to_event\"].dt.days\n        return kaplan_meier_estimator(status, survival_in_days, conf_type=\"log-log\")\n\n    def add_arc_survival(ax, arc_mask, label, color):\n        time, survival_prob, conf_int = masked_survival(survival, arc_mask)\n\n        ax[0].step(time, survival_prob, where=\"post\", color=color)\n        ax[0].fill_between(\n            time,\n            conf_int[0],\n            conf_int[1],\n            alpha=0.25,\n            step=\"post\",\n            label=label,\n            color=color,\n        )\n\n    df = (\n        features_index[[\"therapy\"]]\n        .fillna(\"Missing\")\n        .merge(arc_hbr_score[[\"score\"]], how=\"left\", on=\"spell_id\")\n        .groupby([\"therapy\", \"score\"], as_index=False)\n        .size()\n    )\n    df[\"score_sum\"] = df.groupby(\"score\")[\"size\"].transform(sum)\n    df[\"percent\"] = 100 * df[\"size\"] / df[\"score_sum\"]\n    print(df.sort_values([\"score\", \"therapy\"]))\n    df = df.rename(\n        columns={\"therapy\": \"Therapy\", \"percent\": \"Percent\", \"score\": \"Score\"}\n    ).drop(columns=[\"size\", \"score_sum\"])\n    print(df)\n\n    # Set custom order of therapy (least to most aggressive)\n    df[\"Therapy\"] = pd.Categorical(df[\"Therapy\"], [\"Single\", \"DAPT-AC\", \"DAPT-AP\", \"DAPT-AT\", \"Triple\", \"Missing\"])\n    df = df.sort_values(\"Therapy\")\n\n    # Plot the distribution of therapies\n    sns.barplot(\n        data=df,\n        x=\"Therapy\",\n        y=\"Percent\",\n        hue=\"Score\",\n        ax=ax[1],\n        palette={\n            \"Score = 0\": \"tab:green\",\n            \"0 &lt; Score &lt;= 1\": \"tab:orange\",\n            \"Score &gt; 1\": \"tab:red\",\n        },\n    )\n\n    # Plot survival curves by ARC score\n    arc_mask = arc_hbr_score[\"score\"] == \"Score = 0\"\n    add_arc_survival(ax, arc_mask, \"Score = 0\", \"tab:green\")\n    arc_mask = arc_hbr_score[\"score\"] == \"0 &lt; Score &lt;= 1\"\n    add_arc_survival(ax, arc_mask, \"0 &lt; Score &lt;= 1\", \"tab:orange\")\n    arc_mask = arc_hbr_score[\"score\"] == \"Score &gt; 1\"\n    add_arc_survival(ax, arc_mask, \"Score &gt; 1\", \"tab:red\")\n\n    ax[0].set_ylim(0.90, 1.00)\n    ax[0].set_ylabel(r\"Est. probability of no adverse event\")\n    ax[0].set_xlabel(\"Time (days since index ACS admission)\")\n    ax[0].set_title(\"Bleeding outcome survival curves by ARC HBR score\")\n    ax[0].legend()\n</code></pre>"},{"location":"reference/#pyhbr.analysis.describe.plot_clinical_code_distribution","title":"<code>plot_clinical_code_distribution(ax, data, config)</code>","text":"<p>Plot histograms of the distribution of bleeding/ischaemia codes</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <p>A list of two axes objects</p> required <code>data</code> <p>A loaded data file</p> required <code>config</code> <p>The analysis config (from yaml)</p> required Source code in <code>src\\pyhbr\\analysis\\describe.py</code> <pre><code>def plot_clinical_code_distribution(ax, data, config):\n    \"\"\"Plot histograms of the distribution of bleeding/ischaemia codes\n\n    Args:\n        ax: A list of two axes objects\n        data: A loaded data file\n        config: The analysis config (from yaml)\n    \"\"\"\n\n    bleeding_group = config[\"outcomes\"][\"bleeding\"][\"non_fatal\"][\"group\"]\n    ischaemia_group = config[\"outcomes\"][\"ischaemia\"][\"non_fatal\"][\"group\"]\n\n    # Set the quantile level to find a cut-off that includes most codes\n    level = 0.95\n\n    codes = data[\"codes\"]\n    bleeding_codes = codes[codes[\"group\"].eq(bleeding_group)][\"position\"]\n    bleeding_codes.hist(ax=ax[0], rwidth=0.9)\n    ax[0].set_title(\"Bleeding Codes\")\n    ax[0].set_xlabel(\"Code position (1 is primary, &gt; 1 is secondary)\")\n    ax[0].set_ylabel(\"Total Code Count\")\n\n    q = bleeding_codes.quantile(level)\n    ax[0].axvline(q)\n    ax[0].text(\n        q + 0.5,\n        0.5,\n        f\"{100*level:.0f}% quantile\",\n        rotation=90,\n        transform=ax[0].get_xaxis_transform(),\n    )\n\n    ischaemia_codes = codes[codes[\"group\"].eq(ischaemia_group)][\"position\"]\n    ischaemia_codes.hist(ax=ax[1], rwidth=0.9)\n    ax[1].set_title(\"Ischaemia Codes\")\n    ax[1].set_xlabel(\"Code position\")\n    ax[1].set_ylabel(\"Total Code Count\")\n\n    q = ischaemia_codes.quantile(level)\n    ax[1].axvline(q)\n    ax[1].text(\n        q + 0.5,\n        0.5,\n        f\"{100*level:.0f}% quantile\",\n        rotation=90,\n        transform=ax[1].get_xaxis_transform(),\n    )\n\n    plt.suptitle(\"Distribution of Bleeding/Ischaemia ICD-10 Primary/Secondary Codes\")\n    plt.tight_layout()\n</code></pre>"},{"location":"reference/#pyhbr.analysis.describe.plot_survival_curves","title":"<code>plot_survival_curves(ax, data, config)</code>","text":"<p>Plot survival curves for bleeding/ischaemia broken down by age</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <p>A list of two axes objects</p> required <code>data</code> <p>A loaded data file</p> required <code>config</code> <p>The analysis config (from yaml)</p> required Source code in <code>src\\pyhbr\\analysis\\describe.py</code> <pre><code>def plot_survival_curves(ax, data, config):\n    \"\"\"Plot survival curves for bleeding/ischaemia broken down by age\n\n    Args:\n        ax: A list of two axes objects\n        data: A loaded data file\n        config: The analysis config (from yaml)\n    \"\"\"\n\n    # Mask the dataset by age to get different survival plots\n    features_index = data[\"features_index\"]\n    print(features_index)\n    age_over_75 = features_index[\"age\"] &gt; 75\n\n    # Get bleeding survival data\n    survival = data[\"bleeding_survival\"].merge(\n        features_index, on=\"spell_id\", how=\"left\"\n    )\n\n    # Calculate survival curves for bleeding (over 75)\n    masked = survival[survival[\"age\"] &gt;= 75]\n    status = ~masked[\"right_censor\"]\n    survival_in_days = masked[\"time_to_event\"].dt.days\n    time, survival_prob, conf_int = kaplan_meier_estimator(\n        status, survival_in_days, conf_type=\"log-log\"\n    )\n    ax[0].step(time, survival_prob, where=\"post\", label=\"Age &gt;= 75\")\n    ax[0].fill_between(time, conf_int[0], conf_int[1], alpha=0.25, step=\"post\")\n\n    # Now for under 75\n    masked = survival[survival[\"age\"] &lt; 75]\n    status = ~masked[\"right_censor\"]\n    survival_in_days = masked[\"time_to_event\"].dt.days\n    time, survival_prob, conf_int = kaplan_meier_estimator(\n        status, survival_in_days, conf_type=\"log-log\"\n    )\n    ax[0].step(time, survival_prob, where=\"post\", label=\"Age &lt; 75\")\n    ax[0].fill_between(time, conf_int[0], conf_int[1], alpha=0.25, step=\"post\")\n\n    ax[0].set_ylim(0.75, 1.00)\n    ax[0].set_ylabel(r\"Est. probability of no adverse event\")\n    ax[0].set_xlabel(\"Time (days)\")\n    ax[0].set_title(\"Bleeding Outcome\")\n    ax[0].legend()\n\n    # Get ischaemia survival data\n    survival = data[\"ischaemia_survival\"].merge(\n        features_index, on=\"spell_id\", how=\"left\"\n    )\n\n    # Calculate survival curves for ischaemia (over 75)\n    masked = survival[survival[\"age\"] &gt;= 75]\n    status = ~masked[\"right_censor\"]\n    survival_in_days = masked[\"time_to_event\"].dt.days\n    time, survival_prob, conf_int = kaplan_meier_estimator(\n        status, survival_in_days, conf_type=\"log-log\"\n    )\n    ax[1].step(time, survival_prob, where=\"post\", label=\"Age &gt;= 75\")\n    ax[1].fill_between(time, conf_int[0], conf_int[1], alpha=0.25, step=\"post\")\n\n    # Now for under 75\n    masked = survival[survival[\"age\"] &lt; 75]\n    status = ~masked[\"right_censor\"]\n    survival_in_days = masked[\"time_to_event\"].dt.days\n    time, survival_prob, conf_int = kaplan_meier_estimator(\n        status, survival_in_days, conf_type=\"log-log\"\n    )\n    ax[1].step(time, survival_prob, where=\"post\", label=\"Age &lt; 75\")\n    ax[1].fill_between(time, conf_int[0], conf_int[1], alpha=0.25, step=\"post\")\n\n    ax[1].set_ylim(0.75, 1.00)\n    ax[1].set_ylabel(r\"Est. probability of no adverse event\")\n    ax[1].set_xlabel(\"Time (days)\")\n    ax[1].set_title(\"Ischaemia Outcome\")\n    ax[1].legend()\n\n    plt.tight_layout()\n</code></pre>"},{"location":"reference/#pyhbr.analysis.describe.proportion_missingness","title":"<code>proportion_missingness(data)</code>","text":"<p>Get the proportion of missing values in each column</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>A table where missingness should be calculate for each column</p> required <p>Returns:</p> Type Description <code>Series</code> <p>The proportion of missing values in each column, indexed by the original table column name. The values are sorted in order of increasing missingness</p> Source code in <code>src\\pyhbr\\analysis\\describe.py</code> <pre><code>def proportion_missingness(data: DataFrame) -&gt; Series:\n    \"\"\"Get the proportion of missing values in each column\n\n    Args:\n        data: A table where missingness should be calculate\n            for each column\n\n    Returns:\n        The proportion of missing values in each column, indexed\n            by the original table column name. The values are sorted\n            in order of increasing missingness\n    \"\"\"\n    return (data.isna().sum() / len(data)).sort_values().rename(\"missingness\")\n</code></pre>"},{"location":"reference/#pyhbr.analysis.describe.proportion_nonzero","title":"<code>proportion_nonzero(column)</code>","text":"<p>Get the proportion of non-zero values in a column</p> Source code in <code>src\\pyhbr\\analysis\\describe.py</code> <pre><code>def proportion_nonzero(column: Series) -&gt; float:\n    \"\"\"Get the proportion of non-zero values in a column\"\"\"\n    return (column &gt; 0).sum() / len(column)\n</code></pre>"},{"location":"reference/#pyhbr.analysis.describe.pvalue_chi2_high_risk_vs_outcome","title":"<code>pvalue_chi2_high_risk_vs_outcome(probs, y_test, high_risk_threshold)</code>","text":"<p>Perform a Chi-2 hypothesis test on the contingency between estimated high risk and outcome</p> <p>Get the p-value from the hypothesis test that there is no association between the estimated high-risk category, and the outcome. The p-value is interpreted as the probability of getting obtaining the outcomes corresponding to the model's estimated high-risk category under the assumption that there is no association between the two.</p> <p>Parameters:</p> Name Type Description Default <code>probs</code> <code>DataFrame</code> <p>The model-estimated probabilities (first column is used)</p> required <code>y_test</code> <code>Series</code> <p>Whether the outcome occurred</p> required <code>high_risk_threshold</code> <code>float</code> <p>The cut-off risk (probability) defining an estimate to be high risk.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The p-value for the hypothesis test.</p> Source code in <code>src\\pyhbr\\analysis\\describe.py</code> <pre><code>def pvalue_chi2_high_risk_vs_outcome(\n    probs: DataFrame, y_test: Series, high_risk_threshold: float\n) -&gt; float:\n    \"\"\"Perform a Chi-2 hypothesis test on the contingency between estimated high risk and outcome\n\n    Get the p-value from the hypothesis test that there is no association\n    between the estimated high-risk category, and the outcome. The p-value\n    is interpreted as the probability of getting obtaining the outcomes\n    corresponding to the model's estimated high-risk category under the\n    assumption that there is no association between the two.\n\n    Args:\n        probs: The model-estimated probabilities (first column is used)\n        y_test: Whether the outcome occurred\n        high_risk_threshold: The cut-off risk (probability) defining an\n            estimate to be high risk.\n\n    Returns:\n        The p-value for the hypothesis test.\n    \"\"\"\n\n    # Get the cases (True) where the model estimated a risk\n    # that puts the patient in the high risk category\n    estimated_high_risk = (probs.iloc[:, 0] &gt; high_risk_threshold).rename(\n        \"estimated_high_risk\"\n    )\n\n    # Get the instances (True) in the test set where the outcome\n    # occurred\n    outcome_occurred = y_test.rename(\"outcome_occurred\")\n\n    # Create a contingency table of the estimated high risk\n    # vs. whether the outcome occurred.\n    table = pd.crosstab(estimated_high_risk, outcome_occurred)\n\n    # Hypothesis test whether the estimated high risk category\n    # is related to the outcome (null hypothesis is that there\n    # is no relation).\n    return scipy.stats.chi2_contingency(table.to_numpy()).pvalue\n</code></pre>"},{"location":"reference/#pyhbr.analysis.dim_reduce","title":"<code>dim_reduce</code>","text":"<p>Functions for dimension-reduction of clinical codes</p>"},{"location":"reference/#pyhbr.analysis.dim_reduce.Dataset","title":"<code>Dataset</code>  <code>dataclass</code>","text":"<p>Stores either the train or test set</p> Source code in <code>src\\pyhbr\\analysis\\dim_reduce.py</code> <pre><code>@dataclass\nclass Dataset:\n    \"\"\"Stores either the train or test set\"\"\"\n\n    y: DataFrame\n    X_manual: DataFrame\n    X_reduce: DataFrame\n</code></pre>"},{"location":"reference/#pyhbr.analysis.dim_reduce.make_full_pipeline","title":"<code>make_full_pipeline(model, reducer=None)</code>","text":"<p>Make a model pipeline from the model part and dimension reduction</p> <p>This pipeline has one or two steps:</p> <ul> <li>If no reduction is performed, the only step is \"model\"</li> <li>If dimension reduction is performed, the steps are \"reducer\", \"model\"</li> </ul> <p>This function can be used to make the pipeline with no dimension (pass None to reducer). Otherwise, pass the reducer which will reduce a subset of the columns before fitting the model (use make_column_transformer to create this argument).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Pipeline</code> <p>A list of model fitting steps that should be applied after the (optional) dimension reduction.</p> required <code>reducer</code> <code>Pipeline</code> <p>If non-None, this reduction pipeline is applied before the model to reduce a subset of the columns.</p> <code>None</code> <p>Returns:</p> Type Description <code>Pipeline</code> <p>A scikit-learn pipeline that can be fitted to training data.</p> Source code in <code>src\\pyhbr\\analysis\\dim_reduce.py</code> <pre><code>def make_full_pipeline(model: Pipeline, reducer: Pipeline = None) -&gt; Pipeline:\n    \"\"\"Make a model pipeline from the model part and dimension reduction\n\n    This pipeline has one or two steps:\n\n    * If no reduction is performed, the only step is \"model\"\n    * If dimension reduction is performed, the steps are \"reducer\", \"model\"\n\n    This function can be used to make the pipeline with no dimension\n    (pass None to reducer). Otherwise, pass the reducer which will reduce\n    a subset of the columns before fitting the model (use make_column_transformer\n    to create this argument).\n\n    Args:\n        model: A list of model fitting steps that should be applied\n            after the (optional) dimension reduction.\n        reducer: If non-None, this reduction pipeline is applied before\n            the model to reduce a subset of the columns.\n\n    Returns:\n        A scikit-learn pipeline that can be fitted to training data.\n    \"\"\"\n    if reducer is not None:\n        return Pipeline([(\"reducer\", reducer), (\"model\", model)])\n    else:\n        return Pipeline([(\"model\", model)])\n</code></pre>"},{"location":"reference/#pyhbr.analysis.dim_reduce.make_grad_boost","title":"<code>make_grad_boost(random_state)</code>","text":"<p>Make a new gradient boosting classifier</p> <p>Returns:</p> Type Description <code>Pipeline</code> <p>The unfitted pipeline for the gradient boosting classifier</p> Source code in <code>src\\pyhbr\\analysis\\dim_reduce.py</code> <pre><code>def make_grad_boost(random_state: RandomState) -&gt; Pipeline:\n    \"\"\"Make a new gradient boosting classifier\n\n    Returns:\n        The unfitted pipeline for the gradient boosting classifier\n    \"\"\"\n    random_forest = GradientBoostingClassifier(\n        n_estimators=100, max_depth=10, random_state=random_state\n    )\n    return Pipeline([(\"model\", random_forest)])\n</code></pre>"},{"location":"reference/#pyhbr.analysis.dim_reduce.make_logistic_regression","title":"<code>make_logistic_regression(random_state)</code>","text":"<p>Make a new logistic regression model</p> <p>The model involves scaling all predictors and then applying a logistic regression model.</p> <p>Returns:</p> Type Description <code>Pipeline</code> <p>The unfitted pipeline for the logistic regression model</p> Source code in <code>src\\pyhbr\\analysis\\dim_reduce.py</code> <pre><code>def make_logistic_regression(random_state: RandomState) -&gt; Pipeline:\n    \"\"\"Make a new logistic regression model\n\n    The model involves scaling all predictors and then\n    applying a logistic regression model.\n\n    Returns:\n        The unfitted pipeline for the logistic regression model\n    \"\"\"\n\n    scaler = StandardScaler()\n    logreg = LogisticRegression(random_state=random_state)\n    return Pipeline([(\"scaler\", scaler), (\"model\", logreg)])\n</code></pre>"},{"location":"reference/#pyhbr.analysis.dim_reduce.make_random_forest","title":"<code>make_random_forest(random_state)</code>","text":"<p>Make a new random forest model</p> <p>Returns:</p> Type Description <code>Pipeline</code> <p>The unfitted pipeline for the random forest model</p> Source code in <code>src\\pyhbr\\analysis\\dim_reduce.py</code> <pre><code>def make_random_forest(random_state: RandomState) -&gt; Pipeline:\n    \"\"\"Make a new random forest model\n\n    Returns:\n        The unfitted pipeline for the random forest model\n    \"\"\"\n    random_forest = RandomForestClassifier(\n        n_estimators=100, max_depth=10, random_state=random_state\n    )\n    return Pipeline([(\"model\", random_forest)])\n</code></pre>"},{"location":"reference/#pyhbr.analysis.dim_reduce.make_reducer_pipeline","title":"<code>make_reducer_pipeline(reducer, cols_to_reduce)</code>","text":"<p>Make a wrapper that applies dimension reduction to a subset of columns.</p> <p>A column transformer is necessary if only some of the columns should be dimension-reduced, and others should be preserved. The resulting pipeline is intended for use in a scikit-learn pipeline taking a pandas DataFrame as input (where a subset of the columns are cols_to_reduce).</p> <p>Parameters:</p> Name Type Description Default <code>reducer</code> <p>The dimension reduction model to use for reduction</p> required <code>cols_to_reduce</code> <code>list[str]</code> <p>The list of column names to reduce</p> required <p>Returns:</p> Type Description <code>Pipeline</code> <p>A pipeline which contains the column_transformer that applies the reducer to cols_to_reduce. This can be included as a step in a larger pipeline.</p> Source code in <code>src\\pyhbr\\analysis\\dim_reduce.py</code> <pre><code>def make_reducer_pipeline(reducer, cols_to_reduce: list[str]) -&gt; Pipeline:\n    \"\"\"Make a wrapper that applies dimension reduction to a subset of columns.\n\n    A column transformer is necessary if only some of the columns should be\n    dimension-reduced, and others should be preserved. The resulting pipeline\n    is intended for use in a scikit-learn pipeline taking a pandas DataFrame as\n    input (where a subset of the columns are cols_to_reduce).\n\n    Args:\n        reducer: The dimension reduction model to use for reduction\n        cols_to_reduce: The list of column names to reduce\n\n    Returns:\n        A pipeline which contains the column_transformer that applies the\n            reducer to cols_to_reduce. This can be included as a step in a\n            larger pipeline.\n    \"\"\"\n    column_transformer = ColumnTransformer(\n        [(\"reducer\", reducer, cols_to_reduce)],\n        remainder=\"passthrough\",\n        verbose_feature_names_out=True,\n    )\n    return Pipeline([(\"column_transformer\", column_transformer)])\n</code></pre>"},{"location":"reference/#pyhbr.analysis.dim_reduce.prepare_train_test","title":"<code>prepare_train_test(data_manual, data_reduce, random_state)</code>","text":"<p>Make the test/train datasets for manually-chosen groups and high-dimensional data</p> <p>Parameters:</p> Name Type Description Default <code>data_manual</code> <code>DataFrame</code> <p>The dataset with manually-chosen code groups</p> required <code>data_reduce</code> <code>DataFrame</code> <p>The high-dimensional dataset</p> required <code>random_state</code> <code>RandomState</code> <p>The random state to pick the test/train split</p> required <p>Returns:</p> Type Description <code>(Dataset, Dataset)</code> <p>A tuple (train, test) containing the datasets to be used for training and testing the models. Both contain the outcome y along with the features for both the manually-chosen code groups and the data for dimension reduction.</p> Source code in <code>src\\pyhbr\\analysis\\dim_reduce.py</code> <pre><code>def prepare_train_test(\n    data_manual: DataFrame, data_reduce: DataFrame, random_state: RandomState\n) -&gt; (Dataset, Dataset):\n    \"\"\"Make the test/train datasets for manually-chosen groups and high-dimensional data\n\n    Args:\n        data_manual: The dataset with manually-chosen code groups\n        data_reduce: The high-dimensional dataset\n        random_state: The random state to pick the test/train split\n\n    Returns:\n        A tuple (train, test) containing the datasets to be used for training and\n            testing the models. Both contain the outcome y along with the features\n            for both the manually-chosen code groups and the data for dimension\n            reduction.\n    \"\"\"\n\n    # Check number of rows match\n    if data_manual.shape[0] != data_reduce.shape[0]:\n        raise RuntimeError(\n            \"The number of rows in data_manual and data_reduce do not match.\"\n        )\n\n    test_set_proportion = 0.25\n\n    # First, get the outcomes (y) from the dataframe. This is the\n    # source of test/train outcome data, and is used for both the\n    # manual and UMAP models. Just interested in whether bleeding\n    # occurred (not number of occurrences) for this experiment\n    outcome_name = \"bleeding_al_ani_outcome\"\n    y = data_manual[outcome_name]\n\n    # Get the set of manual code predictors (X0) to use for the\n    # first logistic regression model (all the columns with names\n    # ending in \"_before\").\n    X_manual = data_manual.drop(columns=[outcome_name])\n\n    # Make a random test/train split.=\n    X_train_manual, X_test_manual, y_train, y_test = train_test_split(\n        X_manual, y, test_size=test_set_proportion, random_state=random_state\n    )\n\n    # Extract the test/train sets from the UMAP data based on\n    # the index of the training set for the manual codes\n    X_reduce = data_reduce.drop(columns=[outcome_name])\n    X_train_reduce = X_reduce.loc[X_train_manual.index]\n    X_test_reduce = X_reduce.loc[X_test_manual.index]\n\n    # Store the test/train data together\n    train = Dataset(y_train, X_train_manual, X_train_reduce)\n    test = Dataset(y_test, X_test_manual, X_test_reduce)\n\n    return train, test\n</code></pre>"},{"location":"reference/#pyhbr.analysis.fit","title":"<code>fit</code>","text":""},{"location":"reference/#pyhbr.analysis.fit.fit_model","title":"<code>fit_model(pipe, X_train, y_train, X_test, y_test, num_bootstraps, num_bins, random_state)</code>","text":"<p>Fit the model and bootstrap models, and calculate model performance</p> <p>Parameters:</p> Name Type Description Default <code>pipe</code> <code>Pipeline</code> <p>The model pipeline to fit</p> required <code>X_train</code> <code>DataFrame</code> <p>Training features</p> required <code>y_train</code> <code>DataFrame</code> <p>Training outcomes (containing \"bleeding\"/\"ischaemia\" columns)</p> required <code>X_test</code> <code>DataFrame</code> <p>Test features</p> required <code>y_test</code> <code>DataFrame</code> <p>Test outcomes</p> required <code>num_bootstraps</code> <code>int</code> <p>The number of resamples of the training set to use to fit bootstrap models.</p> required <code>num_bins</code> <code>int</code> <p>The number of equal-size bins to split risk estimates into to calculate calibration curves.</p> required <code>random_state</code> <code>RandomState</code> <p>The source of randomness for the resampling and fitting process.</p> required <p>Returns:</p> Type Description <code>dict[str, DataFrame | Pipeline]</code> <p>Dictionary with keys \"probs\", \"calibrations\", \"roc_curves\", \"roc_aucs\".</p> Source code in <code>src\\pyhbr\\analysis\\fit.py</code> <pre><code>def fit_model(\n    pipe: Pipeline,\n    X_train: DataFrame,\n    y_train: DataFrame,\n    X_test: DataFrame,\n    y_test: DataFrame,\n    num_bootstraps: int,\n    num_bins: int,\n    random_state: RandomState,\n) -&gt; dict[str, DataFrame | Pipeline]:\n    \"\"\"Fit the model and bootstrap models, and calculate model performance\n\n    Args:\n        pipe: The model pipeline to fit\n        X_train: Training features\n        y_train: Training outcomes (containing \"bleeding\"/\"ischaemia\" columns)\n        X_test: Test features\n        y_test: Test outcomes\n        num_bootstraps: The number of resamples of the training set to use to\n            fit bootstrap models.\n        num_bins: The number of equal-size bins to split risk estimates into\n            to calculate calibration curves.\n        random_state: The source of randomness for the resampling and fitting\n            process.\n\n    Returns:\n        Dictionary with keys \"probs\", \"calibrations\", \"roc_curves\", \"roc_aucs\".\n    \"\"\"\n\n    # Calculate the results of the model\n    probs = {}\n    calibrations = {}\n    roc_curves = {}\n    roc_aucs = {}\n    fitted_models = {}\n    feature_importances = {}\n    for outcome in [\"bleeding\", \"ischaemia\"]:\n\n        log.info(f\"Fitting {outcome} model\")\n\n        # Fit the bleeding and ischaemia models on the training set\n        # and bootstrap resamples of the training set (to assess stability)\n        fitted_models[outcome] = stability.fit_model(\n            pipe, X_train, y_train.loc[:, outcome], num_bootstraps, random_state\n        )\n\n        log.info(f\"Running permutation feature importance on {outcome} model M0\")\n        M0 = fitted_models[outcome].M0\n        r = permutation_importance(\n            M0,\n            X_test,\n            y_test.loc[:, outcome],\n            n_repeats=20,\n            random_state=random_state,\n            scoring=\"roc_auc\",\n        )\n        feature_importances[outcome] = {\n            \"names\": X_train.columns,\n            \"result\": r,\n        }\n\n        # Get the predicted probabilities associated with all the resamples of\n        # the bleeding and ischaemia models\n        probs[outcome] = stability.predict_probabilities(fitted_models[outcome], X_test)\n\n        # Get the calibration of the models\n        calibrations[outcome] = calibration.get_variable_width_calibration(\n            probs[outcome], y_test.loc[:, outcome], num_bins\n        )\n\n        # Calculate the ROC curves for the models\n        roc_curves[outcome] = roc.get_roc_curves(probs[outcome], y_test.loc[:, outcome])\n        roc_aucs[outcome] = roc.get_auc(probs[outcome], y_test.loc[:, outcome])\n\n    return {\n        \"probs\": probs,\n        \"calibrations\": calibrations,\n        \"roc_aucs\": roc_aucs,\n        \"roc_curves\": roc_curves,\n        \"fitted_models\": fitted_models,\n        \"feature_importances\": feature_importances,\n    }\n</code></pre>"},{"location":"reference/#pyhbr.analysis.model","title":"<code>model</code>","text":""},{"location":"reference/#pyhbr.analysis.model.DenseTransformer","title":"<code>DenseTransformer</code>","text":"<p>             Bases: <code>TransformerMixin</code></p> <p>Useful when the model requires a dense matrix but the preprocessing steps produce a sparse output</p> Source code in <code>src\\pyhbr\\analysis\\model.py</code> <pre><code>class DenseTransformer(TransformerMixin):\n    \"\"\"Useful when the model requires a dense matrix\n    but the preprocessing steps produce a sparse output\n    \"\"\"\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def transform(self, X, y=None, **fit_params):\n        if hasattr(X, \"todense\"):\n            return np.asarray(X.todense())\n        else:\n            return X\n</code></pre>"},{"location":"reference/#pyhbr.analysis.model.Preprocessor","title":"<code>Preprocessor</code>  <code>dataclass</code>","text":"<p>Preprocessing steps for a subset of columns</p> <p>This holds the set of preprocessing steps that should be applied to a subset of the (named) columns in the input training dataframe.</p> <p>Multiple instances of this classes (for different subsets of columns) are grouped together to create a ColumnTransformer, which preprocesses all columns in the training dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the preprocessor (which will become the name of the transformer in ColumnTransformer</p> required <code>pipe</code> <code>Pipeline</code> <p>The sklearn Pipeline that should be applied to the set of columns</p> required <code>columns</code> <code>list[str]</code> <p>The set of columns that should have pipe applied to them.</p> required Source code in <code>src\\pyhbr\\analysis\\model.py</code> <pre><code>@dataclass\nclass Preprocessor:\n    \"\"\"Preprocessing steps for a subset of columns\n\n    This holds the set of preprocessing steps that should\n    be applied to a subset of the (named) columns in the\n    input training dataframe.\n\n    Multiple instances of this classes (for different subsets\n    of columns) are grouped together to create a ColumnTransformer,\n    which preprocesses all columns in the training dataframe.\n\n    Args:\n        name: The name of the preprocessor (which will become\n            the name of the transformer in ColumnTransformer\n        pipe: The sklearn Pipeline that should be applied to\n            the set of columns\n        columns: The set of columns that should have pipe\n            applied to them.\n    \"\"\"\n\n    name: str\n    pipe: Pipeline\n    columns: list[str]\n</code></pre>"},{"location":"reference/#pyhbr.analysis.model.TradeOffModel","title":"<code>TradeOffModel</code>","text":"<p>             Bases: <code>ClassifierMixin</code>, <code>BaseEstimator</code></p> Source code in <code>src\\pyhbr\\analysis\\model.py</code> <pre><code>class TradeOffModel(ClassifierMixin, BaseEstimator):\n\n    def fit(self, X, y):\n        \"\"\"Use the name of the Y variable to choose between\n        bleeding and ischaemia\n        \"\"\"\n\n        # Get the outcome name to decide between bleeding and\n        # ischaemia model function\n        self.outcome = y.name\n\n        self.classes_ = unique_labels(y)\n\n        self.X_ = X\n        self.y_ = y\n\n        # Return the classifier\n\n        return self\n\n    def decision_function(self, X: DataFrame) -&gt; DataFrame:\n        return self.predict_proba(X)[:, 1]\n\n    def predict_proba(self, X: DataFrame) -&gt; DataFrame:\n        if self.outcome == \"bleeding\":\n            risk = trade_off_model_bleeding_risk(X)\n        else:\n            risk = trade_off_model_ischaemia_risk(X)\n        return np.column_stack((1-risk, risk))\n</code></pre>"},{"location":"reference/#pyhbr.analysis.model.TradeOffModel.fit","title":"<code>fit(X, y)</code>","text":"<p>Use the name of the Y variable to choose between bleeding and ischaemia</p> Source code in <code>src\\pyhbr\\analysis\\model.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Use the name of the Y variable to choose between\n    bleeding and ischaemia\n    \"\"\"\n\n    # Get the outcome name to decide between bleeding and\n    # ischaemia model function\n    self.outcome = y.name\n\n    self.classes_ = unique_labels(y)\n\n    self.X_ = X\n    self.y_ = y\n\n    # Return the classifier\n\n    return self\n</code></pre>"},{"location":"reference/#pyhbr.analysis.model.get_feature_importances","title":"<code>get_feature_importances(fit)</code>","text":"<p>Get a table of the features used in the model along with feature importances</p> <p>Parameters:</p> Name Type Description Default <code>fit</code> <code>Pipeline</code> <p>The fitted Pipeline</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Contains a column for feature names, a column for type, and a feature importance column.</p> Source code in <code>src\\pyhbr\\analysis\\model.py</code> <pre><code>def get_feature_importances(fit: Pipeline) -&gt; DataFrame:\n    \"\"\"Get a table of the features used in the model along with feature importances\n\n    Args:\n        fit: The fitted Pipeline\n\n    Returns:\n        Contains a column for feature names, a column for type, and a feature importance column.\n    \"\"\"\n\n    df = get_feature_names(fit)\n\n    model = fit[\"model\"]\n\n    # Check if the Pipe is a raw model, or a CV search (either\n    # grid or randomised)\n    if hasattr(model, \"best_estimator_\"):\n        # CV model\n        importances = model.best_estimator_.feature_importances_\n    else:\n        importances = model.feature_importances_\n\n    df[\"feature_importances\"] = importances\n    return df.sort_values(\"feature_importances\", ascending=False)\n</code></pre>"},{"location":"reference/#pyhbr.analysis.model.get_feature_names","title":"<code>get_feature_names(fit)</code>","text":"<p>Get a table of feature names</p> <p>The feature names are the names of the columns in the output from the preprocessing step in the fitted pipeline</p> <p>Parameters:</p> Name Type Description Default <code>fit</code> <code>Pipeline</code> <p>A fitted sklearn pipeline, containing a \"preprocess\" step.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>description</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>dict[str, str]: description</p> Source code in <code>src\\pyhbr\\analysis\\model.py</code> <pre><code>def get_feature_names(fit: Pipeline) -&gt; DataFrame:\n    \"\"\"Get a table of feature names\n\n    The feature names are the names of the columns in the output\n    from the preprocessing step in the fitted pipeline\n\n    Args:\n        fit: A fitted sklearn pipeline, containing a \"preprocess\"\n            step.\n\n    Raises:\n        RuntimeError: _description_\n\n    Returns:\n        dict[str, str]: _description_\n    \"\"\"\n\n    # Get the fitted ColumnTransformer from the fitted pipeline\n    preprocess = fit[\"preprocess\"]\n\n    # Map from preprocess name to the relevant step that changes\n    # column names. This must be kept consistent with the\n    # make_*_preprocessor functions\n    relevant_step = {\n        \"category\": \"one_hot_encoder\",\n        \"float\": \"low_variance\",\n        \"flag\": \"one_hot_encode\",\n    }\n\n    # Get the map showing which column transformers (preprocessors)\n    # are responsible which which slices of columns in the output\n    # training dataframe\n    column_slices = preprocess.output_indices_\n\n    # Make an empty list of the right length to store all the columns\n    column_names = get_num_feature_columns(fit) * [None]\n\n    # Make an empty list for the preprocessor groups\n    prep_names = get_num_feature_columns(fit) * [None]\n\n    for name, pipe, columns in preprocess.transformers_:\n\n        # Ignore the remainder step\n        if name == \"remainder\":\n            continue\n\n        step_name = relevant_step[name]\n\n        # Get the step which transforms column names\n        step = pipe[step_name]\n\n        # A special case is required for the low_variance columns\n        # which need original list of columns passing in\n        if name == \"float\":\n            columns = step.get_feature_names_out(columns)\n        else:\n            columns = step.get_feature_names_out()\n\n        # Get the properties of the slice where this set of\n        # columns sits\n        start = column_slices[name].start\n        stop = column_slices[name].stop\n        length = stop - start\n\n        # Check the length of the slice matches the output\n        # columns length\n        if len(columns) != length:\n            raise RuntimeError(\n                \"Length of output columns slice did not match the length of the column names list\"\n            )\n\n        # Get the current slice corresponding to this preprocess\n        s = column_slices[name]\n\n        # Insert the list of colum names by slice\n        column_names[s] = columns\n\n        # Store the preprocessor name for the columns\n        prep_names[s] = (s.stop - s.start) * [name]\n\n    return DataFrame({\"column\": column_names, \"preprocessor\": prep_names})\n</code></pre>"},{"location":"reference/#pyhbr.analysis.model.get_features","title":"<code>get_features(fit, X)</code>","text":"<p>Get the features after preprocessing the input X dataset</p> <p>The features are generated by the \"preprocess\" step in the fitted pipe. This step is a column transformer that one-hot-encodes discrete data, and imputes, centers, and scales numerical data.</p> <p>Note that the result may be a dense or sparse Pandas dataframe, depending on whether the preprocessing steps produce a sparse numpy array or not.</p> <p>Parameters:</p> Name Type Description Default <code>fit</code> <code>Pipeline</code> <p>Fitted pipeline with \"preprocess\" step.</p> required <code>X</code> <code>DataFrame</code> <p>An input dataset (either training or test) containing the input columns to be preprocessed.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The resulting feature columns generated by the preprocessing step.</p> Source code in <code>src\\pyhbr\\analysis\\model.py</code> <pre><code>def get_features(fit: Pipeline, X: DataFrame) -&gt; DataFrame:\n    \"\"\"Get the features after preprocessing the input X dataset\n\n    The features are generated by the \"preprocess\" step in the fitted\n    pipe. This step is a column transformer that one-hot-encodes\n    discrete data, and imputes, centers, and scales numerical data.\n\n    Note that the result may be a dense or sparse Pandas dataframe,\n    depending on whether the preprocessing steps produce a sparse\n    numpy array or not.\n\n    Args:\n        fit: Fitted pipeline with \"preprocess\" step.\n        X: An input dataset (either training or test) containing\n            the input columns to be preprocessed.\n\n    Returns:\n        The resulting feature columns generated by the preprocessing\n            step.\n    \"\"\"\n\n    # Get the preprocessing step and new feature column names\n    preprocess = fit[\"preprocess\"]\n    prep_columns = get_feature_names(fit)\n    X_numpy = preprocess.transform(X)\n\n    # Convert the numpy array or sparse array to a dataframe\n    if scipy.sparse.issparse(X_numpy):\n        return DataFrame.sparse.from_spmatrix(\n            X_numpy,\n            columns=prep_columns[\"column\"],\n            index=X.index,\n        )\n    else:\n        return DataFrame(\n            X_numpy,\n            columns=prep_columns[\"column\"],\n            index=X.index,\n        )\n</code></pre>"},{"location":"reference/#pyhbr.analysis.model.get_num_feature_columns","title":"<code>get_num_feature_columns(fit)</code>","text":"<p>Get the total number of feature columns Args:     fit: The fitted pipeline, containing a \"preprocess\"         step.</p> <p>Returns:</p> Type Description <code>int</code> <p>The total number of columns in the features, after preprocessing.</p> Source code in <code>src\\pyhbr\\analysis\\model.py</code> <pre><code>def get_num_feature_columns(fit: Pipeline) -&gt; int:\n    \"\"\"Get the total number of feature columns\n    Args:\n        fit: The fitted pipeline, containing a \"preprocess\"\n            step.\n\n    Returns:\n        The total number of columns in the features, after\n            preprocessing.\n    \"\"\"\n\n    # Get the map from column transformers to the slices\n    # that they occupy in the training data\n    preprocess = fit[\"preprocess\"]\n    column_slices = preprocess.output_indices_\n\n    total = 0\n    for s in column_slices.values():\n        total += s.stop - s.start\n\n    return total\n</code></pre>"},{"location":"reference/#pyhbr.analysis.model.make_abc","title":"<code>make_abc(random_state, X_train, config)</code>","text":"<p>Make the AdaBoost classifier pipeline</p> Source code in <code>src\\pyhbr\\analysis\\model.py</code> <pre><code>def make_abc(random_state: RandomState, X_train: DataFrame, config: dict[str, Any]) -&gt; Pipeline:\n    \"\"\"Make the AdaBoost classifier pipeline\n    \"\"\"\n\n    preprocessors = [\n        make_category_preprocessor(X_train),\n        make_flag_preprocessor(X_train),\n        make_float_preprocessor(X_train),\n    ]\n    preprocess = make_columns_transformer(preprocessors)\n    mod = AdaBoostClassifier(**config, random_state=random_state)\n    return Pipeline([(\"preprocess\", preprocess), (\"model\", mod)])\n</code></pre>"},{"location":"reference/#pyhbr.analysis.model.make_category_preprocessor","title":"<code>make_category_preprocessor(X_train, drop=None)</code>","text":"<p>Create a preprocessor for string/category columns</p> <p>Columns in the training features that are discrete, represented using strings (\"object\") or \"category\" dtypes, should be one-hot encoded. This generates one new columns for each possible value in the original columns.</p> <p>The ColumnTransformer transformer created from this preprocessor will be called \"category\".</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>DataFrame</code> <p>The training features</p> required <code>drop</code> <p>The drop argument to be passed to OneHotEncoder. Default None means no features will be dropped. Using \"first\" drops the first item in the category, which is useful to avoid collinearity in linear models.</p> <code>None</code> <p>Returns:</p> Type Description <code>Preprocessor | None</code> <p>A preprocessor for processing the discrete columns. None is returned if the training features do not contain any string/category columns</p> Source code in <code>src\\pyhbr\\analysis\\model.py</code> <pre><code>def make_category_preprocessor(X_train: DataFrame, drop=None) -&gt; Preprocessor | None:\n    \"\"\"Create a preprocessor for string/category columns\n\n    Columns in the training features that are discrete, represented\n    using strings (\"object\") or \"category\" dtypes, should be one-hot\n    encoded. This generates one new columns for each possible value\n    in the original columns.\n\n    The ColumnTransformer transformer created from this preprocessor\n    will be called \"category\".\n\n    Args:\n        X_train: The training features\n        drop: The drop argument to be passed to OneHotEncoder. Default\n            None means no features will be dropped. Using \"first\" drops\n            the first item in the category, which is useful to avoid\n            collinearity in linear models.\n\n    Returns:\n        A preprocessor for processing the discrete columns. None is\n            returned if the training features do not contain any\n            string/category columns\n    \"\"\"\n\n    # Category columns should be one-hot encoded (in all these one-hot encoders,\n    # consider the effect of linear dependence among the columns due to the extra\n    # variable compared to dummy encoding -- the relevant parameter is called\n    # 'drop').\n    columns = X_train.columns[\n        (X_train.dtypes == \"object\") | (X_train.dtypes == \"category\")\n    ]\n\n    # Return None if there are no discrete columns.\n    if len(columns) == 0:\n        return None\n\n    pipe = Pipeline(\n        [\n            (\n                \"one_hot_encoder\",\n                OneHotEncoder(\n                    handle_unknown=\"infrequent_if_exist\", min_frequency=0.002, drop=drop\n                ),\n            ),\n        ]\n    )\n\n    return Preprocessor(\"category\", pipe, columns)\n</code></pre>"},{"location":"reference/#pyhbr.analysis.model.make_flag_preprocessor","title":"<code>make_flag_preprocessor(X_train, drop=None)</code>","text":"<p>Create a preprocessor for flag columns</p> <p>Columns in the training features that are flags (bool + NaN) are represented using Int8 (because bool does not allow NaN). These columns are also one-hot encoded.</p> <p>The ColumnTransformer transformer created from this preprocessor will be called \"flag\".</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>DataFrame</code> <p>The training features.</p> required <code>drop</code> <p>The drop argument to be passed to OneHotEncoder. Default None means no features will be dropped. Using \"first\" drops the first item in the category, which is useful to avoid collinearity in linear models.</p> <code>None</code> <p>Returns:</p> Type Description <code>Preprocessor | None</code> <p>A preprocessor for processing the flag columns. None is returned if the training features do not contain any Int8 columns.</p> Source code in <code>src\\pyhbr\\analysis\\model.py</code> <pre><code>def make_flag_preprocessor(X_train: DataFrame, drop=None) -&gt; Preprocessor | None:\n    \"\"\"Create a preprocessor for flag columns\n\n    Columns in the training features that are flags (bool + NaN) are\n    represented using Int8 (because bool does not allow NaN). These\n    columns are also one-hot encoded.\n\n    The ColumnTransformer transformer created from this preprocessor\n    will be called \"flag\".\n\n    Args:\n        X_train: The training features.\n        drop: The drop argument to be passed to OneHotEncoder. Default\n            None means no features will be dropped. Using \"first\" drops\n            the first item in the category, which is useful to avoid\n            collinearity in linear models.\n\n    Returns:\n        A preprocessor for processing the flag columns. None is\n            returned if the training features do not contain any\n            Int8 columns.\n    \"\"\"\n\n    # Flag columns (encoded using Int8, which supports NaN), should be one-hot\n    # encoded (considered separately from category in case we want to do something\n    # different with these).\n    columns = X_train.columns[(X_train.dtypes == \"Int8\")]\n\n    # Return None if there are no discrete columns.\n    if len(columns) == 0:\n        return None\n\n    pipe = Pipeline(\n        [\n            (\n                \"one_hot_encode\",\n                OneHotEncoder(handle_unknown=\"infrequent_if_exist\", drop=drop),\n            ),\n        ]\n    )\n\n    return Preprocessor(\"flag\", pipe, columns)\n</code></pre>"},{"location":"reference/#pyhbr.analysis.model.make_float_preprocessor","title":"<code>make_float_preprocessor(X_train)</code>","text":"<p>Create a preprocessor for float (numerical) columns</p> <p>Columns in the training features that are numerical are encoded using float (to distinguish them from Int8, which is used for flags).</p> <p>Missing values in these columns are imputed using the mean, then low variance columns are removed. The remaining columns are centered and scaled.</p> <p>The ColumnTransformer transformer created from this preprocessor will be called \"float\".</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>DataFrame</code> <p>The training features</p> required <p>Returns:</p> Type Description <code>Preprocessor | None</code> <p>A preprocessor for processing the float columns. None is returned if the training features do not contain any Int8 columns.</p> Source code in <code>src\\pyhbr\\analysis\\model.py</code> <pre><code>def make_float_preprocessor(X_train: DataFrame) -&gt; Preprocessor | None:\n    \"\"\"Create a preprocessor for float (numerical) columns\n\n    Columns in the training features that are numerical are encoded\n    using float (to distinguish them from Int8, which is used for\n    flags).\n\n    Missing values in these columns are imputed using the mean, then\n    low variance columns are removed. The remaining columns are\n    centered and scaled.\n\n    The ColumnTransformer transformer created from this preprocessor\n    will be called \"float\".\n\n    Args:\n        X_train: The training features\n\n    Returns:\n        A preprocessor for processing the float columns. None is\n            returned if the training features do not contain any\n            Int8 columns.\n    \"\"\"\n\n    # Numerical columns -- impute missing values, remove low variance\n    # columns, and then centre and scale the rest.\n    columns = X_train.columns[(X_train.dtypes == \"float\")]\n\n    # Return None if there are no discrete columns.\n    if len(columns) == 0:\n        return None\n\n    pipe = Pipeline(\n        [\n            (\"impute\", SimpleImputer(missing_values=np.nan, strategy=\"mean\")),\n            (\"low_variance\", VarianceThreshold()),\n            (\"scaler\", StandardScaler()),\n        ]\n    )\n\n    return Preprocessor(\"float\", pipe, columns)\n</code></pre>"},{"location":"reference/#pyhbr.analysis.model.make_nearest_neighbours_cv","title":"<code>make_nearest_neighbours_cv(random_state, X_train, config)</code>","text":"<p>Nearest neighbours classifier trained using cross validation</p> <p>Parameters:</p> Name Type Description Default <code>random_state</code> <code>RandomState</code> <p>Source of randomness for creating the model</p> required <code>X_train</code> <code>DataFrame</code> <p>The training dataset containing all features for modelling</p> required <code>config</code> <code>dict[str, Any]</code> <p>The dictionary of keyword arguments to configure the CV search.</p> required <p>Returns:</p> Type Description <code>Pipeline</code> <p>The preprocessing and fitting pipeline.</p> Source code in <code>src\\pyhbr\\analysis\\model.py</code> <pre><code>def make_nearest_neighbours_cv(random_state: RandomState, X_train: DataFrame, config: dict[str, Any]) -&gt; Pipeline:\n    \"\"\"Nearest neighbours classifier trained using cross validation\n\n    Args:\n        random_state: Source of randomness for creating the model\n        X_train: The training dataset containing all features for modelling\n        config: The dictionary of keyword arguments to configure the CV search.\n\n    Returns:\n        The preprocessing and fitting pipeline.\n    \"\"\"\n    preprocessors = [\n        make_category_preprocessor(X_train),\n        make_flag_preprocessor(X_train),\n        make_float_preprocessor(X_train),\n    ]\n    preprocess = make_columns_transformer(preprocessors)\n\n    mod = RandomizedSearchCV(\n        KNeighborsClassifier(),\n        param_distributions=config,\n        random_state=random_state,\n        scoring=\"roc_auc\",\n        cv=5,\n        verbose=3\n    )\n    return Pipeline([(\"preprocess\", preprocess), (\"model\", mod)])\n</code></pre>"},{"location":"reference/#pyhbr.analysis.model.make_random_forest","title":"<code>make_random_forest(random_state, X_train)</code>","text":"<p>Make the random forest model</p> <p>Parameters:</p> Name Type Description Default <code>random_state</code> <code>RandomState</code> <p>Source of randomness for creating the model</p> required <code>X_train</code> <code>DataFrame</code> <p>The training dataset containing all features for modelling</p> required <p>Returns:</p> Type Description <code>Pipeline</code> <p>The preprocessing and fitting pipeline.</p> Source code in <code>src\\pyhbr\\analysis\\model.py</code> <pre><code>def make_random_forest(random_state: RandomState, X_train: DataFrame) -&gt; Pipeline:\n    \"\"\"Make the random forest model\n\n    Args:\n        random_state: Source of randomness for creating the model\n        X_train: The training dataset containing all features for modelling\n\n    Returns:\n        The preprocessing and fitting pipeline.\n    \"\"\"\n\n    preprocessors = [\n        make_category_preprocessor(X_train),\n        make_flag_preprocessor(X_train),\n        make_float_preprocessor(X_train),\n    ]\n    preprocess = make_columns_transformer(preprocessors)\n    mod = RandomForestClassifier(\n        n_estimators=100, max_depth=10, random_state=random_state\n    )\n    return Pipeline([(\"preprocess\", preprocess), (\"model\", mod)])\n</code></pre>"},{"location":"reference/#pyhbr.analysis.model.make_random_forest_cv","title":"<code>make_random_forest_cv(random_state, X_train, config)</code>","text":"<p>Random forest model trained using cross validation</p> <p>Parameters:</p> Name Type Description Default <code>random_state</code> <code>RandomState</code> <p>Source of randomness for creating the model</p> required <code>X_train</code> <code>DataFrame</code> <p>The training dataset containing all features for modelling</p> required <code>config</code> <code>dict[str, Any]</code> <p>The dictionary of keyword arguments to configure the CV search.</p> required <p>Returns:</p> Type Description <code>Pipeline</code> <p>The preprocessing and fitting pipeline.</p> Source code in <code>src\\pyhbr\\analysis\\model.py</code> <pre><code>def make_random_forest_cv(random_state: RandomState, X_train: DataFrame, config: dict[str, Any]) -&gt; Pipeline:\n    \"\"\"Random forest model trained using cross validation\n\n    Args:\n        random_state: Source of randomness for creating the model\n        X_train: The training dataset containing all features for modelling\n        config: The dictionary of keyword arguments to configure the CV search.\n\n    Returns:\n        The preprocessing and fitting pipeline.\n    \"\"\"\n    preprocessors = [\n        make_category_preprocessor(X_train),\n        make_flag_preprocessor(X_train),\n        make_float_preprocessor(X_train),\n    ]\n    preprocess = make_columns_transformer(preprocessors)\n\n    mod = RandomizedSearchCV(\n        RandomForestClassifier(random_state=random_state),\n        param_distributions=config,\n        random_state=random_state,\n        scoring=\"roc_auc\",\n        cv=5,\n        verbose=3\n    )\n    return Pipeline([(\"preprocess\", preprocess), (\"model\", mod)])\n</code></pre>"},{"location":"reference/#pyhbr.analysis.model.make_trade_off","title":"<code>make_trade_off(random_state, X_train, config)</code>","text":"<p>Make the ARC HBR bleeding/ischaemia trade-off model</p> <p>Parameters:</p> Name Type Description Default <code>random_state</code> <code>RandomState</code> <p>Source of randomness for creating the model</p> required <code>X_train</code> <code>DataFrame</code> <p>The training dataset containing all features for modelling</p> required <p>Returns:</p> Type Description <code>Pipeline</code> <p>The preprocessing and fitting pipeline.</p> Source code in <code>src\\pyhbr\\analysis\\model.py</code> <pre><code>def make_trade_off(random_state: RandomState, X_train: DataFrame, config: dict[str, Any]) -&gt; Pipeline:\n    \"\"\"Make the ARC HBR bleeding/ischaemia trade-off model\n\n    Args:\n        random_state: Source of randomness for creating the model\n        X_train: The training dataset containing all features for modelling\n\n    Returns:\n        The preprocessing and fitting pipeline.\n    \"\"\"\n\n    #preprocess = make_columns_transformer(preprocessors)\n    mod = TradeOffModel()\n    return Pipeline([(\"model\", mod)])\n</code></pre>"},{"location":"reference/#pyhbr.analysis.model.make_xgboost_cv","title":"<code>make_xgboost_cv(random_state, X_train, config)</code>","text":"<p>XGBoost model trained using cross validation</p> <p>Parameters:</p> Name Type Description Default <code>random_state</code> <code>RandomState</code> <p>Source of randomness for creating the model</p> required <code>X_train</code> <code>DataFrame</code> <p>The training dataset containing all features for modelling</p> required <code>config</code> <code>dict[str, Any]</code> <p>The dictionary of keyword arguments to configure the CV search.</p> required <p>Returns:</p> Type Description <code>Pipeline</code> <p>The preprocessing and fitting pipeline.</p> Source code in <code>src\\pyhbr\\analysis\\model.py</code> <pre><code>def make_xgboost_cv(random_state: RandomState, X_train: DataFrame, config: dict[str, Any]) -&gt; Pipeline:\n    \"\"\"XGBoost model trained using cross validation\n\n    Args:\n        random_state: Source of randomness for creating the model\n        X_train: The training dataset containing all features for modelling\n        config: The dictionary of keyword arguments to configure the CV search.\n\n    Returns:\n        The preprocessing and fitting pipeline.\n    \"\"\"\n    preprocessors = [\n        make_category_preprocessor(X_train),\n        make_flag_preprocessor(X_train),\n        make_float_preprocessor(X_train),\n    ]\n    preprocess = make_columns_transformer(preprocessors)\n\n    mod = RandomizedSearchCV(\n        XGBClassifier(random_state=random_state),\n        param_distributions=config,\n        random_state=random_state,\n        scoring=\"roc_auc\",\n        cv=5,\n        verbose=3\n    )\n    return Pipeline([(\"preprocess\", preprocess), (\"model\", mod)])\n</code></pre>"},{"location":"reference/#pyhbr.analysis.model.trade_off_model_bleeding_risk","title":"<code>trade_off_model_bleeding_risk(features)</code>","text":"<p>ARC-HBR bleeding part of the trade-off model</p> <p>This function implements the bleeding model contained here https://pubmed.ncbi.nlm.nih.gov/33404627/. The numbers used below come from correspondence with the authors.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>DataFrame</code> <p>must contain age, smoking, copd, hb, egfr_x, oac.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>The bleeding risks as a Series.</p> Source code in <code>src\\pyhbr\\analysis\\model.py</code> <pre><code>def trade_off_model_bleeding_risk(features: DataFrame) -&gt; Series:\n    \"\"\"ARC-HBR bleeding part of the trade-off model\n\n    This function implements the bleeding model contained here\n    https://pubmed.ncbi.nlm.nih.gov/33404627/. The numbers used\n    below come from correspondence with the authors.\n\n\n    Args:\n        features: must contain age, smoking, copd, hb, egfr_x, oac.\n\n    Returns:\n        The bleeding risks as a Series.\n    \"\"\"\n\n    # Age component, right=False for &gt;= 65, setting upper limit == 1000 to catch all\n    age = pd.cut(features[\"age\"], [0, 65, 1000], labels=[1, 1.5], right=False).astype(\"float\")\n\n    smoking = np.where(features[\"smoking\"] == \"yes\", 1.47, 1)\n    copd = np.where(features[\"copd\"].fillna(0) == 1, 1.39, 1)\n\n    # Fill NA with a high Hb value (50) to treat missing as low risk\n    hb = pd.cut(\n        10 * features[\"hb\"].fillna(50),\n        [0, 110, 130, 1000],\n        labels=[3.99, 1.69, 1],\n        right=False,\n    ).astype(\"float\")\n\n    # Fill NA with a high eGFR value (500) to treat missing as low risk\n    egfr = pd.cut(\n        features[\"egfr_x\"].fillna(500),\n        [0, 30, 60, 1000],\n        labels=[1.43, 0.99, 1],\n        right=False,\n    ).astype(\"float\")\n\n    # Complex PCI and liver/cancer composite\n    complex_score = np.where(features[\"complex_pci_index\"], 1.32, 1.0)\n    liver_cancer_surgery = np.where((features[\"cancer_before\"] + features[\"ckd_before\"]) &gt; 0, 1.63, 1.0)\n\n    oac = np.where(features[\"oac\"] == 1, 2.0, 1.0)\n\n    # Calculate bleeding risk\n    xb = age*smoking*copd*liver_cancer_surgery*hb*egfr*complex_score*oac\n    risk = 1 - 0.986**xb\n\n    return risk\n</code></pre>"},{"location":"reference/#pyhbr.analysis.model.trade_off_model_ischaemia_risk","title":"<code>trade_off_model_ischaemia_risk(features)</code>","text":"<p>ARC-HBR ischaemia part of the trade-off model</p> <p>This function implements the bleeding model contained here https://pubmed.ncbi.nlm.nih.gov/33404627/. The numbers used below come from correspondence with the authors.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>DataFrame</code> <p>must contain diabetes_before, smoking, </p> required <p>Returns:</p> Type Description <code>Series</code> <p>The ischaemia risks as a Series.</p> Source code in <code>src\\pyhbr\\analysis\\model.py</code> <pre><code>def trade_off_model_ischaemia_risk(features: DataFrame) -&gt; Series:\n    \"\"\"ARC-HBR ischaemia part of the trade-off model\n\n    This function implements the bleeding model contained here\n    https://pubmed.ncbi.nlm.nih.gov/33404627/. The numbers used\n    below come from correspondence with the authors.\n\n    Args:\n        features: must contain diabetes_before, smoking, \n\n    Returns:\n        The ischaemia risks as a Series.\n    \"\"\"\n\n    diabetes = np.where(features[\"diabetes_before\"] &gt; 0, 1.56, 1)\n    smoking = np.where(features[\"smoking\"] == \"yes\", 1.47, 1)\n    prior_mi = np.where(features[\"mi_schnier_before\"] &gt; 0, 1.89, 1)\n\n    # Interpreting \"presentation\" as stemi vs. nstemi\n    presentation = np.where(features[\"stemi_index\"], 1.82, 1)\n\n    # Fill NA with a high Hb value (50) to treat missing as low risk\n    hb = pd.cut(\n        10 * features[\"hb\"].fillna(50),\n        [0, 110, 130, 1000],\n        labels=[1.5, 1.27, 1],\n        right=False,\n    ).astype(\"float\")\n\n    # Fill NA with a high eGFR value (500) to treat missing as low risk\n    egfr = pd.cut(\n        features[\"egfr_x\"].fillna(500),\n        [0, 30, 60, 1000],\n        labels=[1.69, 1.3, 1],\n        right=False,\n    ).astype(\"float\")\n\n    # TODO bare metal stent (missing from data)\n    complex_score = np.where(features[\"complex_pci_index\"], 1.5, 1.0)\n    bms = 1.0\n\n    # Calculate bleeding risk\n    xb = diabetes*smoking*prior_mi*presentation*hb*egfr*complex_score*bms\n    risk = 1 - 0.986**xb\n\n    return risk\n</code></pre>"},{"location":"reference/#pyhbr.analysis.patient_viewer","title":"<code>patient_viewer</code>","text":""},{"location":"reference/#pyhbr.analysis.patient_viewer.get_patient_history","title":"<code>get_patient_history(patient_id, hic_data)</code>","text":"<p>Get a list of all this patient's episode data</p> <p>Parameters:</p> Name Type Description Default <code>patient_id</code> <code>str</code> <p>Which patient to fetch</p> required <code>hic_data</code> <code>HicData</code> <p>Contains <code>episodes</code> and <code>codes</code> tables</p> required <p>Returns:</p> Type Description <p>A table indexed by spell_id, episode_id, type (of clinical code) and clinical code position.</p> Source code in <code>src\\pyhbr\\analysis\\patient_viewer.py</code> <pre><code>def get_patient_history(patient_id: str, hic_data: HicData):\n    \"\"\"Get a list of all this patient's episode data\n\n    Args:\n        patient_id: Which patient to fetch\n        hic_data: Contains `episodes` and `codes` tables\n\n    Returns:\n        A table indexed by spell_id, episode_id, type (of clinical code)\n            and clinical code position.\n    \"\"\"\n    df = hic_data.codes.merge(\n        hic_data.episodes[[\"patient_id\", \"spell_id\", \"episode_start\"]],\n        how=\"left\",\n        on=\"episode_id\",\n    )\n    this_patient = (\n        df[df[\"patient_id\"] == patient_id]\n        .sort_values([\"episode_start\", \"type\",\"position\"])\n        .drop(columns=\"group\")\n        .set_index([\"spell_id\", \"episode_id\", \"type\", \"position\"])\n    ).drop_duplicates()\n    return this_patient\n</code></pre>"},{"location":"reference/#pyhbr.analysis.roc","title":"<code>roc</code>","text":"<p>ROC Curves</p> <p>The file calculates the ROC curves of the bootstrapped models (for assessing ROC curve stability; see stability.py).</p>"},{"location":"reference/#pyhbr.analysis.roc.AucData","title":"<code>AucData</code>  <code>dataclass</code>","text":"Source code in <code>src\\pyhbr\\analysis\\roc.py</code> <pre><code>@dataclass\nclass AucData:\n    model_under_test_auc: float\n    resample_auc: list[float]\n\n    def mean_resample_auc(self) -&gt; float:\n        \"\"\"Get the mean of the resampled AUCs\n        \"\"\"\n        return np.mean(self.resample_auc)\n\n    def std_dev_resample_auc(self) -&gt; float:\n        \"\"\"Get the standard deviation of the resampled AUCs\n        \"\"\"\n        return np.mean(self.resample_auc)\n\n    def roc_auc_spread(self) -&gt; DataFrame:\n        return Series(self.resample_auc + [self.model_under_test_auc]).quantile([0.25, 0.5, 0.75])\n</code></pre>"},{"location":"reference/#pyhbr.analysis.roc.AucData.mean_resample_auc","title":"<code>mean_resample_auc()</code>","text":"<p>Get the mean of the resampled AUCs</p> Source code in <code>src\\pyhbr\\analysis\\roc.py</code> <pre><code>def mean_resample_auc(self) -&gt; float:\n    \"\"\"Get the mean of the resampled AUCs\n    \"\"\"\n    return np.mean(self.resample_auc)\n</code></pre>"},{"location":"reference/#pyhbr.analysis.roc.AucData.std_dev_resample_auc","title":"<code>std_dev_resample_auc()</code>","text":"<p>Get the standard deviation of the resampled AUCs</p> Source code in <code>src\\pyhbr\\analysis\\roc.py</code> <pre><code>def std_dev_resample_auc(self) -&gt; float:\n    \"\"\"Get the standard deviation of the resampled AUCs\n    \"\"\"\n    return np.mean(self.resample_auc)\n</code></pre>"},{"location":"reference/#pyhbr.analysis.roc.get_auc","title":"<code>get_auc(probs, y_test)</code>","text":"<p>Get the area under the ROC curves for the fitted models</p> <p>Compute area under the ROC curve (AUC) for the model-under-test (the first column of probs), and the other bootstrapped models (other columns of probs).</p> Source code in <code>src\\pyhbr\\analysis\\roc.py</code> <pre><code>def get_auc(probs: DataFrame, y_test: Series) -&gt; AucData:\n    \"\"\"Get the area under the ROC curves for the fitted models\n\n    Compute area under the ROC curve (AUC) for the model-under-test\n    (the first column of probs), and the other bootstrapped models\n    (other columns of probs).\n\n    \"\"\"\n    model_under_test_auc = roc_auc_score(y_test, probs.iloc[:,0]) # Model-under test\n    resample_auc = []\n    for column in probs:\n        resample_auc.append(roc_auc_score(y_test, probs[column]))\n    return AucData(model_under_test_auc, resample_auc)\n</code></pre>"},{"location":"reference/#pyhbr.analysis.roc.get_roc_curves","title":"<code>get_roc_curves(probs, y_test)</code>","text":"<p>Get the ROC curves for the fitted models</p> <p>Get the ROC curves for all models (whose probability predictions for the positive class are columns of probs) based on the outcomes in y_test. Rows of y_test correspond to rows of probs. The result is a list of pairs, one for each model (column of probs). Each pair contains the vector of x- and y-coordinates of the ROC curve.</p> <p>Parameters:</p> Name Type Description Default <code>probs</code> <code>DataFrame</code> <p>The probabilities predicted by all the fitted models. The first column is the model-under-test (the training set), and the other columns are resamples of the training set.</p> required <code>y_test</code> <code>Series</code> <p>The outcome data corresponding to each row of probs.</p> required <p>Returns:</p> Type Description <code>list[DataFrame]</code> <p>A list of DataFrames, each of which contains one ROC curve, corresponding to the columns in probs. The columns of the DataFrames are <code>fpr</code> (false positive rate) and <code>tpr</code> (true positive rate)</p> Source code in <code>src\\pyhbr\\analysis\\roc.py</code> <pre><code>def get_roc_curves(probs: DataFrame, y_test: Series) -&gt; list[DataFrame]:\n    \"\"\"Get the ROC curves for the fitted models\n\n    Get the ROC curves for all models (whose probability\n    predictions for the positive class are columns of probs) based\n    on the outcomes in y_test. Rows of y_test correspond to rows of\n    probs. The result is a list of pairs, one for each model (column\n    of probs). Each pair contains the vector of x- and y-coordinates\n    of the ROC curve.\n\n    Args:\n        probs: The probabilities predicted by all the fitted models.\n            The first column is the model-under-test (the training set),\n            and the other columns are resamples of the training set.\n        y_test: The outcome data corresponding to each row of probs.\n\n    Returns:\n        A list of DataFrames, each of which contains one ROC curve,\n            corresponding to the columns in probs. The columns of the\n            DataFrames are `fpr` (false positive rate) and `tpr` (true\n            positive rate)\n    \"\"\"\n    curves = []\n    for n in range(probs.shape[1]):\n        fpr, tpr, _ = roc_curve(y_test, probs.iloc[:, n])\n        curves.append(DataFrame({\"fpr\": fpr, \"tpr\": tpr}))\n    return curves\n</code></pre>"},{"location":"reference/#pyhbr.analysis.roc.plot_roc_curves","title":"<code>plot_roc_curves(ax, curves, auc, title='ROC-stability Curves')</code>","text":"<p>Plot ROC curves of the model-under-test and resampled models</p> <p>Plot the set of bootstrapped ROC curves (an instability plot), using the data in curves (a list of curves to plot). Assume that the first curve is the model-under-test (which is coloured differently).</p> <p>The auc argument is an array where the first element is the AUC of the model under test, and the second element is the mean AUC of the bootstrapped models, and the third element is the standard deviation of the AUC of the bootstrapped models (these latter two measure stability). This argument is the output from get_bootstrapped_auc.</p> Source code in <code>src\\pyhbr\\analysis\\roc.py</code> <pre><code>def plot_roc_curves(ax, curves, auc, title = \"ROC-stability Curves\"):\n    \"\"\"Plot ROC curves of the model-under-test and resampled models\n\n    Plot the set of bootstrapped ROC curves (an instability plot),\n    using the data in curves (a list of curves to plot). Assume that the\n    first curve is the model-under-test (which is coloured differently).\n\n    The auc argument is an array where the first element is the AUC of the\n    model under test, and the second element is the mean AUC of the\n    bootstrapped models, and the third element is the standard deviation\n    of the AUC of the bootstrapped models (these latter two measure\n    stability). This argument is the output from get_bootstrapped_auc.\n    \"\"\"\n    mut_curve = curves[0]  # model-under-test\n    ax.plot(mut_curve[\"fpr\"], mut_curve[\"tpr\"], color=\"r\")\n    for curve in curves[1:]:\n        ax.plot(curve[\"fpr\"], curve[\"tpr\"], color=\"b\", linewidth=0.3, alpha=0.4)\n    ax.axline([0, 0], [1, 1], color=\"k\", linestyle=\"--\")\n    ax.legend(\n        [\n            f\"Model (AUC = {auc.model_under_test_auc:.2f})\",\n            f\"Bootstrapped models\",\n        ]\n    )\n    ax.set_title(title)\n    ax.set_xlabel(\"False positive rate\")\n    ax.set_ylabel(\"True positive rate\")\n</code></pre>"},{"location":"reference/#pyhbr.analysis.stability","title":"<code>stability</code>","text":"<p>Assessing model stability</p> <p>Model stability of an internally-validated model refers to how well models developed on a similar internal population agree with each other. The methodology for assessing model stability follows Riley and Collins, 2022 (https://arxiv.org/abs/2211.01061)</p> <p>Assessing model stability is an end-to-end test of the entire model development process. Riley and Collins do not refer to a test/train split, but their method will be interpreted as applying to the training set (with instability measures assessed by applying models to the test set). As a result, the first step in the process is to split the internal dataset into a training set P0 and a test set T.</p> <p>Assuming that a training set P0 is used to develop a model M0 using a model development  process D (involving steps such cross-validation and hyperparameter tuning in the training set, and validation of accuracy of model prediction in the test set), the following steps are required to assess the stability of M0:</p> <ol> <li>Bootstrap resample P0 with replacement M &gt;= 200 times, creating    M new datasets Pm that are all the same size as P0</li> <li>Apply D to each Pm, to obtain M new models Mn which are all    comparable with M0.</li> <li>Collect together the predictions from all Mn and compare them    to the predictions from M0 for each sample in the test set T.</li> <li>From the data in 3, plot instability plots such as a scatter    plot of M0 predictions on the x-axis and all the Mn predictions    on the y-axis, for each sample of T. In addition, plot graphs    of how all the model validation metrics vary as a function of    the bootstrapped models Mn.</li> </ol> <p>Implementation</p> <p>A function is required that takes the original training set P0 and generates N bootstrapped resamples Pn that are the same size as P.</p> <p>A function is required that wraps the entire model into one call, taking as input the bootstrapped resample Pn and providing as output the bootstrapped model Mn. This function can then be called M times to generate the bootstrapped models. This function is not defined in this file (see the fit.py file)</p> <p>An aggregating function will then take all the models Mn, the model-under-test M0, and the test set T, and make predictions using all the models for each sample in the test set. It should return all these predictions (probabilities) in a 2D array, where each row corresponds to a test-set sample, column 0 is the probability from M0, and columns 1 through M are the probabilities from each Mn.</p> <p>This 2D array may be used as the basis of instability plots. Paired with information about the true outcomes y_test, this can also be used to plot ROC-curve variability (i.e. plotting the ROC curve for all model M0 and Mn on one graph). Any other accuracy metric of interest can be calculated from this information (i.e. for step 4 above).</p>"},{"location":"reference/#pyhbr.analysis.stability.FittedModel","title":"<code>FittedModel</code>  <code>dataclass</code>","text":"<p>Stores a model fitted to a training set and resamples of the training set.</p> Source code in <code>src\\pyhbr\\analysis\\stability.py</code> <pre><code>@dataclass\nclass FittedModel:\n    \"\"\"Stores a model fitted to a training set and resamples of the training set.\"\"\"\n\n    M0: Pipeline\n    Mm: list[Pipeline]\n\n    def flatten(self) -&gt; list[Pipeline]:\n        \"\"\"Get a flat list of all the models\n\n        Returns:\n            The list of fitted models, with M0 at the front\n        \"\"\"\n        return [self.M0] + self.Mm\n</code></pre>"},{"location":"reference/#pyhbr.analysis.stability.FittedModel.flatten","title":"<code>flatten()</code>","text":"<p>Get a flat list of all the models</p> <p>Returns:</p> Type Description <code>list[Pipeline]</code> <p>The list of fitted models, with M0 at the front</p> Source code in <code>src\\pyhbr\\analysis\\stability.py</code> <pre><code>def flatten(self) -&gt; list[Pipeline]:\n    \"\"\"Get a flat list of all the models\n\n    Returns:\n        The list of fitted models, with M0 at the front\n    \"\"\"\n    return [self.M0] + self.Mm\n</code></pre>"},{"location":"reference/#pyhbr.analysis.stability.Resamples","title":"<code>Resamples</code>  <code>dataclass</code>","text":"<p>Store a training set along with M resamples of it</p> <p>Parameters:</p> Name Type Description Default <code>X0</code> <code>DataFrame</code> <p>The matrix of predictors</p> required <code>Y0</code> <code>DataFrame</code> <p>The matrix of outcomes (one column per outcome)</p> required <code>Xm</code> <code>list[DataFrame]</code> <p>A list of resamples of the predictors</p> required <code>Ym</code> <code>list[DataFrame]</code> <p>A list of resamples of the outcomes</p> required Source code in <code>src\\pyhbr\\analysis\\stability.py</code> <pre><code>@dataclass\nclass Resamples:\n    \"\"\"Store a training set along with M resamples of it\n\n    Args:\n        X0: The matrix of predictors\n        Y0: The matrix of outcomes (one column per outcome)\n        Xm: A list of resamples of the predictors\n        Ym: A list of resamples of the outcomes\n    \"\"\"\n\n    X0: DataFrame\n    Y0: DataFrame\n    Xm: list[DataFrame]\n    Ym: list[DataFrame]\n</code></pre>"},{"location":"reference/#pyhbr.analysis.stability.absolute_instability","title":"<code>absolute_instability(probs)</code>","text":"<p>Get a list of the absolute percentage-point differences</p> <p>Compare the primary model to the bootstrap models by flattening all the bootstrap model estimates and calculating the absolute difference between the primary model estimate and the bootstraps. Results are expressed in percentage points.</p> <p>Parameters:</p> Name Type Description Default <code>probs</code> <code>DataFrame</code> <p>First column is primary model risk estimates, other columns are bootstrap model estimates.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>A Series of absolute percentage-point discrepancies between the primary model predictions and the bootstrap  estimates.</p> Source code in <code>src\\pyhbr\\analysis\\stability.py</code> <pre><code>def absolute_instability(probs: DataFrame) -&gt; Series:\n    \"\"\"Get a list of the absolute percentage-point differences\n\n    Compare the primary model to the bootstrap models by flattening\n    all the bootstrap model estimates and calculating the absolute\n    difference between the primary model estimate and the bootstraps.\n    Results are expressed in percentage points.\n\n    Args:\n        probs: First column is primary model risk estimates, other\n            columns are bootstrap model estimates.\n\n    Returns:\n        A Series of absolute percentage-point discrepancies between\n            the primary model predictions and the bootstrap \n            estimates.\n    \"\"\"\n\n    # Make a table containing the initial risk (from the\n    # model under test) and a column for all other risks\n    prob_compare = 100 * probs.melt(\n        id_vars=\"prob_M0\", value_name=\"bootstrap_risk\", var_name=\"initial_risk\"\n    )\n\n    # Round the resulting risk error to 2 decimal places (i.e. to 0.01%). This truncates very small values\n    # to zero, which means the resulting log y scale is not artificially extended downwards.\n    return (\n        (prob_compare[\"bootstrap_risk\"] - prob_compare[\"prob_M0\"])\n        .abs()\n        .round(decimals=2)\n    )\n</code></pre>"},{"location":"reference/#pyhbr.analysis.stability.average_absolute_instability","title":"<code>average_absolute_instability(probs)</code>","text":"<p>Get the average absolute error between primary model and bootstrap estimates.</p> <p>This function computes the average of the absolute difference between the risks estimated by the primary model, and the risks estimated by the bootstrap models. For example, if the primary model estimates 1%, and a bootstrap model provides 2% and 3%, the result is 1.5% error.</p> <p>Expressed differently, the function calculates the average percentage-point difference between the model under test and bootstrap models.</p> <p>Using the absolute error instead of the relative error is more useful in practice, because it does not inflate errors between very small risks. Since most risks are on the order &lt; 20%, with a risk threshold like 5%, it is easier to interpret an absolute risk difference.</p> <p>Further granularity in the variability of risk estimates as a function of risk is obtained by looking at the instability box plot.</p> <p>Parameters:</p> Name Type Description Default <code>probs</code> <code>DataFrame</code> <p>The table of risks estimated by the models. The first column is the model under test, and the other columns are bootstrap models.</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>A mean and confidence interval for the estimate. The units are percent.</p> Source code in <code>src\\pyhbr\\analysis\\stability.py</code> <pre><code>def average_absolute_instability(probs: DataFrame) -&gt; dict[str, float]:\n    \"\"\"Get the average absolute error between primary model and bootstrap estimates.\n\n    This function computes the average of the absolute difference between the risks\n    estimated by the primary model, and the risks estimated by the bootstrap models.\n    For example, if the primary model estimates 1%, and a bootstrap model provides\n    2% and 3%, the result is 1.5% error.\n\n    Expressed differently, the function calculates the average percentage-point\n    difference between the model under test and bootstrap models.\n\n    Using the absolute error instead of the relative error is more useful in\n    practice, because it does not inflate errors between very small risks. Since\n    most risks are on the order &lt; 20%, with a risk threshold like 5%, it is\n    easier to interpret an absolute risk difference.\n\n    Further granularity in the variability of risk estimates as a function of\n    risk is obtained by looking at the instability box plot.\n\n    Args:\n        probs: The table of risks estimated by the models. The first column is\n            the model under test, and the other columns are bootstrap models.\n\n    Returns:\n        A mean and confidence interval for the estimate. The units are percent.\n    \"\"\"\n\n    absolute_errors = absolute_instability(probs)\n    return absolute_errors.quantile([0.025, 0.5, 0.975])\n</code></pre>"},{"location":"reference/#pyhbr.analysis.stability.fit_model","title":"<code>fit_model(model, X0, y0, M, random_state)</code>","text":"<p>Fit a model to a training set and resamples of the training set.</p> <p>Use the unfitted model pipeline to:</p> <ul> <li>Fit a model to the training set (X0, y0)</li> <li>Fit a model to M resamples (Xm, ym) of the training set</li> </ul> <p>The model is an unfitted scikit-learn Pipeline. Note that if RandomState is used when specifying the model, then the models used to fit the resamples here will be statstical clones (i.e. they might not necessarily produce the same result on the same data). clone() is called on model before fitting, so each fit gets a new clean object.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Pipeline</code> <p>An unfitted scikit-learn pipeline, which is used as the basis for all the fits. Each fit calls clone() on this object before fitting, to get a new model with clean parameters. The cloned fitted models are then stored in the returned fitted model.</p> required <code>X0</code> <code>DataFrame</code> <p>The training set features</p> required <code>y0</code> <code>Series</code> <p>The training set outcome</p> required <code>M</code> <code>int</code> <p>How many resamples to take from the training set (ideally &gt;= 200)</p> required <code>random_state</code> <code>RandomState</code> <p>The source of randomness for model fitting</p> required <p>Returns:</p> Type Description <code>FittedModel</code> <p>An object containing the model fitted on (X0,y0) and all (Xm,ym)</p> Source code in <code>src\\pyhbr\\analysis\\stability.py</code> <pre><code>def fit_model(\n    model: Pipeline, X0: DataFrame, y0: Series, M: int, random_state: RandomState\n) -&gt; FittedModel:\n    \"\"\"Fit a model to a training set and resamples of the training set.\n\n    Use the unfitted model pipeline to:\n\n    * Fit a model to the training set (X0, y0)\n    * Fit a model to M resamples (Xm, ym) of the training set\n\n    The model is an unfitted scikit-learn Pipeline. Note that if RandomState is used\n    when specifying the model, then the models used to fit the resamples here will\n    be _statstical clones_ (i.e. they might not necessarily produce the same result\n    on the same data). clone() is called on model before fitting, so each fit gets a\n    new clean object.\n\n    Args:\n        model: An unfitted scikit-learn pipeline, which is used as the basis for\n            all the fits. Each fit calls clone() on this object before fitting, to\n            get a new model with clean parameters. The cloned fitted models are then\n            stored in the returned fitted model.\n        X0: The training set features\n        y0: The training set outcome\n        M (int): How many resamples to take from the training set (ideally &gt;= 200)\n        random_state: The source of randomness for model fitting\n\n    Returns:\n        An object containing the model fitted on (X0,y0) and all (Xm,ym)\n    \"\"\"\n\n    # Develop a single model from the training set (X0_train, y0_train),\n    # using any method (e.g. including cross validation and hyperparameter\n    # tuning) using training set data. This is referred to as D in\n    # stability.py.\n    log.info(\"Fitting model-under-test\")\n    pipe = clone(model)\n    M0 = pipe.fit(X0, y0)\n\n    # Resample the training set to obtain the new datasets (Xm, ym)\n    log.info(f\"Creating {M} bootstrap resamples of training set\")\n    resamples = make_bootstrapped_resamples(X0, y0, M, random_state)\n\n    # Develop all the bootstrap models to compare with the model-under-test M0\n    log.info(\"Fitting bootstrapped models\")\n    Mm = []\n    for m in range(M):\n        pipe = clone(model)\n        ym = resamples.Ym[m]\n        Xm = resamples.Xm[m]\n        Mm.append(pipe.fit(Xm, ym))\n\n    return FittedModel(M0, Mm)\n</code></pre>"},{"location":"reference/#pyhbr.analysis.stability.get_average_instability","title":"<code>get_average_instability(probs)</code>","text":"<p>Instability is the extent to which the bootstrapped models give a different prediction from the model under test. The average instability is an average of the SMAPE between the prediction of the model-under-test and the predictions of each of the other bootstrap models (i.e. pairing the model-under-test) with a single bootstrapped model gives one SMAPE value, and these are averaged over all the bootstrap models).</p> <p>SMAPE is preferable to mean relative error, because the latter diverges when the prediction from the model-under-test is very small. It may however be better still to use the log of the accuracy ratio; see https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error, since the probabilities are all positive (or maybe there is a better thing for comparing probabilities specifically)</p> <p>Testing: not yet tested</p> Source code in <code>src\\pyhbr\\analysis\\stability.py</code> <pre><code>def get_average_instability(probs):\n    \"\"\"\n    Instability is the extent to which the bootstrapped models\n    give a different prediction from the model under test. The\n    average instability is an average of the SMAPE between\n    the prediction of the model-under-test and the predictions of\n    each of the other bootstrap models (i.e. pairing the model-under-test)\n    with a single bootstrapped model gives one SMAPE value, and\n    these are averaged over all the bootstrap models).\n\n    SMAPE is preferable to mean relative error, because the latter\n    diverges when the prediction from the model-under-test is very small.\n    It may however be better still to use the log of the accuracy ratio;\n    see https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error,\n    since the probabilities are all positive (or maybe there is a better\n    thing for comparing probabilities specifically)\n\n    Testing: not yet tested\n    \"\"\"\n    num_rows = probs.shape[0]\n    num_cols = probs.shape[1]\n\n    smape_over_bootstraps = []\n\n    # Loop over each boostrap model\n    for j in range(1, num_cols):\n\n        # Calculate SMAPE between bootstrap model j and\n        # the model-under-test\n        smape_over_bootstraps.append(smape(probs[:, 0], probs[:, j]))\n\n    return np.mean(smape_over_bootstraps)\n</code></pre>"},{"location":"reference/#pyhbr.analysis.stability.get_reclass_probabilities","title":"<code>get_reclass_probabilities(probs, y_test, threshold)</code>","text":"<p>Get the probability of risk reclassification for each patient</p> <p>Parameters:</p> Name Type Description Default <code>probs</code> <code>DataFrame</code> <p>The matrix of probabilities from the model-under-test (first column) and the bootstrapped models (subsequent models).</p> required <code>y_test</code> <code>Series</code> <p>The true outcome corresponding to each row of the probs matrix. This is used to colour the points based on whether the outcome occurred on not.</p> required <code>threshold</code> <code>float</code> <p>The risk level at which a patient is considered high risk</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A table containing columns \"original_risk\", \"unstable_prob\", and \"outcome\".</p> Source code in <code>src\\pyhbr\\analysis\\stability.py</code> <pre><code>def get_reclass_probabilities(probs: DataFrame, y_test: Series, threshold: float) -&gt; DataFrame:\n    \"\"\"Get the probability of risk reclassification for each patient\n\n    Args:\n        probs: The matrix of probabilities from the model-under-test\n            (first column) and the bootstrapped models (subsequent\n            models).\n        y_test: The true outcome corresponding to each row of the\n            probs matrix. This is used to colour the points based on\n            whether the outcome occurred on not.\n        threshold: The risk level at which a patient is considered high risk\n\n    Returns:\n        A table containing columns \"original_risk\", \"unstable_prob\", and\n            \"outcome\".\n    \"\"\"\n\n    # For the predictions of each model, categorise patients as\n    # high risk or not based on the threshold.\n    high_risk = probs &gt; threshold\n\n    # Find the subsets of patients who were flagged as high risk\n    # by the original model.\n    originally_low_risk = high_risk[~high_risk.iloc[:, 0]]\n    originally_high_risk = high_risk[high_risk.iloc[:, 0]]\n\n    # Count how many of the patients remained high risk or\n    # low risk in the bootstrapped models.\n    stayed_high_risk = originally_high_risk.iloc[:, 1:].sum(axis=1)\n    stayed_low_risk = (~originally_low_risk.iloc[:, 1:]).sum(axis=1)\n\n    # Calculate the number of patients who changed category (category\n    # unstable)\n    num_resamples = probs.shape[1]\n    stable_count = pd.concat([stayed_low_risk, stayed_high_risk])\n    unstable_prob = (\n        ((num_resamples - stable_count) / num_resamples)\n        .rename(\"unstable_prob\")\n        .to_frame()\n    )\n\n    # Merge the original risk with the unstable count\n    original_risk = probs.iloc[:, 0].rename(\"original_risk\")\n    return (\n        original_risk.to_frame()\n        .merge(unstable_prob, on=\"spell_id\", how=\"left\")\n        .merge(y_test.rename(\"outcome\"), on=\"spell_id\", how=\"left\")\n    )\n</code></pre>"},{"location":"reference/#pyhbr.analysis.stability.make_bootstrapped_resamples","title":"<code>make_bootstrapped_resamples(X0, y0, M, random_state)</code>","text":"<p>Make M resamples of the training data</p> <p>Makes M bootstrapped resamples of a training set (X0,y0). M should be at least 200 (as per recommendation).</p> <p>Parameters:</p> Name Type Description Default <code>X0</code> <code>DataFrame</code> <p>The features in the training set to be resampled</p> required <code>y0</code> <code>DataFrame</code> <p>The outcome in the training set to be resampled. Can have multiple columns (corresponding to different outcomes).</p> required <code>M</code> <code>int</code> <p>How many resamples to take</p> required <code>random_state</code> <code>RandomState</code> <p>Source of randomness for resampling</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of rows in X0 and y0 do not match</p> <p>Returns:</p> Type Description <code>Resamples</code> <p>An object containing the original training set and the resamples.</p> Source code in <code>src\\pyhbr\\analysis\\stability.py</code> <pre><code>def make_bootstrapped_resamples(\n    X0: DataFrame, y0: DataFrame, M: int, random_state: RandomState\n) -&gt; Resamples:\n    \"\"\"Make M resamples of the training data\n\n    Makes M bootstrapped resamples of a training set (X0,y0).\n    M should be at least 200 (as per recommendation).\n\n    Args:\n        X0: The features in the training set to be resampled\n        y0: The outcome in the training set to be resampled. Can have multiple\n            columns (corresponding to different outcomes).\n        M: How many resamples to take\n        random_state: Source of randomness for resampling\n\n    Raises:\n        ValueError: If the number of rows in X0 and y0 do not match\n\n    Returns:\n        An object containing the original training set and the resamples.\n    \"\"\"\n\n    if len(X0) != len(y0):\n        raise ValueError(\"Number of rows in X0_train and y0_train must match\")\n    if M &lt; 200:\n        warnings.warn(\"M should be at least 200; see Riley and Collins, 2022\")\n\n    Xm = []\n    ym = []\n    for _ in range(M):\n        X, y = resample(X0, y0, random_state=random_state)\n        Xm.append(X)\n        ym.append(y)\n\n    return Resamples(X0, y0, Xm, ym)\n</code></pre>"},{"location":"reference/#pyhbr.analysis.stability.plot_instability","title":"<code>plot_instability(ax, probs, y_test, title='Probability stability')</code>","text":"<p>Plot the instability of risk predictions</p> <p>This function plots a scatter graph of one point per value in the test set (row of probs), where the x-axis is the value of the model under test (the first column of probs), and the y-axis is every other probability predicted from the bootstrapped models Mn (the other columns of probs). The predictions from the model-under-test corresponds to the straight line at 45 degrees through the origin</p> <p>For a stable model M0, the scattered points should be close to the M0 line, indicating that the bootstrapped models Mn broadly agree with the predictions made by M0.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>The axes on which to plot the risks</p> required <code>probs</code> <code>DataFrame</code> <p>The matrix of probabilities from the model-under-test (first column) and the bootstrapped models (subsequent models).</p> required <code>y_test</code> <code>Series</code> <p>The true outcome corresponding to each row of the probs matrix. This is used to colour the points based on whether the outcome occurred on not.</p> required <code>title</code> <p>The title to place on the axes.</p> <code>'Probability stability'</code> Source code in <code>src\\pyhbr\\analysis\\stability.py</code> <pre><code>def plot_instability(\n    ax: Axes, probs: DataFrame, y_test: Series, title=\"Probability stability\"\n):\n    \"\"\"Plot the instability of risk predictions\n\n    This function plots a scatter graph of one point\n    per value in the test set (row of probs), where the\n    x-axis is the value of the model under test (the\n    first column of probs), and the y-axis is every other\n    probability predicted from the bootstrapped models Mn\n    (the other columns of probs). The predictions from\n    the model-under-test corresponds to the straight line\n    at 45 degrees through the origin\n\n    For a stable model M0, the scattered points should be\n    close to the M0 line, indicating that the bootstrapped\n    models Mn broadly agree with the predictions made by M0.\n\n    Args:\n        ax: The axes on which to plot the risks\n        probs: The matrix of probabilities from the model-under-test\n            (first column) and the bootstrapped models (subsequent\n            models).\n        y_test: The true outcome corresponding to each row of the\n            probs matrix. This is used to colour the points based on\n            whether the outcome occurred on not.\n        title: The title to place on the axes.\n    \"\"\"\n\n    num_rows = probs.shape[0]\n    num_cols = probs.shape[1]\n    x = []\n    y = []\n    c = []\n    # Keep track of an example point to plot\n    example_risk = 1\n    example_second_risk = 1\n    for i in range(num_rows):\n        for j in range(1, num_cols):\n\n            # Get the pair of risks\n            risk = 100 * probs.iloc[i, 0]\n            second_risk = 100 * probs.iloc[i, j]\n\n            # Keep track of the worst discrepancy\n            # in the upper left quadrant\n            if (\n                (1.0 &lt; risk &lt; 10.0)\n                and (second_risk &gt; risk)\n                and (second_risk / risk) &gt; (example_second_risk / example_risk)\n            ):\n                example_risk = risk\n                example_second_risk = second_risk\n\n            x.append(risk)  # Model-under-test\n            y.append(second_risk)  # Other bootstrapped models\n            c.append(y_test.iloc[i]),  # What was the actual outcome\n\n    colour_map = {0: \"b\", 1: \"r\"}\n\n    text = f\"Model risk {example_risk:.1f}%, bootstrap risk {example_second_risk:.1f}%\"\n    ax.annotate(\n        text,\n        xy=(example_risk, example_second_risk),\n        xycoords=\"data\",\n        xytext=(example_risk, 95),\n        fontsize=9,\n        verticalalignment=\"top\",\n        horizontalalignment=\"center\",\n        textcoords=\"data\",\n        arrowprops={\"arrowstyle\": \"-&gt;\"},\n        backgroundcolor=\"w\",\n    )\n\n    for outcome_to_plot, colour in colour_map.items():\n        x_to_plot = [x for x, outcome in zip(x, c) if outcome == outcome_to_plot]\n        y_to_plot = [y for y, outcome in zip(y, c) if outcome == outcome_to_plot]\n        ax.scatter(x_to_plot, y_to_plot, c=colour, s=1, marker=\".\")\n\n    ax.axline([0, 0], [1, 1])\n\n    ax.set_xlim(0.01, 100)\n    ax.set_ylim(0.01, 100)\n    ax.set_xscale(\"log\")\n    ax.set_yscale(\"log\")\n    ax.xaxis.set_major_formatter(mtick.PercentFormatter())\n    ax.yaxis.set_major_formatter(mtick.PercentFormatter())\n\n    ax.legend(\n        [\n            \"Did not occur\",\n            \"Event occurred\",\n        ],\n        markerscale=10,\n        loc=\"lower right\",\n    )\n    ax.set_title(title)\n    ax.set_xlabel(\"Risk estimate from model\")\n    ax.set_ylabel(\"Risk estimates from equivalent models\")\n</code></pre>"},{"location":"reference/#pyhbr.analysis.stability.plot_reclass_instability","title":"<code>plot_reclass_instability(ax, probs, y_test, threshold, title='Stability of Risk Class')</code>","text":"<p>Plot the probability of reclassification by predicted risk</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>The axes on which to draw the plot</p> required <code>probs</code> <code>DataFrame</code> <p>The matrix of probabilities from the model-under-test (first column) and the bootstrapped models (subsequent models).</p> required <code>y_test</code> <code>Series</code> <p>The true outcome corresponding to each row of the probs matrix. This is used to colour the points based on whether the outcome occurred on not.</p> required <code>threshold</code> <code>float</code> <p>The risk level at which a patient is considered high risk</p> required <code>title</code> <code>str</code> <p>The plot title.</p> <code>'Stability of Risk Class'</code> Source code in <code>src\\pyhbr\\analysis\\stability.py</code> <pre><code>def plot_reclass_instability(\n    ax: Axes,\n    probs: DataFrame,\n    y_test: Series,\n    threshold: float,\n    title: str = \"Stability of Risk Class\",\n):\n    \"\"\"Plot the probability of reclassification by predicted risk\n\n    Args:\n        ax: The axes on which to draw the plot\n        probs: The matrix of probabilities from the model-under-test\n            (first column) and the bootstrapped models (subsequent\n            models).\n        y_test: The true outcome corresponding to each row of the\n            probs matrix. This is used to colour the points based on\n            whether the outcome occurred on not.\n        threshold: The risk level at which a patient is considered high risk\n        title: The plot title.\n    \"\"\"\n\n    df = get_reclass_probabilities(probs, y_test, threshold)\n\n    x = 100*df[\"original_risk\"]\n    y = 100*df[\"unstable_prob\"]\n    c = df[\"outcome\"]\n    colour_map = {False: \"b\", True: \"r\"}\n\n    # TODO: Plot is all black now, this can go\n    for outcome_to_plot, colour in colour_map.items():\n        x_to_plot = [x for x, outcome in zip(x, c) if outcome == outcome_to_plot]\n        y_to_plot = [y for y, outcome in zip(y, c) if outcome == outcome_to_plot]\n        ax.scatter(x_to_plot, y_to_plot, c=\"k\", s=1, marker=\".\")\n\n    # ax.legend(\n    #     [\n    #         \"Did not occur\",\n    #         \"Event occurred\",\n    #     ],\n    #     markerscale=15\n    # )\n\n    # Plot the risk category threshold and label it\n    ax.axline(\n        [100 * threshold, 0],\n        [100 * threshold, 1],\n        c=\"r\",\n    )\n\n    # Plot the 50% line for more-likely-than-not reclassification\n    ax.axline([0, 50], [100, 50], c=\"r\")\n\n    # Get the lower axis limits\n    min_risk = 100 * df[\"original_risk\"].min()\n    min_unstable_prob = 100 * df[\"unstable_prob\"].min()\n\n    # Plot boxes to show high and low risk groups\n    # low_risk_rect = Rectangle((min_risk, min_unstable_prob), 100*threshold, 100, facecolor=\"g\", alpha=0.3)\n    # ax[1].add_patch(low_risk_rect)\n    # high_risk_rect = Rectangle((100*threshold, min_unstable_prob), 100*(1 - threshold), 100, facecolor=\"r\", alpha=0.3)\n    # ax[1].add_patch(high_risk_rect)\n\n    text_str = f\"High-risk threshold ({100*threshold:.2f}%)\"\n    ax.text(\n        100 * threshold,\n        min_unstable_prob * 1.1,\n        text_str,\n        fontsize=9,\n        rotation=\"vertical\",\n        color=\"r\",\n        horizontalalignment=\"center\",\n        verticalalignment=\"bottom\",\n        backgroundcolor=\"w\",\n    )\n\n    text_str = f\"Prob. of reclassification = 50%\"\n    ax.text(\n        0.011,\n        50,\n        text_str,\n        fontsize=9,\n        # rotation=\"vertical\",\n        color=\"r\",\n        # horizontalalignment=\"center\",\n        verticalalignment=\"center\",\n        backgroundcolor=\"w\",\n    )\n\n    # Calculate the number of patients who fall in each stability group.\n    # Unstable means\n    num_high_risk = (df[\"original_risk\"] &gt;= threshold).sum()\n    num_low_risk = (df[\"original_risk\"] &lt; threshold).sum()\n\n    num_stable = (df[\"unstable_prob\"] &lt; 0.5).sum()\n    num_unstable = (df[\"unstable_prob\"] &gt;= 0.5).sum()\n\n    high_risk_and_unstable = (\n        (df[\"original_risk\"] &gt;= threshold) &amp; (df[\"unstable_prob\"] &gt;= 0.5)\n    ).sum()\n\n    high_risk_and_stable = (\n        (df[\"original_risk\"] &gt;= threshold) &amp; (df[\"unstable_prob\"] &lt; 0.5)\n    ).sum()\n\n    low_risk_and_unstable = (\n        (df[\"original_risk\"] &lt; threshold) &amp; (df[\"unstable_prob\"] &gt;= 0.5)\n    ).sum()\n\n    low_risk_and_stable = (\n        (df[\"original_risk\"] &lt; threshold) &amp; (df[\"unstable_prob\"] &lt; 0.5)\n    ).sum()\n\n    # Count the number of events in each risk group\n    num_events_in_low_risk_group = df[df[\"original_risk\"] &lt; threshold][\"outcome\"].sum()\n    num_events_in_high_risk_group = df[df[\"original_risk\"] &gt;= threshold][\n        \"outcome\"\n    ].sum()\n\n    ax.set_xlim(0.009, 110)\n    ax.set_ylim(0.9 * min_unstable_prob, 110)\n\n    text_str = f\"Unstable\\nN = {low_risk_and_unstable}\"\n    ax.text(\n        0.011,\n        90,\n        text_str,\n        fontsize=9,\n        verticalalignment=\"top\",\n        backgroundcolor=\"w\",\n    )\n\n    text_str = f\"Unstable\\nN = {high_risk_and_unstable}\"\n    ax.text(\n        90,\n        90,\n        text_str,\n        fontsize=9,\n        verticalalignment=\"top\",\n        horizontalalignment=\"right\",\n        backgroundcolor=\"w\",\n    )\n\n    text_str = f\"Stable\\nN = {low_risk_and_stable}\"\n    ax.text(\n        0.011,\n        40,\n        text_str,\n        fontsize=9,\n        verticalalignment=\"top\",\n        horizontalalignment=\"left\",\n        backgroundcolor=\"w\",\n    )\n\n    text_str = f\"Stable\\nN = {high_risk_and_stable}\"\n    ax.text(\n        90,\n        40,\n        text_str,\n        fontsize=9,\n        verticalalignment=\"top\",\n        horizontalalignment=\"right\",\n        backgroundcolor=\"w\",\n    )\n\n    # Set axis properties\n    ax.set_xscale(\"log\")\n    ax.set_yscale(\"log\")\n    ax.xaxis.set_major_formatter(mtick.PercentFormatter())\n    ax.yaxis.set_major_formatter(mtick.PercentFormatter())\n\n    ax.set_title(title)\n    ax.set_xlabel(\"Risk estimate from model\")\n    ax.set_ylabel(\"Probability of risk reclassification by equivalent model\")\n</code></pre>"},{"location":"reference/#pyhbr.analysis.stability.plot_stability_analysis","title":"<code>plot_stability_analysis(ax, outcome_name, probs, y_test, high_risk_thresholds)</code>","text":"<p>Plot the two stability plots</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>The axes on which to plot the graphs (must have two</p> required <code>outcome_name</code> <code>str</code> <p>One of \"bleeding\" or \"ischaemia\"</p> required <code>probs</code> <code>DataFrame</code> <p>The model predictions. The first column is the model-under-test, and the other columns are the bootstrap model predictions.</p> required <code>y_test</code> <code>DataFrame</code> <p>The outcomes table, with columns for \"bleeding\" and \"ischaemia\".</p> required <code>high_risk_thresholds</code> <code>dict[str, float]</code> <p>Map containing the vertical risk prediction threshold for \"bleeding\" and \"ischaemia\".</p> required Source code in <code>src\\pyhbr\\analysis\\stability.py</code> <pre><code>def plot_stability_analysis(\n    ax: Axes,\n    outcome_name: str,\n    probs: DataFrame,\n    y_test: DataFrame,\n    high_risk_thresholds: dict[str, float],\n):\n    \"\"\"Plot the two stability plots\n\n    Args:\n        ax: The axes on which to plot the graphs (must have two\n        outcome_name: One of \"bleeding\" or \"ischaemia\"\n        probs: The model predictions. The first column is\n            the model-under-test, and the other columns are\n            the bootstrap model predictions.\n        y_test: The outcomes table, with columns for \"bleeding\"\n            and \"ischaemia\".\n        high_risk_thresholds: Map containing the vertical risk\n            prediction threshold for \"bleeding\" and \"ischaemia\".\n    \"\"\"\n    plot_instability_boxes(\n        ax[0],\n        probs[outcome_name],\n    )\n    plot_reclass_instability(\n        ax[1],\n        probs[outcome_name],\n        y_test.loc[:, outcome_name],\n        high_risk_thresholds[outcome_name],\n    )\n</code></pre>"},{"location":"reference/#pyhbr.analysis.stability.predict_probabilities","title":"<code>predict_probabilities(fitted_model, X_test)</code>","text":"<p>Predict outcome probabilities using the fitted models on the test set</p> <p>Aggregating function which finds the predicted probability from the model-under-test M0 and all the bootstrapped models Mn on each sample of the training set features X_test. The result is a 2D numpy array, where each row corresponds to a test-set sample, the first column is the predicted probabilities from M0, and the following N columns are the predictions from all the other Mn.</p> <p>Note: the numbers in the matrix are the probabilities of 1 in the test set y_test.</p> <p>Parameters:</p> Name Type Description Default <code>fitted_model</code> <code>FittedModel</code> <p>The model fitted on the training set and resamples</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>An table of probabilities of the positive outcome in the class, where each column comes from a different model. Column zero corresponds to the training set, and the other columns are from the resamples. The index for the DataFrame is the same as X_test</p> Source code in <code>src\\pyhbr\\analysis\\stability.py</code> <pre><code>def predict_probabilities(fitted_model: FittedModel, X_test: DataFrame) -&gt; DataFrame:\n    \"\"\"Predict outcome probabilities using the fitted models on the test set\n\n    Aggregating function which finds the predicted probability\n    from the model-under-test M0 and all the bootstrapped models\n    Mn on each sample of the training set features X_test. The\n    result is a 2D numpy array, where each row corresponds to\n    a test-set sample, the first column is the predicted probabilities\n    from M0, and the following N columns are the predictions from all\n    the other Mn.\n\n    Note: the numbers in the matrix are the probabilities of 1 in the\n    test set y_test.\n\n    Args:\n        fitted_model: The model fitted on the training set and resamples\n\n    Returns:\n        An table of probabilities of the positive outcome in the class,\n            where each column comes from a different model. Column zero\n            corresponds to the training set, and the other columns are\n            from the resamples. The index for the DataFrame is the same\n            as X_test\n    \"\"\"\n    columns = []\n    for m, M in enumerate(fitted_model.flatten()):\n        log.info(f\"Predicting test-set probabilities {m}\")\n        columns.append(M.predict_proba(X_test)[:, 1])\n\n    raw_probs = np.column_stack(columns)\n\n    df = DataFrame(raw_probs)\n    df.columns = [f\"prob_M{m}\" for m in range(len(fitted_model.Mm) + 1)]\n    df.index = X_test.index\n    return df\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes","title":"<code>clinical_codes</code>","text":"<p>Contains utilities for clinical code groups</p>"},{"location":"reference/#pyhbr.clinical_codes.Category","title":"<code>Category</code>  <code>dataclass</code>","text":"<p>Code/categories struct</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the category (e.g. I20) or clinical code (I20.1)</p> <code>docs</code> <code>str</code> <p>The description of the category or code</p> <code>index</code> <code>str | tuple[str, str]</code> <p>Used to sort a list of Categories</p> <code>categories</code> <code>list[Category] | None</code> <p>For a category, the list of sub-categories contained. None for a code.</p> <code>exclude</code> <code>set[str] | None</code> <p>Contains code groups which do not contain any members from this category or any of its sub-categories.</p> Source code in <code>src\\pyhbr\\clinical_codes\\__init__.py</code> <pre><code>@dataclass\nclass Category:\n    \"\"\"Code/categories struct\n\n    Attributes:\n        name: The name of the category (e.g. I20) or clinical code (I20.1)\n        docs: The description of the category or code\n        index: Used to sort a list of Categories\n        categories: For a category, the list of sub-categories contained.\n            None for a code.\n        exclude: Contains code groups which do not contain any members\n            from this category or any of its sub-categories.\n\n    \"\"\"\n\n    name: str\n    docs: str\n    index: str | tuple[str, str]\n    categories: list[Category] | None\n    exclude: set[str] | None\n\n    def is_leaf(self):\n        \"\"\"Check if the categories is a leaf node\n\n        Returns:\n            True if leaf node (i.e. clinical code), false otherwise\n        \"\"\"\n        return self.categories is None\n\n    def excludes(self, group: str) -&gt; bool:\n        \"\"\"Check if this category excludes a code group\n\n        Args:\n            group: The string name of the group to check\n\n        Returns:\n            True if the group is excluded; False otherwise\n        \"\"\"\n        if self.exclude is not None:\n            return group in self.exclude\n        else:\n            return False\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes.Category.excludes","title":"<code>excludes(group)</code>","text":"<p>Check if this category excludes a code group</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>str</code> <p>The string name of the group to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the group is excluded; False otherwise</p> Source code in <code>src\\pyhbr\\clinical_codes\\__init__.py</code> <pre><code>def excludes(self, group: str) -&gt; bool:\n    \"\"\"Check if this category excludes a code group\n\n    Args:\n        group: The string name of the group to check\n\n    Returns:\n        True if the group is excluded; False otherwise\n    \"\"\"\n    if self.exclude is not None:\n        return group in self.exclude\n    else:\n        return False\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes.Category.is_leaf","title":"<code>is_leaf()</code>","text":"<p>Check if the categories is a leaf node</p> <p>Returns:</p> Type Description <p>True if leaf node (i.e. clinical code), false otherwise</p> Source code in <code>src\\pyhbr\\clinical_codes\\__init__.py</code> <pre><code>def is_leaf(self):\n    \"\"\"Check if the categories is a leaf node\n\n    Returns:\n        True if leaf node (i.e. clinical code), false otherwise\n    \"\"\"\n    return self.categories is None\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes.ClinicalCode","title":"<code>ClinicalCode</code>  <code>dataclass</code>","text":"<p>Store a clinical code together with its description.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The code itself, e.g. \"I21.0\"</p> <code>docs</code> <code>str</code> <p>The code description, e.g. \"Acute transmural myocardial infarction of anterior wall\"</p> Source code in <code>src\\pyhbr\\clinical_codes\\__init__.py</code> <pre><code>@dataclass\nclass ClinicalCode:\n    \"\"\"Store a clinical code together with its description.\n\n    Attributes:\n        name: The code itself, e.g. \"I21.0\"\n        docs: The code description, e.g. \"Acute\n            transmural myocardial infarction of anterior wall\"\n    \"\"\"\n\n    name: str\n    docs: str\n\n    def normalise(self):\n        \"\"\"Return the name without whitespace/dots, as lowercase\n\n        See the documentation for [normalize_code()][pyhbr.clinical_codes.normalise_code].\n\n        Returns:\n            The normalized form of this clinical code\n        \"\"\"\n        return normalise_code(self.name)\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes.ClinicalCode.normalise","title":"<code>normalise()</code>","text":"<p>Return the name without whitespace/dots, as lowercase</p> <p>See the documentation for normalize_code().</p> <p>Returns:</p> Type Description <p>The normalized form of this clinical code</p> Source code in <code>src\\pyhbr\\clinical_codes\\__init__.py</code> <pre><code>def normalise(self):\n    \"\"\"Return the name without whitespace/dots, as lowercase\n\n    See the documentation for [normalize_code()][pyhbr.clinical_codes.normalise_code].\n\n    Returns:\n        The normalized form of this clinical code\n    \"\"\"\n    return normalise_code(self.name)\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes.ClinicalCodeTree","title":"<code>ClinicalCodeTree</code>  <code>dataclass</code>","text":"<p>Code definition file structure</p> Source code in <code>src\\pyhbr\\clinical_codes\\__init__.py</code> <pre><code>@serde\n@dataclass\nclass ClinicalCodeTree:\n    \"\"\"Code definition file structure\"\"\"\n\n    categories: list[Category]\n    groups: set[str]\n\n    def codes_in_group(self, group: str) -&gt; list[ClinicalCode]:\n        \"\"\"Get the clinical codes in a code group\n\n        Args:\n            group: The group to fetch\n\n        Raises:\n            ValueError: Raised if the requested group does not exist\n\n        Returns:\n            The list of code groups\n        \"\"\"\n        if not group in self.groups:\n            raise ValueError(f\"'{group}' is not a valid code group ({self.groups})\")\n\n        return get_codes_in_group(group, self.categories)\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes.ClinicalCodeTree.codes_in_group","title":"<code>codes_in_group(group)</code>","text":"<p>Get the clinical codes in a code group</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>str</code> <p>The group to fetch</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Raised if the requested group does not exist</p> <p>Returns:</p> Type Description <code>list[ClinicalCode]</code> <p>The list of code groups</p> Source code in <code>src\\pyhbr\\clinical_codes\\__init__.py</code> <pre><code>def codes_in_group(self, group: str) -&gt; list[ClinicalCode]:\n    \"\"\"Get the clinical codes in a code group\n\n    Args:\n        group: The group to fetch\n\n    Raises:\n        ValueError: Raised if the requested group does not exist\n\n    Returns:\n        The list of code groups\n    \"\"\"\n    if not group in self.groups:\n        raise ValueError(f\"'{group}' is not a valid code group ({self.groups})\")\n\n    return get_codes_in_group(group, self.categories)\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes.codes_in_any_group","title":"<code>codes_in_any_group(codes)</code>","text":"<p>Get a DataFrame of all the codes in any group in a codes file</p> <p>Returns a table with the normalised code (lowercase/no whitespace/no dots) in column <code>code</code>, and the group containing the code in the column <code>group</code>.</p> <p>All codes which are in any group will be included.</p> <p>Codes will be duplicated if they appear in more than one group.</p> <p>Parameters:</p> Name Type Description Default <code>codes</code> <code>ClinicalCodeTree</code> <p>The tree clinical codes (e.g. ICD-10 or OPCS-4, loaded from a file) to search for codes</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: All codes in any group in the codes file</p> Source code in <code>src\\pyhbr\\clinical_codes\\__init__.py</code> <pre><code>def codes_in_any_group(codes: ClinicalCodeTree) -&gt; pd.DataFrame:\n    \"\"\"Get a DataFrame of all the codes in any group in a codes file\n\n    Returns a table with the normalised code (lowercase/no whitespace/no\n    dots) in column `code`, and the group containing the code in the\n    column `group`.\n\n    All codes which are in any group will be included.\n\n    Codes will be duplicated if they appear in more than one group.\n\n    Args:\n        codes: The tree clinical codes (e.g. ICD-10 or OPCS-4, loaded\n            from a file) to search for codes\n\n    Returns:\n        pd.DataFrame: All codes in any group in the codes file\n    \"\"\"\n    dfs = []\n    for g in codes.groups:\n        clinical_codes = codes.codes_in_group(g)\n        normalised_codes = [c.normalise() for c in clinical_codes]\n        docs = [c.docs for c in clinical_codes]\n        df = pd.DataFrame({\"code\": normalised_codes, \"docs\": docs, \"group\": g})\n        dfs.append(df)\n\n    return pd.concat(dfs).reset_index(drop=True)\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes.filter_to_groups","title":"<code>filter_to_groups(codes_table, codes)</code>","text":"<p>Filter a table of raw clinical codes to only keep codes in groups</p> <p>Use this function to drop clinical codes which are not of interest, and convert all codes to normalised form (lowercase, no whitespace, no dot).</p> <p>This function is tested on the HIC dataset, but should be modifiable for use with any data source returning diagnoses and procedures as separate tables in long format. Consider modifying the columns of codes_table that are contained in the output.</p> <p>Parameters:</p> Name Type Description Default <code>codes_table</code> <code>DataFrame</code> <p>Either a diagnoses or procedures table. For this function to work, it needs:</p> <ul> <li>A <code>code</code> column containing the clinical code.</li> <li>An <code>episode_id</code> identifying which episode contains the code.</li> <li>A <code>position</code> identifying the primary/secondary position of the     code in the episode.</li> </ul> required <code>codes</code> <code>ClinicalCodeTree</code> <p>The clinical codes object (previously loaded from a file) containing code groups to use.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A table containing the episode ID, the clinical code (normalised), the group containing the code, and the code position.</p> Source code in <code>src\\pyhbr\\clinical_codes\\__init__.py</code> <pre><code>def filter_to_groups(\n    codes_table: pd.DataFrame, codes: ClinicalCodeTree\n) -&gt; pd.DataFrame:\n    \"\"\"Filter a table of raw clinical codes to only keep codes in groups\n\n    Use this function to drop clinical codes which are not of interest,\n    and convert all codes to normalised form (lowercase, no whitespace, no dot).\n\n    This function is tested on the HIC dataset, but should be modifiable\n    for use with any data source returning diagnoses and procedures as\n    separate tables in long format. Consider modifying the columns of\n    codes_table that are contained in the output.\n\n    Args:\n        codes_table: Either a diagnoses or procedures table. For this\n            function to work, it needs:\n\n            * A `code` column containing the clinical code.\n            * An `episode_id` identifying which episode contains the code.\n            * A `position` identifying the primary/secondary position of the\n                code in the episode.\n\n        codes: The clinical codes object (previously loaded from a file)\n            containing code groups to use.\n\n    Returns:\n        A table containing the episode ID, the clinical code (normalised),\n            the group containing the code, and the code position.\n\n    \"\"\"\n    codes_with_groups = codes_in_any_group(codes)\n    codes_table[\"code\"] = codes_table[\"code\"].apply(normalise_code)\n    codes_table = pd.merge(codes_table, codes_with_groups, on=\"code\", how=\"inner\")\n    codes_table = codes_table[[\"episode_id\", \"code\", \"docs\", \"group\", \"position\"]]\n\n    return codes_table\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes.get_code_groups","title":"<code>get_code_groups(diagnosis_codes, procedure_codes)</code>","text":"<p>Get a table of any diagnosis/procedure code which is in a code group</p> <p>This function converts the code tree formats into a simple table containing  normalised codes (lowercase, no dot), the documentation string for the code, what group the code is in, and whether it is a diagnosis or procedure code</p> <p>Parameters:</p> Name Type Description Default <code>diagnosis_codes</code> <code>ClinicalCodeTree</code> <p>The tree of diagnosis codes</p> required <code>procedure_codes</code> <code>ClinicalCodeTree</code> <p>The tree of procedure codes</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A table with columns <code>code</code>, <code>docs</code>, <code>group</code> and <code>type</code>.</p> Source code in <code>src\\pyhbr\\clinical_codes\\__init__.py</code> <pre><code>def get_code_groups(diagnosis_codes: ClinicalCodeTree, procedure_codes: ClinicalCodeTree) -&gt; DataFrame:\n    \"\"\"Get a table of any diagnosis/procedure code which is in a code group\n\n    This function converts the code tree formats into a simple table containing \n    normalised codes (lowercase, no dot), the documentation string for the code,\n    what group the code is in, and whether it is a diagnosis or procedure code\n\n    Args:\n        diagnosis_codes: The tree of diagnosis codes\n        procedure_codes: The tree of procedure codes\n\n    Returns:\n        A table with columns `code`, `docs`, `group` and `type`.\n    \"\"\"\n\n    diagnosis_groups = codes_in_any_group(diagnosis_codes)\n    procedure_groups = codes_in_any_group(procedure_codes)\n    diagnosis_groups[\"type\"] = \"diagnosis\"\n    procedure_groups[\"type\"] = \"procedure\"\n    code_groups = pd.concat([diagnosis_groups, procedure_groups]).reset_index(drop=True)\n    code_groups[\"type\"] = code_groups[\"type\"].astype(\"category\")\n    return code_groups\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes.get_codes_in_group","title":"<code>get_codes_in_group(group, categories)</code>","text":"<p>Helper function to get clinical codes in a group</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>str</code> <p>The group to fetch</p> required <code>categories</code> <code>list[Category]</code> <p>The list of categories to search for codes</p> required <p>Returns:</p> Type Description <code>list[ClinicalCode]</code> <p>A list of clinical codes in the group</p> Source code in <code>src\\pyhbr\\clinical_codes\\__init__.py</code> <pre><code>def get_codes_in_group(group: str, categories: list[Category]) -&gt; list[ClinicalCode]:\n    \"\"\"Helper function to get clinical codes in a group\n\n    Args:\n        group: The group to fetch\n        categories: The list of categories to search for codes\n\n    Returns:\n        A list of clinical codes in the group\n    \"\"\"\n\n    # Filter out the categories that exclude the group\n    categories_left = [c for c in categories if not c.excludes(group)]\n\n    codes_in_group = []\n\n    # Loop over the remaining categories. For all the leaf\n    # categories, if there is no exclude for this group,\n    # include it in the results. For non-leaf categories,\n    # call this function again and append the resulting codes\n    for category in categories_left:\n        if category.is_leaf() and not category.excludes(group):\n            code = ClinicalCode(name=category.name, docs=category.docs)\n            codes_in_group.append(code)\n        else:\n            sub_categories = category.categories\n            # Check it is non-empty (or refactor logic)\n            new_codes = get_codes_in_group(group, sub_categories)\n            codes_in_group.extend(new_codes)\n\n    return codes_in_group\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes.load_from_file","title":"<code>load_from_file(path)</code>","text":"<p>Load a clinical codes file relative to the working directory</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the codes file relative to the current working directory.</p> required <p>Returns:</p> Type Description <code>ClinicalCodeTree</code> <p>The contents of the file</p> Source code in <code>src\\pyhbr\\clinical_codes\\__init__.py</code> <pre><code>def load_from_file(path: str) -&gt; ClinicalCodeTree:\n    \"\"\"Load a clinical codes file relative to the working directory\n\n    Args:\n        path: The path to the codes file relative to the current\n            working directory.\n\n    Returns:\n        The contents of the file\n    \"\"\"\n    with open(path, \"r\") as file:\n        contents = file.read()\n        return from_yaml(ClinicalCodeTree, contents)\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes.load_from_package","title":"<code>load_from_package(name)</code>","text":"<p>Load a clinical codes file from the pyhbr package.</p> <p>The clinical codes are stored in yaml format, and this function returns a dictionary corresponding to the structure of the yaml file.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pyhbr.clinical_codes as codes\n&gt;&gt;&gt; tree = codes.load_from_package(\"icd10_test.yaml\")\n&gt;&gt;&gt; group = tree.codes_in_group(\"group_1\")\n&gt;&gt;&gt; [code.name for code in group]\n['I20.0', 'I20.1', 'I20.8', 'I20.9']\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The file name of the codes file to load</p> required <p>Returns:</p> Type Description <code>ClinicalCodeTree</code> <p>The contents of the file.</p> Source code in <code>src\\pyhbr\\clinical_codes\\__init__.py</code> <pre><code>def load_from_package(name: str) -&gt; ClinicalCodeTree:\n    \"\"\"Load a clinical codes file from the pyhbr package.\n\n    The clinical codes are stored in yaml format, and this\n    function returns a dictionary corresponding to the structure\n    of the yaml file.\n\n    Examples:\n        &gt;&gt;&gt; import pyhbr.clinical_codes as codes\n        &gt;&gt;&gt; tree = codes.load_from_package(\"icd10_test.yaml\")\n        &gt;&gt;&gt; group = tree.codes_in_group(\"group_1\")\n        &gt;&gt;&gt; [code.name for code in group]\n        ['I20.0', 'I20.1', 'I20.8', 'I20.9']\n\n    Args:\n        name: The file name of the codes file to load\n\n    Returns:\n        The contents of the file.\n    \"\"\"\n    contents = res_files(\"pyhbr.clinical_codes.files\").joinpath(name).read_text()\n    return from_yaml(ClinicalCodeTree, contents)\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes.normalise_code","title":"<code>normalise_code(code)</code>","text":"<p>Remove whitespace/dots, and convert to lower-case</p> <p>The format of clinical codes can vary across different data sources. A simple way to compare codes is to convert them into a common format and compare them as strings. The purpose of this function is to define the common format, which uses all lower-case letters, does not contain any dots, and does not include any leading/trailing whitespace.</p> <p>Comparing codes for equality does not immediately allow checking whether one code is a sub-category of another. It also ignores clinical code annotations such as dagger/asterisk.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; normalise_code(\"  I21.0 \")\n'i210'\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>The raw code, e.g.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The normalised form of the clinical code</p> Source code in <code>src\\pyhbr\\clinical_codes\\__init__.py</code> <pre><code>def normalise_code(code: str) -&gt; str:\n    \"\"\"Remove whitespace/dots, and convert to lower-case\n\n    The format of clinical codes can vary across different data\n    sources. A simple way to compare codes is to convert them into\n    a common format and compare them as strings. The purpose of\n    this function is to define the common format, which uses all\n    lower-case letters, does not contain any dots, and does not\n    include any leading/trailing whitespace.\n\n    Comparing codes for equality does not immediately allow checking\n    whether one code is a sub-category of another. It also ignores\n    clinical code annotations such as dagger/asterisk.\n\n    Examples:\n        &gt;&gt;&gt; normalise_code(\"  I21.0 \")\n        'i210'\n\n    Args:\n        code: The raw code, e.g.\n\n    Returns:\n        The normalised form of the clinical code\n    \"\"\"\n    return code.lower().strip().replace(\".\", \"\")\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes.codes_editor","title":"<code>codes_editor</code>","text":"<p>Edit groups of ICD-10 and OPCS-4 codes</p>"},{"location":"reference/#pyhbr.clinical_codes.codes_editor.codes_editor","title":"<code>codes_editor</code>","text":""},{"location":"reference/#pyhbr.clinical_codes.codes_editor.codes_editor.run_app","title":"<code>run_app()</code>","text":"<p>Run the main codes editor application</p> Source code in <code>src\\pyhbr\\clinical_codes\\codes_editor\\codes_editor.py</code> <pre><code>def run_app() -&gt; None:\n    \"\"\"Run the main codes editor application\n    \"\"\"\n\n    # You need one (and only one) QApplication instance per application.\n    # Pass in sys.argv to allow command line arguments for your app.\n    # If you know you won't use command line arguments QApplication([]) works too.\n    app = QApplication(sys.argv)\n\n    # Create a Qt widget, which will be our window.\n    window = MainWindow()\n    window.show()\n\n    # Start the event loop.\n    app.exec()\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes.counting","title":"<code>counting</code>","text":"<p>Utilities for counting clinical codes satisfying conditions</p>"},{"location":"reference/#pyhbr.clinical_codes.counting.count_code_groups","title":"<code>count_code_groups(index_spells, filtered_episodes)</code>","text":"<p>Count the number of matching codes relative to index episodes</p> <p>This function counts the rows for each index spell ID in the output of filter_by_code_groups, and adds 0 for any index spell ID without any matching rows in filtered_episodes.</p> <p>The intent is to count the number of codes (one per row) that matched filter conditions in other episodes with respect to the index spell.</p> <p>Parameters:</p> Name Type Description Default <code>index_spells</code> <code>DataFrame</code> <p>The index spells, which provides the list of spell IDs of interest. The output will be NA for any spell ID that does not have any matching rows in filtered_episodes.</p> required <code>filtered_episodes</code> <code>DataFrame</code> <p>The output from filter_by_code_groups, which produces a table where each row represents a matching code.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>How many codes (rows) occurred for each index spell</p> Source code in <code>src\\pyhbr\\clinical_codes\\counting.py</code> <pre><code>def count_code_groups(index_spells: DataFrame, filtered_episodes: DataFrame) -&gt; Series:\n    \"\"\"Count the number of matching codes relative to index episodes\n\n    This function counts the rows for each index spell ID in the output of\n    filter_by_code_groups, and adds 0 for any index spell ID without\n    any matching rows in filtered_episodes.\n\n    The intent is to count the number of codes (one per row) that matched\n    filter conditions in other episodes with respect to the index spell.\n\n    Args:\n        index_spells: The index spells, which provides the list of\n            spell IDs of interest. The output will be NA for any spell\n            ID that does not have any matching rows in filtered_episodes.\n        filtered_episodes: The output from filter_by_code_groups,\n            which produces a table where each row represents a matching\n            code.\n\n    Returns:\n        How many codes (rows) occurred for each index spell\n    \"\"\"\n    df = (\n        filtered_episodes.groupby(\"index_spell_id\")\n        .size()\n        .rename(\"count\")\n        .to_frame()\n        .reset_index(names=\"spell_id\")\n        .set_index(\"spell_id\")\n    )\n    return index_spells[[]].merge(df, how=\"left\", on=\"spell_id\").fillna(0)[\"count\"]\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes.counting.count_events","title":"<code>count_events(index_spells, events, event_name)</code>","text":"<p>Count the occurrences (rows) of an event given in long format.</p> <p>The input table (events) contains instances of events, one per row, where the event_name contains the name of a string column labelling the events. The table also contains a <code>spell_id</code> column, which may be  associated with multiple rows.</p> <p>The function pivots the events so that there is one row per spell, each event has its own column, and the table contains the total number of each event associated with the spell.</p> <p>The index_spells table is required because some index spells may have no events. These index spells will have a row of zeros in the output.</p> <p>Parameters:</p> Name Type Description Default <code>index_spells</code> <code>DataFrame</code> <p>Must have Pandas index <code>spell_id</code></p> required <code>events</code> <code>DataFrame</code> <p>Contains a <code>spell_id</code> column and an event_name column.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A table of the counts for each event (one event per column), with Pandas index <code>spell_id</code>.</p> Source code in <code>src\\pyhbr\\clinical_codes\\counting.py</code> <pre><code>def count_events(index_spells: DataFrame, events: DataFrame, event_name: str) -&gt; DataFrame:\n    \"\"\"Count the occurrences (rows) of an event given in long format.\n\n    The input table (events) contains instances of events, one per row,\n    where the event_name contains the name of a string column labelling the\n    events. The table also contains a `spell_id` column, which may be \n    associated with multiple rows.\n\n    The function pivots the events so that there is one row per spell,\n    each event has its own column, and the table contains the total number\n    of each event associated with the spell.\n\n    The index_spells table is required because some index spells may have\n    no events. These index spells will have a row of zeros in the output.\n\n    Args:\n        index_spells: Must have Pandas index `spell_id`\n        events: Contains a `spell_id` column and an event_name\n            column.\n\n    Returns:\n        A table of the counts for each event (one event per column), with\n            Pandas index `spell_id`.\n    \"\"\"\n\n    # Pivot the prescriptions into one column per medicine type,\n    # and prefix the name with \"prior_\" (e.g. \"prior_oac\").\n    nonzero_counts = (\n        events.groupby(\"spell_id\")[event_name]\n        .value_counts()\n        .unstack(fill_value=0)\n    )\n    all_counts = (\n        index_spells[[]].merge(nonzero_counts, how=\"left\", on=\"spell_id\").fillna(0)\n    )\n    return all_counts\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes.counting.get_all_other_codes","title":"<code>get_all_other_codes(index_spells, episodes, codes)</code>","text":"<p>For each patient, get clinical codes in other episodes before/after the index</p> <p>This makes a table of index episodes (which is the first episode of the index spell) along with all other episodes for a patient. Two columns <code>index_episode_id</code> and <code>other_episode_id</code> identify the two episodes for each row (they may be equal), and other information is stored such as the time of the base episode, the time to the other episode, and clinical code information for the other episode.</p> <p>This table is used as the basis for all processing involving counting codes before and after an episode.</p> <p>Note</p> <p>Episodes will not be included in the result if they do not have any clinical     codes that are in any code group.</p> <p>Parameters:</p> Name Type Description Default <code>index_spells</code> <code>DataFrame</code> <p>Contains <code>episode_id</code> as an index.</p> required <code>episodes</code> <code>DataFrame</code> <p>Contains <code>episode_id</code> as an index, and <code>patient_id</code> and <code>episode_start</code> as columns</p> required <code>codes</code> <code>DataFrame</code> <p>Contains <code>episode_id</code> and other code data as columns</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A table containing columns <code>index_episode_id</code>, <code>other_episode_id</code>, <code>index_episode_start</code>, <code>time_to_other_episode</code>, and code data columns for the other episode. Note that the base episode itself is included as an other episode.</p> Source code in <code>src\\pyhbr\\clinical_codes\\counting.py</code> <pre><code>def get_all_other_codes(\n    index_spells: DataFrame, episodes: DataFrame, codes: DataFrame\n) -&gt; DataFrame:\n    \"\"\"For each patient, get clinical codes in other episodes before/after the index\n\n    This makes a table of index episodes (which is the first episode of the index spell)\n    along with all other episodes for a patient. Two columns `index_episode_id` and\n    `other_episode_id` identify the two episodes for each row (they may be equal), and\n    other information is stored such as the time of the base episode, the time to the\n    other episode, and clinical code information for the other episode.\n\n    This table is used as the basis for all processing involving counting codes before\n    and after an episode.\n\n    !!! note\n        Episodes will not be included in the result if they do not have any clinical\n            codes that are in any code group.\n\n    Args:\n        index_spells: Contains `episode_id` as an index.\n        episodes: Contains `episode_id` as an index, and `patient_id` and `episode_start` as columns\n        codes: Contains `episode_id` and other code data as columns\n\n    Returns:\n        A table containing columns `index_episode_id`, `other_episode_id`,\n            `index_episode_start`, `time_to_other_episode`, and code data columns\n            for the other episode. Note that the base episode itself is included\n            as an other episode.\n    \"\"\"\n\n    # Remove everything but the index episode_id (in case base_episodes\n    # already has the columns)\n    df = index_spells.reset_index(names=\"spell_id\").set_index(\"episode_id\")[\n        [\"spell_id\"]\n    ]\n\n    index_episode_info = df.merge(\n        episodes[[\"patient_id\", \"episode_start\"]], how=\"left\", on=\"episode_id\"\n    ).rename(\n        columns={\"episode_start\": \"index_episode_start\", \"spell_id\": \"index_spell_id\"}\n    )\n\n    other_episodes = (\n        index_episode_info.reset_index(names=\"index_episode_id\")\n        .merge(\n            episodes[[\"episode_start\", \"patient_id\", \"spell_id\"]].reset_index(\n                names=\"other_episode_id\"\n            ),\n            how=\"left\",\n            on=\"patient_id\",\n        )\n        .rename(columns={\"spell_id\": \"other_spell_id\"})\n    )\n\n    other_episodes[\"time_to_other_episode\"] = (\n        other_episodes[\"episode_start\"] - other_episodes[\"index_episode_start\"]\n    )\n\n    # Use an inner join to filter out other episodes that have no associated codes\n    # in any group.\n    with_codes = other_episodes.merge(\n        codes, how=\"inner\", left_on=\"other_episode_id\", right_on=\"episode_id\"\n    ).drop(columns=[\"patient_id\", \"episode_start\", \"episode_id\"])\n\n    return with_codes\n</code></pre>"},{"location":"reference/#pyhbr.clinical_codes.counting.get_time_window","title":"<code>get_time_window(time_diff_table, window_start, window_end, time_diff_column='time_to_other_episode')</code>","text":"<p>Get events that occurred in a time window with respect to a base event</p> <p>Use the time_diff_column column to filter the time_diff_table to just those that occurred between window_start and window_end with respect to the base.  For example, rows can represent an index episode paired with other episodes, with the time_diff_column representing the time to the other episode.</p> <p>The arguments window_start and window_end control the minimum and maximum  values for the time difference. Use positive values for a window after the  base event, and use negative values for a window before the base event.</p> <p>Events on the boundary of the window are included.</p> <p>Note that the base event itself will be included as a row if window_start is negative and window_end is positive.</p> <p>Parameters:</p> Name Type Description Default <code>time_diff_table</code> <code>DataFrame</code> <p>Table containing at least the <code>time_diff_column</code></p> required <code>window_start</code> <code>timedelta</code> <p>The smallest value of <code>time_diff_column</code> that will be included in the returned table. Can be negative, meaning events before the base event will be included.</p> required <code>window_end</code> <code>timedelta</code> <p>The largest value of <code>time_diff_column</code> that will be included in the returned table. Can be negative, meaning events after the base will be included.</p> required <code>time_diff_column</code> <code>str</code> <p>The name of the column containing the time difference, which is positive for an event occurring after the base event.</p> <code>'time_to_other_episode'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The rows within the specific time window</p> Source code in <code>src\\pyhbr\\clinical_codes\\counting.py</code> <pre><code>def get_time_window(\n    time_diff_table: DataFrame,\n    window_start: timedelta,\n    window_end: timedelta,\n    time_diff_column: str = \"time_to_other_episode\",\n) -&gt; DataFrame:\n    \"\"\"Get events that occurred in a time window with respect to a base event\n\n    Use the time_diff_column column to filter the time_diff_table to just those\n    that occurred between window_start and window_end with respect to the base. \n    For example, rows can represent an index episode paired with other episodes,\n    with the time_diff_column representing the time to the other episode.\n\n    The arguments window_start and window_end control the minimum and maximum \n    values for the time difference. Use positive values for a window after the \n    base event, and use negative values for a window before the base event.\n\n    Events on the boundary of the window are included.\n\n    Note that the base event itself will be included as a row if window_start\n    is negative and window_end is positive.\n\n    Args:\n        time_diff_table: Table containing at least the `time_diff_column`\n        window_start: The smallest value of `time_diff_column` that will be included\n            in the returned table. Can be negative, meaning events before the base\n            event will be included.\n        window_end: The largest value of `time_diff_column` that will be included in\n            the returned table. Can be negative, meaning events after the base\n            will be included.\n        time_diff_column: The name of the column containing the time difference,\n            which is positive for an event occurring after the base event.\n\n    Returns:\n        The rows within the specific time window\n    \"\"\"\n    df = time_diff_table\n    return df[\n        (df[time_diff_column] &lt;= window_end) &amp; (df[time_diff_column] &gt;= window_start)\n    ]\n</code></pre>"},{"location":"reference/#pyhbr.common","title":"<code>common</code>","text":"<p>Common utilities for other modules.</p> <p>A collection of routines used by the data source or analysis functions.</p>"},{"location":"reference/#pyhbr.common.CheckedTable","title":"<code>CheckedTable</code>","text":"<p>Wrapper for sqlalchemy table with checks for table/columns</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>class CheckedTable:\n    \"\"\"Wrapper for sqlalchemy table with checks for table/columns\"\"\"\n\n    def __init__(self, table_name: str, engine: Engine, schema=\"dbo\") -&gt; None:\n        \"\"\"Get a CheckedTable by reading from the remote server\n\n        This is a wrapper around the sqlalchemy Table for\n        catching errors when accessing columns through the\n        c attribute.\n\n        Args:\n            table_name: The name of the table whose metadata should be retrieved\n            engine: The database connection\n\n        Returns:\n            The table data for use in SQL queries\n        \"\"\"\n        self.name = table_name\n        metadata_obj = MetaData(schema=schema)\n        try:\n            self.table = Table(self.name, metadata_obj, autoload_with=engine)\n        except NoSuchTableError as e:\n            raise RuntimeError(\n                f\"Could not find table '{e}' in database connection '{engine.url}'\"\n            ) from e\n\n    def col(self, column_name: str) -&gt; Column:\n        \"\"\"Get a column\n\n        Args:\n            column_name: The name of the column to fetch.\n\n        Raises:\n            RuntimeError: Thrown if the column does not exist\n        \"\"\"\n        try:\n            return self.table.c[column_name]\n        except AttributeError as e:\n            raise RuntimeError(\n                f\"Could not find column name '{column_name}' in table '{self.name}'\"\n            ) from e\n</code></pre>"},{"location":"reference/#pyhbr.common.CheckedTable.__init__","title":"<code>__init__(table_name, engine, schema='dbo')</code>","text":"<p>Get a CheckedTable by reading from the remote server</p> <p>This is a wrapper around the sqlalchemy Table for catching errors when accessing columns through the c attribute.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>The name of the table whose metadata should be retrieved</p> required <code>engine</code> <code>Engine</code> <p>The database connection</p> required <p>Returns:</p> Type Description <code>None</code> <p>The table data for use in SQL queries</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def __init__(self, table_name: str, engine: Engine, schema=\"dbo\") -&gt; None:\n    \"\"\"Get a CheckedTable by reading from the remote server\n\n    This is a wrapper around the sqlalchemy Table for\n    catching errors when accessing columns through the\n    c attribute.\n\n    Args:\n        table_name: The name of the table whose metadata should be retrieved\n        engine: The database connection\n\n    Returns:\n        The table data for use in SQL queries\n    \"\"\"\n    self.name = table_name\n    metadata_obj = MetaData(schema=schema)\n    try:\n        self.table = Table(self.name, metadata_obj, autoload_with=engine)\n    except NoSuchTableError as e:\n        raise RuntimeError(\n            f\"Could not find table '{e}' in database connection '{engine.url}'\"\n        ) from e\n</code></pre>"},{"location":"reference/#pyhbr.common.CheckedTable.col","title":"<code>col(column_name)</code>","text":"<p>Get a column</p> <p>Parameters:</p> Name Type Description Default <code>column_name</code> <code>str</code> <p>The name of the column to fetch.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Thrown if the column does not exist</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def col(self, column_name: str) -&gt; Column:\n    \"\"\"Get a column\n\n    Args:\n        column_name: The name of the column to fetch.\n\n    Raises:\n        RuntimeError: Thrown if the column does not exist\n    \"\"\"\n    try:\n        return self.table.c[column_name]\n    except AttributeError as e:\n        raise RuntimeError(\n            f\"Could not find column name '{column_name}' in table '{self.name}'\"\n        ) from e\n</code></pre>"},{"location":"reference/#pyhbr.common.chunks","title":"<code>chunks(patient_ids, n)</code>","text":"<p>Divide a list of patient ids into n-sized chunks</p> <p>The last chunk may be shorter.</p> <p>Parameters:</p> Name Type Description Default <code>patient_ids</code> <code>list[str]</code> <p>The List of IDs to chunk</p> required <code>n</code> <code>int</code> <p>The chunk size.</p> required <p>Returns:</p> Type Description <code>list[list[str]]</code> <p>A list containing chunks (list) of patient IDs</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def chunks(patient_ids: list[str], n: int) -&gt; list[list[str]]:\n    \"\"\"Divide a list of patient ids into n-sized chunks\n\n    The last chunk may be shorter.\n\n    Args:\n        patient_ids: The List of IDs to chunk\n        n: The chunk size.\n\n    Returns:\n        A list containing chunks (list) of patient IDs\n    \"\"\"\n    return [patient_ids[i : i + n] for i in range(0, len(patient_ids), n)]\n</code></pre>"},{"location":"reference/#pyhbr.common.current_commit","title":"<code>current_commit()</code>","text":"<p>Get current commit.</p> <p>Returns:</p> Type Description <code>str</code> <p>Get the first 12 characters of the current commit, using the first repository found above the current working directory. If the working directory is not in a git repository, return \"nogit\".</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def current_commit() -&gt; str:\n    \"\"\"Get current commit.\n\n    Returns:\n        Get the first 12 characters of the current commit,\n            using the first repository found above the current\n            working directory. If the working directory is not\n            in a git repository, return \"nogit\".\n    \"\"\"\n    try:\n        repo = Repo(search_parent_directories=True)\n        sha = repo.head.object.hexsha[0:11]\n        return sha\n    except InvalidGitRepositoryError:\n        return \"nogit\"\n</code></pre>"},{"location":"reference/#pyhbr.common.current_timestamp","title":"<code>current_timestamp()</code>","text":"<p>Get the current timestamp.</p> <p>Returns:</p> Type Description <code>int</code> <p>The current timestamp (since epoch) rounded to the nearest second.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def current_timestamp() -&gt; int:\n    \"\"\"Get the current timestamp.\n\n    Returns:\n        The current timestamp (since epoch) rounded\n            to the nearest second.\n    \"\"\"\n    return int(time())\n</code></pre>"},{"location":"reference/#pyhbr.common.get_data","title":"<code>get_data(engine, query, *args)</code>","text":"<p>Convenience function to make a query and fetch data.</p> <p>Wraps a function like hic.demographics_query with a call to pd.read_data.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>The database connection</p> required <code>query</code> <code>Callable[[Engine, ...], Select]</code> <p>A function returning a sqlalchemy Select statement</p> required <code>*args</code> <code>...</code> <p>Positional arguments to be passed to query in addition to engine (which is passed first). Make sure they are passed in the same order expected by the query function.</p> <code>()</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The pandas dataframe containing the SQL data</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def get_data(\n    engine: Engine, query: Callable[[Engine, ...], Select], *args: ...\n) -&gt; DataFrame:\n    \"\"\"Convenience function to make a query and fetch data.\n\n    Wraps a function like hic.demographics_query with a\n    call to pd.read_data.\n\n    Args:\n        engine: The database connection\n        query: A function returning a sqlalchemy Select statement\n        *args: Positional arguments to be passed to query in addition\n            to engine (which is passed first). Make sure they are passed\n            in the same order expected by the query function.\n\n    Returns:\n        The pandas dataframe containing the SQL data\n    \"\"\"\n    stmt = query(engine, *args)\n    df = read_sql(stmt, engine)\n\n    # Convert the column names to regular strings instead\n    # of sqlalchemy.sql.elements.quoted_name. This avoids\n    # an error down the line in sklearn, which cannot\n    # process sqlalchemy column title tuples.\n    df.columns = [str(col) for col in df.columns]\n\n    return df\n</code></pre>"},{"location":"reference/#pyhbr.common.get_data_by_patient","title":"<code>get_data_by_patient(engine, query, patient_ids, *args)</code>","text":"<p>Fetch data using a query restricted by patient ID</p> <p>The patient_id list is chunked into 2000 long batches to fit within an SQL IN clause, and each chunk is run as a separate query. The results are assembled into a single DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>The database connection</p> required <code>query</code> <code>Callable[[Engine, ...], Select]</code> <p>A function returning a sqlalchemy Select statement. Must take a list[str] as an argument after engine.</p> required <code>patient_ids</code> <code>list[str]</code> <p>A list of patient IDs to restrict the query.</p> required <code>*args</code> <code>...</code> <p>Further positional arguments that will be passed to the query function after the patient_ids positional argument.</p> <code>()</code> <p>Returns:</p> Type Description <code>list[DataFrame]</code> <p>A list of dataframes, one corresponding to each chunk.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def get_data_by_patient(\n    engine: Engine,\n    query: Callable[[Engine, ...], Select],\n    patient_ids: list[str],\n    *args: ...,\n) -&gt; list[DataFrame]:\n    \"\"\"Fetch data using a query restricted by patient ID\n\n    The patient_id list is chunked into 2000 long batches to fit\n    within an SQL IN clause, and each chunk is run as a separate\n    query. The results are assembled into a single DataFrame.\n\n    Args:\n        engine: The database connection\n        query: A function returning a sqlalchemy Select statement. Must\n            take a list[str] as an argument after engine.\n        patient_ids: A list of patient IDs to restrict the query.\n        *args: Further positional arguments that will be passed to the\n            query function after the patient_ids positional argument.\n\n    Returns:\n        A list of dataframes, one corresponding to each chunk.\n    \"\"\"\n    dataframes = []\n    patient_id_chunks = chunks(patient_ids, 2000)\n    num_chunks = len(patient_id_chunks)\n    chunk_count = 1\n    for chunk in patient_id_chunks:\n        print(f\"Fetching chunk {chunk_count}/{num_chunks}\")\n        dataframes.append(get_data(engine, query, chunk, *args))\n        chunk_count += 1\n    return dataframes\n</code></pre>"},{"location":"reference/#pyhbr.common.get_saved_files_by_name","title":"<code>get_saved_files_by_name(name, save_dir, extension)</code>","text":"<p>Get all saved data files matching name</p> <p>Get the list of files in the save_dir folder matching name. Return the result as a table of file path, commit hash, and saved date. The table is sorted by timestamp, with the most recent file first.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If save_dir does not exist, or there are files in save_dir within invalid file names (not in the format name_commit_timestamp.pkl).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the saved file to load. This matches name in the filename name_commit_timestamp.pkl.</p> required <code>save_dir</code> <code>str</code> <p>The directory to search for files.</p> required <code>extension</code> <code>str</code> <p>What file extension to look for. Do not include the dot.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe with columns <code>path</code>, <code>commit</code> and <code>created_data</code>.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def get_saved_files_by_name(name: str, save_dir: str, extension: str) -&gt; DataFrame:\n    \"\"\"Get all saved data files matching name\n\n    Get the list of files in the save_dir folder matching\n    name. Return the result as a table of file path, commit\n    hash, and saved date. The table is sorted by timestamp,\n    with the most recent file first.\n\n    Raises:\n        RuntimeError: If save_dir does not exist, or there are files\n            in save_dir within invalid file names (not in the format\n            name_commit_timestamp.pkl).\n\n    Args:\n        name: The name of the saved file to load. This matches name in\n            the filename name_commit_timestamp.pkl.\n        save_dir: The directory to search for files.\n        extension: What file extension to look for. Do not include the dot.\n\n    Returns:\n        A dataframe with columns `path`, `commit` and `created_data`.\n    \"\"\"\n\n    # Check for missing datasets directory\n    if not os.path.isdir(save_dir):\n        raise RuntimeError(\n            f\"Missing folder '{save_dir}'. Check your working directory.\"\n        )\n\n    # Read all the .pkl files in the directory\n    files = DataFrame({\"path\": os.listdir(save_dir)})\n\n    # Identify the file name part. The horrible regex matches the\n    # expression _[commit_hash]_[timestamp].pkl. It is important to\n    # match this part, because \"anything\" can happen in the name part\n    # (including underscores and letters and numbers), so splitting on\n    # _ would not work. The name can then be removed.\n    files[\"name\"] = files[\"path\"].str.replace(\n        rf\"_([0-9]|[a-zA-Z])*_\\d*\\.{extension}\", \"\", regex=True\n    )\n\n    # Remove all the files whose name does not match, and drop\n    # the name from the path\n    files = files[files[\"name\"] == name]\n    if files.shape[0] == 0:\n        raise ValueError(\n            f\"There is no file with the name '{name}' in the datasets directory\"\n        )\n    files[\"commit_and_timestamp\"] = files[\"path\"].str.replace(name + \"_\", \"\")\n\n    # Split the commit and timestamp up (note also the extension)\n    try:\n        files[[\"commit\", \"timestamp\", \"extension\"]] = files[\n            \"commit_and_timestamp\"\n        ].str.split(r\"_|\\.\", expand=True)\n    except Exception as exc:\n        raise RuntimeError(\n            \"Failed to parse files in the datasets folder. \"\n            \"Ensure that all files have the correct format \"\n            \"name_commit_timestamp.extension, and \"\n            \"remove any files not matching this \"\n            \"pattern. TODO handle this error properly, \"\n            \"see save_datasets.py.\"\n        ) from exc\n\n    files[\"created_date\"] = to_datetime(files[\"timestamp\"].astype(int), unit=\"s\")\n    recent_first = files.sort_values(by=\"timestamp\", ascending=False).reset_index()[\n        [\"path\", \"commit\", \"created_date\"]\n    ]\n    return recent_first\n</code></pre>"},{"location":"reference/#pyhbr.common.load_exact_item","title":"<code>load_exact_item(name, save_dir='save_data')</code>","text":"<p>Load a previously saved item (pickle) from file by exact filename</p> <p>This is similar to load_item, but loads the exact filename given by name instead of looking for the most recent file. name must contain the commit, timestamp, and file extension.</p> <p>A RuntimeError is raised if the file does not exist.</p> <p>To load an item that is an object from a library (e.g. a pandas DataFrame), the library must be installed (otherwise you will get a ModuleNotFound exception). However, you do not have to import the library before calling this function.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the item to load</p> required <code>save_fir</code> <p>Which folder to load the item from.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The data item loaded.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def load_exact_item(\n    name: str, save_dir: str = \"save_data\"\n) -&gt; Any:\n    \"\"\"Load a previously saved item (pickle) from file by exact filename\n\n    This is similar to load_item, but loads the exact filename given by name\n    instead of looking for the most recent file. name must contain the\n    commit, timestamp, and file extension.\n\n    A RuntimeError is raised if the file does not exist.\n\n    To load an item that is an object from a library (e.g. a pandas DataFrame),\n    the library must be installed (otherwise you will get a ModuleNotFound\n    exception). However, you do not have to import the library before calling this\n    function.\n\n    Args:\n        name: The name of the item to load\n        save_fir: Which folder to load the item from.\n\n    Returns:\n        The data item loaded. \n\n    \"\"\"\n\n    # Make the path to the file\n    file_path = Path(save_dir) / Path(name)\n\n    # If the file does not exist, raise an error\n    if not file_path.exists():\n        raise RuntimeError(f\"The file {name} does not exist in the directory {save_dir}\")\n\n    # Load a generic pickle. Note that if this is a pandas dataframe,\n    # pandas must be installed (otherwise you will get module not found).\n    # The same goes for a pickle storing an object from any other library.\n    with open(file_path, \"rb\") as file:\n        return pickle.load(file)\n</code></pre>"},{"location":"reference/#pyhbr.common.load_item","title":"<code>load_item(name, interactive=False, save_dir='save_data')</code>","text":"<p>Load a previously saved item (pickle) from file</p> <p>Use this function to load a file that was previously saved using save_item(). By default, the latest version of the item will be returned (the one with the most recent timestamp).</p> <p>None is returned if an interactive load is cancelled by the user.</p> <p>To load an item that is an object from a library (e.g. a pandas DataFrame), the library must be installed (otherwise you will get a ModuleNotFound exception). However, you do not have to import the library before calling this function.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the item to load</p> required <code>interactive</code> <code>bool</code> <p>If True, let the user pick which item version to load interactively. If False, non-interactively load the most recent item (i.e. with the most recent timestamp). The commit hash is not considered when loading the item.</p> <code>False</code> <code>save_fir</code> <p>Which folder to load the item from.</p> required <p>Returns:</p> Type Description <code>(Any, Path)</code> <p>A tuple, with the python object loaded from file as first element and the Path to the item as the second element, or None if the user cancelled an interactive load.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def load_item(\n    name: str, interactive: bool = False, save_dir: str = \"save_data\"\n) -&gt; (Any, Path):\n    \"\"\"Load a previously saved item (pickle) from file\n\n    Use this function to load a file that was previously saved using\n    save_item(). By default, the latest version of the item will be returned\n    (the one with the most recent timestamp).\n\n    None is returned if an interactive load is cancelled by the user.\n\n    To load an item that is an object from a library (e.g. a pandas DataFrame),\n    the library must be installed (otherwise you will get a ModuleNotFound\n    exception). However, you do not have to import the library before calling this\n    function.\n\n    Args:\n        name: The name of the item to load\n        interactive: If True, let the user pick which item version to load interactively.\n            If False, non-interactively load the most recent item (i.e. with the most\n            recent timestamp). The commit hash is not considered when loading the item.\n        save_fir: Which folder to load the item from.\n\n    Returns:\n        A tuple, with the python object loaded from file as first element and the\n            Path to the item as the second element, or None if the user cancelled\n            an interactive load.\n\n    \"\"\"\n    if interactive:\n        item_path = pick_saved_file_interactive(name, save_dir, \"pkl\")\n    else:\n        item_path = pick_most_recent_saved_file(name, save_dir, \"pkl\")\n\n    if item_path is None:\n        print(\"Aborted (interactive) load item\")\n        return None, None\n\n    print(f\"Loading {item_path}\")\n\n    # Load a generic pickle. Note that if this is a pandas dataframe,\n    # pandas must be installed (otherwise you will get module not found).\n    # The same goes for a pickle storing an object from any other library.\n    with open(item_path, \"rb\") as file:\n        return pickle.load(file), item_path\n</code></pre>"},{"location":"reference/#pyhbr.common.load_most_recent_data_files","title":"<code>load_most_recent_data_files(analysis_name, save_dir)</code>","text":"<p>Load the most recent timestamp data file matching the analysis name</p> <p>The data file is a pickle of a dictionary, containing pandas DataFrames and other metadata. It is expected to contain a \"raw_file\" key, which contains the path to the associated raw data file.</p> <p>Both files are loaded, and a tuple of all the data is returned</p> <p>Parameters:</p> Name Type Description Default <code>analysis_name</code> <code>str</code> <p>The \"analysis_name\" key from the config file, which is the filename prefix</p> required <code>save_dir</code> <code>str</code> <p>The folder to load the data from</p> required <p>Returns:</p> Type Description <code>(dict[str, Any], dict[str, Any], str)</code> <p>(data, raw_data, data_path). data and raw_data are dictionaries containing (mainly) Pandas DataFrames, and data_path is the path to the data file (this can be stored in any output products from this script to record which data file was used to generate the data.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def load_most_recent_data_files(analysis_name: str, save_dir: str) -&gt; (dict[str, Any], dict[str, Any], str):\n    \"\"\"Load the most recent timestamp data file matching the analysis name\n\n    The data file is a pickle of a dictionary, containing pandas DataFrames and\n    other metadata. It is expected to contain a \"raw_file\" key, which contains\n    the path to the associated raw data file.\n\n    Both files are loaded, and a tuple of all the data is returned\n\n    Args:\n        analysis_name: The \"analysis_name\" key from the config file, which is the filename prefix\n        save_dir: The folder to load the data from\n\n    Returns:\n        (data, raw_data, data_path). data and raw_data are dictionaries containing\n            (mainly) Pandas DataFrames, and data_path is the path to the data\n            file (this can be stored in any output products from this script to\n            record which data file was used to generate the data.\n    \"\"\"\n\n    item_name = f\"{analysis_name}_data\"\n    log.info(f\"Loading most recent data file '{item_name}'\")\n    data, data_path = load_item(item_name, save_dir=save_dir)\n\n    raw_file = data[\"raw_file\"]\n    log.info(f\"Loading the underlying raw data file '{raw_file}'\")\n    raw_data = load_exact_item(raw_file, save_dir=save_dir)\n\n    log.info(f\"Items in the data file {data.keys()}\")\n    log.info(f\"Items in the raw data file: {raw_data.keys()}\")\n\n    return data, raw_data, data_path\n</code></pre>"},{"location":"reference/#pyhbr.common.make_engine","title":"<code>make_engine(con_string='mssql+pyodbc://dsn', database='hic_cv_test')</code>","text":"<p>Make a sqlalchemy engine</p> <p>This function is intended for use with Microsoft SQL Server. The preferred method to connect to the server on Windows is to use a Data Source Name (DSN). To use the default connection string argument, set up a data source name called \"dsn\" using the program \"ODBC Data Sources\".</p> <p>If you need to access multiple different databases on the same server, you will need different engines. Specify the database name while creating the engine (this will override a default database in the DSN, if there is one).</p> <p>Parameters:</p> Name Type Description Default <code>con_string</code> <code>str</code> <p>The sqlalchemy connection string.</p> <code>'mssql+pyodbc://dsn'</code> <code>database</code> <code>str</code> <p>The database name to connect to.</p> <code>'hic_cv_test'</code> <p>Returns:</p> Type Description <code>Engine</code> <p>The sqlalchemy engine</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def make_engine(\n    con_string: str = \"mssql+pyodbc://dsn\", database: str = \"hic_cv_test\"\n) -&gt; Engine:\n    \"\"\"Make a sqlalchemy engine\n\n    This function is intended for use with Microsoft SQL\n    Server. The preferred method to connect to the server\n    on Windows is to use a Data Source Name (DSN). To use the\n    default connection string argument, set up a data source\n    name called \"dsn\" using the program \"ODBC Data Sources\".\n\n    If you need to access multiple different databases on the\n    same server, you will need different engines. Specify the\n    database name while creating the engine (this will override\n    a default database in the DSN, if there is one).\n\n    Args:\n        con_string: The sqlalchemy connection string.\n        database: The database name to connect to.\n\n    Returns:\n        The sqlalchemy engine\n    \"\"\"\n    connect_args = {\"database\": database}\n    return create_engine(con_string, connect_args=connect_args)\n</code></pre>"},{"location":"reference/#pyhbr.common.make_new_save_item_path","title":"<code>make_new_save_item_path(name, save_dir, extension)</code>","text":"<p>Make the path to save a new item to the save_dir</p> <p>The name will have the format name_{current_common}_{timestamp}.{extension}.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The base name for the new filename</p> required <code>save_dir</code> <code>str</code> <p>The folder in which to place the item</p> required <code>extension</code> <code>str</code> <p>The file extension (omit the dot)</p> required <p>Returns:</p> Type Description <code>Path</code> <p>The relative path to the new object to be saved</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def make_new_save_item_path(name: str, save_dir: str, extension: str) -&gt; Path:\n    \"\"\"Make the path to save a new item to the save_dir\n\n    The name will have the format name_{current_common}_{timestamp}.{extension}.\n\n    Args:\n        name: The base name for the new filename\n        save_dir: The folder in which to place the item\n        extension: The file extension (omit the dot)\n\n    Returns:\n        The relative path to the new object to be saved\n    \"\"\"\n\n    # Make the file suffix out of the current git\n    # commit hash and the current time\n    filename = f\"{name}_{current_commit()}_{current_timestamp()}.{extension}\"\n    return Path(save_dir) / Path(filename)\n</code></pre>"},{"location":"reference/#pyhbr.common.mean_confidence_interval","title":"<code>mean_confidence_interval(data, confidence=0.95)</code>","text":"<p>Compute the confidence interval around the mean</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Series</code> <p>A series of numerical values to compute the confidence interval.</p> required <code>confidence</code> <code>float</code> <p>The confidence interval to compute.</p> <code>0.95</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>A map containing the keys \"mean\", \"lower\", and \"upper\". The latter keys contain the confidence interval limits.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def mean_confidence_interval(\n    data: Series, confidence: float = 0.95\n) -&gt; dict[str, float]:\n    \"\"\"Compute the confidence interval around the mean\n\n    Args:\n        data: A series of numerical values to compute the confidence interval.\n        confidence: The confidence interval to compute.\n\n    Returns:\n        A map containing the keys \"mean\", \"lower\", and \"upper\". The latter\n            keys contain the confidence interval limits.\n    \"\"\"\n    a = 1.0 * np.array(data)\n    n = len(a)\n    mean = np.mean(a)\n    standard_error = scipy.stats.sem(a)\n\n    # Check this\n    half_width = standard_error * scipy.stats.t.ppf((1 + confidence) / 2.0, n - 1)\n    return {\n        \"mean\": mean,\n        \"confidence\": confidence,\n        \"lower\": mean - half_width,\n        \"upper\": mean + half_width,\n    }\n</code></pre>"},{"location":"reference/#pyhbr.common.median_to_string","title":"<code>median_to_string(instability, unit='%')</code>","text":"<p>Convert the median-quartile DataFrame to a String</p> <p>Parameters:</p> Name Type Description Default <code>instability</code> <code>DataFrame</code> <p>Table containing three rows, indexed by 0.5 (median), 0.25 (lower quartile) and 0.75 (upper quartile).</p> required <code>unit</code> <p>What units to add to the values in the string.</p> <code>'%'</code> <p>Returns:</p> Type Description <code>str</code> <p>A string containing the median, and the lower and upper quartiles.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def median_to_string(instability: DataFrame, unit=\"%\") -&gt; str:\n    \"\"\"Convert the median-quartile DataFrame to a String\n\n    Args:\n        instability: Table containing three rows, indexed by\n            0.5 (median), 0.25 (lower quartile) and 0.75\n            (upper quartile).\n        unit: What units to add to the values in the string.\n\n    Returns:\n        A string containing the median, and the lower and upper\n            quartiles.\n    \"\"\"\n    return f\"{instability.loc[0.5]:.2f}{unit} Q [{instability.loc[0.025]:.2f}{unit}, {instability.loc[0.975]:.2f}{unit}]\"\n</code></pre>"},{"location":"reference/#pyhbr.common.pick_most_recent_saved_file","title":"<code>pick_most_recent_saved_file(name, save_dir, extension='pkl')</code>","text":"<p>Get the path to the most recent file matching name.</p> <p>Like pick_saved_file_interactive, but automatically selects the most recent file in save_data.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the saved file to list</p> required <code>save_dir</code> <code>str</code> <p>The directory to search for files</p> required <code>extension</code> <code>str</code> <p>What file extension to look for. Do not include the dot.</p> <code>'pkl'</code> <p>Returns:</p> Type Description <code>Path</code> <p>The relative path to the most recent matching file.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def pick_most_recent_saved_file(\n    name: str, save_dir: str, extension: str = \"pkl\"\n) -&gt; Path:\n    \"\"\"Get the path to the most recent file matching name.\n\n    Like pick_saved_file_interactive, but automatically selects the most\n    recent file in save_data.\n\n    Args:\n        name: The name of the saved file to list\n        save_dir: The directory to search for files\n        extension: What file extension to look for. Do not include the dot.\n\n    Returns:\n        The relative path to the most recent matching file.\n    \"\"\"\n    recent_first = get_saved_files_by_name(name, save_dir, extension)\n    return Path(save_dir) / Path(recent_first.loc[0, \"path\"])\n</code></pre>"},{"location":"reference/#pyhbr.common.pick_saved_file_interactive","title":"<code>pick_saved_file_interactive(name, save_dir, extension='pkl')</code>","text":"<p>Select a file matching name interactively</p> <p>Print a list of the saved items in the save_dir folder, along with the date and time it was generated, and the commit hash, and let the user pick which item should be loaded interactively. The full filename of the resulting file is returned, which can then be read by the user.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the saved file to list</p> required <code>save_dir</code> <code>str</code> <p>The directory to search for files</p> required <code>extension</code> <code>str</code> <p>What file extension to look for. Do not include the dot.</p> <code>'pkl'</code> <p>Returns:</p> Type Description <code>str | None</code> <p>The absolute path to the interactively selected file, or None if the interactive load was aborted.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def pick_saved_file_interactive(\n    name: str, save_dir: str, extension: str = \"pkl\"\n) -&gt; str | None:\n    \"\"\"Select a file matching name interactively\n\n    Print a list of the saved items in the save_dir folder, along\n    with the date and time it was generated, and the commit hash,\n    and let the user pick which item should be loaded interactively.\n    The full filename of the resulting file is returned, which can\n    then be read by the user.\n\n    Args:\n        name: The name of the saved file to list\n        save_dir: The directory to search for files\n        extension: What file extension to look for. Do not include the dot.\n\n    Returns:\n        The absolute path to the interactively selected file, or None\n            if the interactive load was aborted.\n    \"\"\"\n\n    recent_first = get_saved_files_by_name(name, save_dir, extension)\n    print(recent_first)\n\n    num_datasets = recent_first.shape[0]\n    while True:\n        try:\n            raw_choice = input(\n                f\"Pick a dataset to load: [{0} - {num_datasets-1}] (type q[uit]/exit, then Enter, to quit): \"\n            )\n            if \"exit\" in raw_choice or \"q\" in raw_choice:\n                return None\n            choice = int(raw_choice)\n        except Exception:\n            print(f\"{raw_choice} is not valid; try again.\")\n            continue\n        if choice &lt; 0 or choice &gt;= num_datasets:\n            print(f\"{choice} is not in range; try again.\")\n            continue\n        break\n\n    full_path = os.path.join(save_dir, recent_first.loc[choice, \"path\"])\n    return full_path\n</code></pre>"},{"location":"reference/#pyhbr.common.query_yes_no","title":"<code>query_yes_no(question, default='yes')</code>","text":"<p>Ask a yes/no question via raw_input() and return their answer.</p> <p>From https://stackoverflow.com/a/3041990.</p> <p>\"question\" is a string that is presented to the user. \"default\" is the presumed answer if the user just hits .         It must be \"yes\" (the default), \"no\" or None (meaning         an answer is required of the user). <p>The \"answer\" return value is True for \"yes\" or False for \"no\".</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def query_yes_no(question, default=\"yes\"):\n    \"\"\"Ask a yes/no question via raw_input() and return their answer.\n\n    From https://stackoverflow.com/a/3041990.\n\n    \"question\" is a string that is presented to the user.\n    \"default\" is the presumed answer if the user just hits &lt;Enter&gt;.\n            It must be \"yes\" (the default), \"no\" or None (meaning\n            an answer is required of the user).\n\n    The \"answer\" return value is True for \"yes\" or False for \"no\".\n    \"\"\"\n    valid = {\"yes\": True, \"y\": True, \"ye\": True, \"no\": False, \"n\": False}\n    if default is None:\n        prompt = \" [y/n] \"\n    elif default == \"yes\":\n        prompt = \" [Y/n] \"\n    elif default == \"no\":\n        prompt = \" [y/N] \"\n    else:\n        raise ValueError(\"invalid default answer: '%s'\" % default)\n\n    while True:\n        sys.stdout.write(question + prompt)\n        choice = input().lower()\n        if default is not None and choice == \"\":\n            return valid[default]\n        elif choice in valid:\n            return valid[choice]\n        else:\n            sys.stdout.write(\"Please respond with 'yes' or 'no' \" \"(or 'y' or 'n').\\n\")\n</code></pre>"},{"location":"reference/#pyhbr.common.read_config_file","title":"<code>read_config_file(yaml_path)</code>","text":"<p>Read the configuration file from</p> <p>Parameters:</p> Name Type Description Default <code>yaml_path</code> <code>str</code> <p>The path to the experiment config file</p> required Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def read_config_file(yaml_path: str):\n    \"\"\"Read the configuration file from\n\n    Args:\n        yaml_path: The path to the experiment config file\n    \"\"\"\n    # Read the configuration file\n    with open(yaml_path) as stream:\n        try:\n            return yaml.safe_load(stream)\n        except yaml.YAMLError as exc:\n            print(f\"Failed to load config file: {exc}\")\n            exit(1)\n</code></pre>"},{"location":"reference/#pyhbr.common.requires_commit","title":"<code>requires_commit()</code>","text":"<p>Check whether changes need committing</p> <p>To make most effective use of the commit hash stored with a save_item call, the current branch should be clean (all changes committed). Call this function to check.</p> <p>Returns False if there is no git repository.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the working directory is in a git repository that requires a commit; False otherwise.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def requires_commit() -&gt; bool:\n    \"\"\"Check whether changes need committing\n\n    To make most effective use of the commit hash stored with a\n    save_item call, the current branch should be clean (all changes\n    committed). Call this function to check.\n\n    Returns False if there is no git repository.\n\n    Returns:\n        True if the working directory is in a git repository that requires\n            a commit; False otherwise.\n    \"\"\"\n    try:\n        repo = Repo(search_parent_directories=True)\n        return repo.is_dirty(untracked_files=True)\n    except InvalidGitRepositoryError:\n        # No need to commit if not repository\n        return False\n</code></pre>"},{"location":"reference/#pyhbr.common.save_item","title":"<code>save_item(item, name, save_dir='save_data/', enforce_clean_branch=True, prompt_commit=False)</code>","text":"<p>Save an item to a pickle file</p> <p>Saves a python object (e.g. a pandas DataFrame) dataframe in the save_dir folder, using a filename that includes the current timestamp and the current commit hash. Use load_item to retrieve the file.</p> <p>Important</p> <p>Ensure that <code>save_data/</code> (or your chosen <code>save_dir</code>) is added to the .gitignore of your repository to ensure sensitive data is not committed.</p> <p>By storing the commit hash and timestamp, it is possible to identify when items were created and what code created them. To make most effective use of the commit hash, ensure that you commit, and do not make any further code edits, before running a script that calls save_item (otherwise the commit hash will not quite reflect the state of the running code).</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Any</code> <p>The python object to save (e.g. pandas DataFrame)</p> required <code>name</code> <code>str</code> <p>The name of the item. The filename will be created by adding a suffix for the current commit and the timestamp to show when the data was saved (format: <code>name_commit_timestamp.pkl</code>)</p> required <code>save_dir</code> <code>str</code> <p>Where to save the data, relative to the current working directory. The directory will be created if it does not exist.</p> <code>'save_data/'</code> <code>enforce_clean_branch</code> <p>If True, the function will raise an exception if an attempt is made to save an item when the repository has uncommitted changes.</p> <code>True</code> <code>prompt_commit</code> <p>if enforce_clean_branch is true, choose whether the prompt the user to commit on an unclean branch. This can help avoiding losing the results of a long-running script. Prefer to use false if the script is cheap to run.</p> <code>False</code> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def save_item(\n    item: Any,\n    name: str,\n    save_dir: str = \"save_data/\",\n    enforce_clean_branch=True,\n    prompt_commit=False,\n) -&gt; None:\n    \"\"\"Save an item to a pickle file\n\n    Saves a python object (e.g. a pandas DataFrame) dataframe in the save_dir\n    folder, using a filename that includes the current timestamp and the current\n    commit hash. Use load_item to retrieve the file.\n\n    !!! important\n        Ensure that `save_data/` (or your chosen `save_dir`) is added to the\n        .gitignore of your repository to ensure sensitive data is not committed.\n\n    By storing the commit hash and timestamp, it is possible to identify when items\n    were created and what code created them. To make most effective use of the\n    commit hash, ensure that you commit, and do not make any further code edits,\n    before running a script that calls save_item (otherwise the commit hash will\n    not quite reflect the state of the running code).\n\n    Args:\n        item: The python object to save (e.g. pandas DataFrame)\n        name: The name of the item. The filename will be created by adding\n            a suffix for the current commit and the timestamp to show when the\n            data was saved (format: `name_commit_timestamp.pkl`)\n        save_dir: Where to save the data, relative to the current working directory.\n            The directory will be created if it does not exist.\n        enforce_clean_branch: If True, the function will raise an exception if an attempt\n            is made to save an item when the repository has uncommitted changes.\n        prompt_commit: if enforce_clean_branch is true, choose whether the prompt the\n            user to commit on an unclean branch. This can help avoiding losing\n            the results of a long-running script. Prefer to use false if the script\n            is cheap to run.\n    \"\"\"\n\n    if enforce_clean_branch:\n\n        abort_msg = \"Aborting save_item() because branch is not clean. Commit your changes before saving item to increase the chance of reproducing the item based on the filename commit hash.\"\n\n        if prompt_commit:\n            # If the branch is not clean, prompt the user to commit to avoid losing\n            # long-running model results. Take care to only commit if the state of\n            # the repository truly reflects what was run (i.e. if no changes were made\n            # while the script was running).\n            while requires_commit():\n                print(abort_msg)\n                print(\n                    \"You can commit now and then retry the save after committing.\"\n                )\n                retry_save = query_yes_no(\n                    \"Do you want to retry the save? Commit, then select yes, or choose no to abort the save.\"\n                )\n\n                if not retry_save:\n                    print(f\"Aborting save of {name}\")\n                    return\n\n            # If we get out the loop without returning, then the branch\n            # is not clean and the save can proceed.\n            print(\"Branch now clean, proceeding to save\")\n\n        else:\n\n            if requires_commit():\n                # In this case, unconditionally throw an error\n                raise RuntimeError(abort_msg)\n\n    if not Path(save_dir).exists():\n        print(f\"Creating missing folder '{save_dir}' for storing item\")\n        Path(save_dir).mkdir(parents=True, exist_ok=True)\n\n    path = make_new_save_item_path(name, save_dir, \"pkl\")\n    with open(path, \"wb\") as file:\n        print(f\"Saving {str(path)}\")\n        pickle.dump(item, file)\n</code></pre>"},{"location":"reference/#pyhbr.data_source","title":"<code>data_source</code>","text":"<p>Routines for fetching data from sources.</p> <p>This module is intended to interface to the data source, and should be modified to port this package to new SQL databases.</p>"},{"location":"reference/#pyhbr.data_source.hic","title":"<code>hic</code>","text":"<p>SQL queries and functions for HIC (v3, UHBW) data.</p> <p>Most data available in the HIC tables is fetched in the  queries below, apart from columns which are all-NULL, provide keys/IDs that will not be used, or provide duplicate information (e.g. duplicated in two tables).</p>"},{"location":"reference/#pyhbr.data_source.hic.demographics_query","title":"<code>demographics_query(engine)</code>","text":"<p>Get demographic information from HIC data</p> <p>The date/time at which the data was obtained is not stored in the table, but patient age can be computed from the date of the episode under consideration and the year_of_birth in this table.</p> <p>The underlying table does have a cause_of_death column, but it is all null, so not included.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>the connection to the database</p> required <p>Returns:</p> Type Description <code>Select</code> <p>SQL query to retrieve episodes table</p> Source code in <code>src\\pyhbr\\data_source\\hic.py</code> <pre><code>def demographics_query(engine: Engine) -&gt; Select:\n    \"\"\"Get demographic information from HIC data\n\n    The date/time at which the data was obtained is\n    not stored in the table, but patient age can be\n    computed from the date of the episode under consideration\n    and the year_of_birth in this table.\n\n    The underlying table does have a cause_of_death column,\n    but it is all null, so not included.\n\n    Args:\n        engine: the connection to the database\n\n    Returns:\n        SQL query to retrieve episodes table\n    \"\"\"\n    table = CheckedTable(\"cv1_demographics\", engine)\n    return select(\n        table.col(\"subject\").cast(String).label(\"patient_id\"),\n        table.col(\"gender\"),\n        table.col(\"year_of_birth\"),\n        table.col(\"death_date\"),\n    )\n</code></pre>"},{"location":"reference/#pyhbr.data_source.hic.diagnoses_query","title":"<code>diagnoses_query(engine)</code>","text":"<p>Get the diagnoses corresponding to episodes</p> <p>This should be linked to the episodes table to obtain information about the diagnoses in the episode.</p> <p>Diagnoses are encoded using ICD-10 codes, and the position column contains the order of diagnoses in the episode (1-indexed).</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>the connection to the database</p> required <p>Returns:</p> Type Description <code>Select</code> <p>SQL query to retrieve diagnoses table</p> Source code in <code>src\\pyhbr\\data_source\\hic.py</code> <pre><code>def diagnoses_query(engine: Engine) -&gt; Select:\n    \"\"\"Get the diagnoses corresponding to episodes\n\n    This should be linked to the episodes table to\n    obtain information about the diagnoses in the episode.\n\n    Diagnoses are encoded using ICD-10 codes, and the\n    position column contains the order of diagnoses in\n    the episode (1-indexed).\n\n    Args:\n        engine: the connection to the database\n\n    Returns:\n        SQL query to retrieve diagnoses table\n    \"\"\"\n    table = CheckedTable(\"cv1_episodes_diagnosis\", engine)\n    return select(\n        table.col(\"episode_identifier\").cast(String).label(\"episode_id\"),\n        table.col(\"diagnosis_date_time\").label(\"time\"),\n        table.col(\"diagnosis_position\").label(\"position\"),\n        table.col(\"diagnosis_code_icd\").label(\"code\"),\n    )\n</code></pre>"},{"location":"reference/#pyhbr.data_source.hic.episodes_query","title":"<code>episodes_query(engine, start_date, end_date)</code>","text":"<p>Get the episodes list in the HIC data</p> <p>This table does not contain any episode information, just a patient and an episode id for linking to diagnosis and procedure information in other tables.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>the connection to the database</p> required <code>start_date</code> <code>date</code> <p>first valid consultant-episode start date</p> required <code>end_date</code> <code>date</code> <p>last valid consultant-episode start date</p> required <p>Returns:</p> Type Description <code>Select</code> <p>SQL query to retrieve episodes table</p> Source code in <code>src\\pyhbr\\data_source\\hic.py</code> <pre><code>def episodes_query(engine: Engine, start_date: date, end_date: date) -&gt; Select:\n    \"\"\"Get the episodes list in the HIC data\n\n    This table does not contain any episode information,\n    just a patient and an episode id for linking to diagnosis\n    and procedure information in other tables.\n\n    Args:\n        engine: the connection to the database\n        start_date: first valid consultant-episode start date\n        end_date: last valid consultant-episode start date\n\n    Returns:\n        SQL query to retrieve episodes table\n    \"\"\"\n    table = CheckedTable(\"cv1_episodes\", engine)\n    return select(\n        table.col(\"subject\").cast(String).label(\"patient_id\"),\n        table.col(\"episode_identifier\").cast(String).label(\"episode_id\"),\n        table.col(\"spell_identifier\").cast(String).label(\"spell_id\"),\n        table.col(\"episode_start_time\").label(\"episode_start\"),\n        table.col(\"episode_end_time\").label(\"episode_end\"),\n        table.col(\"admission_date_time\").label(\"admission\"),\n        table.col(\"discharge_date_time\").label(\"discharge\"),\n    ).where(\n        table.col(\"episode_start_time\") &gt;= start_date,\n        table.col(\"episode_end_time\") &lt;= end_date,\n    )\n</code></pre>"},{"location":"reference/#pyhbr.data_source.hic.pathology_blood_query","title":"<code>pathology_blood_query(engine, investigations)</code>","text":"<p>Get the table of blood test results in the HIC data</p> <p>Since blood tests in this table are not associated with an episode directly by key, it is necessary to link them based on the patient identifier and date. This operation can be quite slow if the blood tests table is large. One way to reduce the size is to filter by investigation using the investigations parameter. The investigation codes in the HIC data are shown below:</p> <code>investigation</code> Description OBR_BLS_UL LFT OBR_BLS_UE UREA,CREAT + ELECTROLYTES OBR_BLS_FB FULL BLOOD COUNT OBR_BLS_UT THYROID FUNCTION TEST OBR_BLS_TP TOTAL PROTEIN OBR_BLS_CR C-REACTIVE PROTEIN OBR_BLS_CS CLOTTING SCREEN OBR_BLS_FI FIB-4 OBR_BLS_AS AST OBR_BLS_CA CALCIUM GROUP OBR_BLS_TS TSH AND FT4 OBR_BLS_FO SERUM FOLATE OBR_BLS_PO PHOSPHATE OBR_BLS_LI LIPID PROFILE OBR_POC_VG POCT BLOOD GAS VENOUS SAMPLE OBR_BLS_HD HDL CHOLESTEROL OBR_BLS_FT FREE T4 OBR_BLS_FE SERUM FERRITIN OBR_BLS_GP ELECTROLYTES NO POTASSIUM OBR_BLS_CH CHOLESTEROL OBR_BLS_MG MAGNESIUM OBR_BLS_CO CORTISOL <p>Each test is similarly encoded. The valid test codes in the full blood count and U+E investigations are shown below:</p> <code>investigation</code> <code>test</code> Description OBR_BLS_FB OBX_BLS_NE Neutrophils OBR_BLS_FB OBX_BLS_PL Platelets OBR_BLS_FB OBX_BLS_WB White Cell Count OBR_BLS_FB OBX_BLS_LY Lymphocytes OBR_BLS_FB OBX_BLS_MC MCV OBR_BLS_FB OBX_BLS_HB Haemoglobin OBR_BLS_FB OBX_BLS_HC Haematocrit OBR_BLS_UE OBX_BLS_NA Sodium OBR_BLS_UE OBX_BLS_UR Urea OBR_BLS_UE OBX_BLS_K Potassium OBR_BLS_UE OBX_BLS_CR Creatinine OBR_BLS_UE OBX_BLS_EP eGFR/1.73m2 (CKD-EPI) <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>the connection to the database</p> required <code>investigations</code> <code>list[str]</code> <p>Which types of laboratory test to include in the query. Fetching fewer types of test makes the query faster.</p> required <p>Returns:</p> Type Description <code>Engine</code> <p>SQL query to retrieve blood tests table</p> Source code in <code>src\\pyhbr\\data_source\\hic.py</code> <pre><code>def pathology_blood_query(engine: Engine, investigations: list[str]) -&gt; Engine:\n    \"\"\"Get the table of blood test results in the HIC data\n\n    Since blood tests in this table are not associated with an episode\n    directly by key, it is necessary to link them based on the patient\n    identifier and date. This operation can be quite slow if the blood\n    tests table is large. One way to reduce the size is to filter by\n    investigation using the investigations parameter. The investigation\n    codes in the HIC data are shown below:\n\n    | `investigation` | Description                 |\n    |-----------------|-----------------------------|\n    | OBR_BLS_UL      |                          LFT|\n    | OBR_BLS_UE      |    UREA,CREAT + ELECTROLYTES|\n    | OBR_BLS_FB      |             FULL BLOOD COUNT|\n    | OBR_BLS_UT      |        THYROID FUNCTION TEST|\n    | OBR_BLS_TP      |                TOTAL PROTEIN|\n    | OBR_BLS_CR      |           C-REACTIVE PROTEIN|\n    | OBR_BLS_CS      |              CLOTTING SCREEN|\n    | OBR_BLS_FI      |                        FIB-4|\n    | OBR_BLS_AS      |                          AST|\n    | OBR_BLS_CA      |                CALCIUM GROUP|\n    | OBR_BLS_TS      |                  TSH AND FT4|\n    | OBR_BLS_FO      |                SERUM FOLATE|\n    | OBR_BLS_PO      |                    PHOSPHATE|\n    | OBR_BLS_LI      |                LIPID PROFILE|\n    | OBR_POC_VG      | POCT BLOOD GAS VENOUS SAMPLE|\n    | OBR_BLS_HD      |              HDL CHOLESTEROL|\n    | OBR_BLS_FT      |                      FREE T4|\n    | OBR_BLS_FE      |               SERUM FERRITIN|\n    | OBR_BLS_GP      |    ELECTROLYTES NO POTASSIUM|\n    | OBR_BLS_CH      |                  CHOLESTEROL|\n    | OBR_BLS_MG      |                    MAGNESIUM|\n    | OBR_BLS_CO      |                     CORTISOL|\n\n    Each test is similarly encoded. The valid test codes in the full\n    blood count and U+E investigations are shown below:\n\n    | `investigation` | `test`     | Description          |\n    |-----------------|------------|----------------------|\n    | OBR_BLS_FB      | OBX_BLS_NE |           Neutrophils|\n    | OBR_BLS_FB      | OBX_BLS_PL |             Platelets|\n    | OBR_BLS_FB      | OBX_BLS_WB |      White Cell Count|\n    | OBR_BLS_FB      | OBX_BLS_LY |           Lymphocytes|\n    | OBR_BLS_FB      | OBX_BLS_MC |                   MCV|\n    | OBR_BLS_FB      | OBX_BLS_HB |           Haemoglobin|\n    | OBR_BLS_FB      | OBX_BLS_HC |           Haematocrit|\n    | OBR_BLS_UE      | OBX_BLS_NA |                Sodium|\n    | OBR_BLS_UE      | OBX_BLS_UR |                  Urea|\n    | OBR_BLS_UE      | OBX_BLS_K  |             Potassium|\n    | OBR_BLS_UE      | OBX_BLS_CR |            Creatinine|\n    | OBR_BLS_UE      | OBX_BLS_EP | eGFR/1.73m2 (CKD-EPI)|\n\n    Args:\n        engine: the connection to the database\n        investigations: Which types of laboratory\n            test to include in the query. Fetching fewer types of\n            test makes the query faster.\n\n    Returns:\n        SQL query to retrieve blood tests table\n    \"\"\"\n\n    table = CheckedTable(\"cv1_pathology_blood\", engine)\n    return select(\n        table.col(\"subject\").cast(String).label(\"patient_id\"),\n        table.col(\"investigation_code\").label(\"investigation\"),\n        table.col(\"test_code\").label(\"test\"),\n        table.col(\"test_result\").label(\"result\"),\n        table.col(\"test_result_unit\").label(\"unit\"),\n        table.col(\"sample_collected_date_time\").label(\"sample_date\"),\n        table.col(\"result_available_date_time\").label(\"result_date\"),\n        table.col(\"result_flag\"),\n        table.col(\"result_lower_range\"),\n        table.col(\"result_upper_range\"),\n    ).where(table.col(\"investigation_code\").in_(investigations))\n</code></pre>"},{"location":"reference/#pyhbr.data_source.hic.pharmacy_prescribing_query","title":"<code>pharmacy_prescribing_query(engine, table_name='cv1_pharmacy_prescribing')</code>","text":"<p>Get medicines prescribed to patients over time</p> <p>This table contains information about medicines  prescribed to patients, identified by patient and time (i.e. it is not associated to an episode). The information includes the medicine name, dose (includes unit), frequency,  form (e.g. tablets), route (e.g. oral), and whether the medicine was present on admission.</p> <p>The most commonly occurring formats for various relevant medicines are shown in the table below:</p> <code>name</code> <code>dose</code> <code>frequency</code> <code>drug_form</code> <code>route</code> aspirin 75 mg in the MORNING NaN Oral aspirin 75 mg in the MORNING dispersible tablet Oral clopidogrel 75 mg in the MORNING film coated tablets Oral ticagrelor 90 mg TWICE a day tablets Oral warfarin 3 mg ONCE a day  at 18:00 NaN Oral warfarin 5 mg ONCE a day  at 18:00 tablets Oral apixaban 5 mg TWICE a day tablets Oral dabigatran etexilate 110 mg TWICE a day capsules Oral edoxaban 60 mg in the MORNING tablets Oral rivaroxaban 20 mg in the MORNING film coated tablets Oral <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>the connection to the database</p> required <code>table_name</code> <code>str</code> <p>This defaults to \"cv1_pharmacy_prescribing\" for UHBW, but can be overwritten with \"HIC_Pharmacy\" for ICB.</p> <code>'cv1_pharmacy_prescribing'</code> <p>Returns:</p> Type Description <code>Select</code> <p>SQL query to retrieve procedures table</p> Source code in <code>src\\pyhbr\\data_source\\hic.py</code> <pre><code>def pharmacy_prescribing_query(engine: Engine, table_name: str = \"cv1_pharmacy_prescribing\") -&gt; Select:\n    \"\"\"Get medicines prescribed to patients over time\n\n    This table contains information about medicines \n    prescribed to patients, identified by patient and time\n    (i.e. it is not associated to an episode). The information\n    includes the medicine name, dose (includes unit), frequency, \n    form (e.g. tablets), route (e.g. oral), and whether the\n    medicine was present on admission.\n\n    The most commonly occurring formats for various relevant\n    medicines are shown in the table below:\n\n    | `name`       | `dose`  | `frequency`    | `drug_form`         | `route` |\n    |--------------|---------|----------------|---------------------|---------|\n    | aspirin      | 75 mg   | in the MORNING | NaN                 | Oral    |\n    | aspirin      | 75 mg   | in the MORNING | dispersible tablet  | Oral    |\n    | clopidogrel  | 75 mg   | in the MORNING | film coated tablets | Oral    |\n    | ticagrelor   | 90 mg   | TWICE a day    | tablets             | Oral    |\n    | warfarin     | 3 mg    | ONCE a day  at 18:00 | NaN           | Oral    |\n    | warfarin     | 5 mg    | ONCE a day  at 18:00 | tablets       | Oral    |       \n    | apixaban     | 5 mg    | TWICE a day          | tablets       | Oral    |\n    | dabigatran etexilate | 110 mg | TWICE a day   | capsules      | Oral    |\n    | edoxaban     | 60 mg   | in the MORNING       | tablets       | Oral    |\n    | rivaroxaban  | 20 mg   | in the MORNING | film coated tablets | Oral    |\n\n    Args:\n        engine: the connection to the database\n        table_name: This defaults to \"cv1_pharmacy_prescribing\" for UHBW,\n            but can be overwritten with \"HIC_Pharmacy\" for ICB.\n\n    Returns:\n        SQL query to retrieve procedures table\n    \"\"\"\n\n    # This field name depends on UHBW vs. ICB.\n    if table_name == \"cv1_pharmacy_prescribing\":\n        patient_id_field = \"subject\"\n    else:\n        patient_id_field = \"nhs_number\"\n\n    table = CheckedTable(table_name, engine)\n    return select(\n        table.col(patient_id_field).cast(String).label(\"patient_id\"),\n        table.col(\"order_date_time\").label(\"order_date\"),\n        table.col(\"medication_name\").label(\"name\"),\n        table.col(\"ordered_dose\").label(\"dose\"),\n        table.col(\"ordered_frequency\").label(\"frequency\"),\n        table.col(\"ordered_drug_form\").label(\"drug_form\"),\n        table.col(\"ordered_route\").label(\"route\"),\n        table.col(\"admission_medicine_y_n\").label(\"on_admission\"),\n    )\n</code></pre>"},{"location":"reference/#pyhbr.data_source.hic.procedures_query","title":"<code>procedures_query(engine)</code>","text":"<p>Get the procedures corresponding to episodes</p> <p>This should be linked to the episodes table to obtain information about the procedures in the episode.</p> <p>Procedures are encoded using OPCS-4 codes, and the position column contains the order of procedures in the episode (1-indexed).</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>the connection to the database</p> required <p>Returns:</p> Type Description <code>Select</code> <p>SQL query to retrieve procedures table</p> Source code in <code>src\\pyhbr\\data_source\\hic.py</code> <pre><code>def procedures_query(engine: Engine) -&gt; Select:\n    \"\"\"Get the procedures corresponding to episodes\n\n    This should be linked to the episodes table to\n    obtain information about the procedures in the episode.\n\n    Procedures are encoded using OPCS-4 codes, and the\n    position column contains the order of procedures in\n    the episode (1-indexed).\n\n    Args:\n        engine: the connection to the database\n\n    Returns:\n        SQL query to retrieve procedures table\n    \"\"\"\n    table = CheckedTable(\"cv1_episodes_procedures\", engine)\n    return select(\n        table.col(\"episode_identifier\").cast(String).label(\"episode_id\"),\n        table.col(\"procedure_date_time\").label(\"time\"),\n        table.col(\"procedure_position\").label(\"position\"),\n        table.col(\"procedure_code_opcs\").label(\"code\"),\n    )\n</code></pre>"},{"location":"reference/#pyhbr.data_source.hic_covid","title":"<code>hic_covid</code>","text":"<p>SQL queries and functions for HIC (COVID-19, UHBW) data.</p>"},{"location":"reference/#pyhbr.data_source.hic_covid.episodes_query","title":"<code>episodes_query(engine)</code>","text":"<p>Get the episodes list in the HIC data</p> <p>This table does not contain any episode information, just a patient and an episode id for linking to diagnosis and procedure information in other tables.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>the connection to the database</p> required <code>start_date</code> <p>first valid consultant-episode start date</p> required <code>end_date</code> <p>last valid consultant-episode start date</p> required <p>Returns:</p> Type Description <code>Select</code> <p>SQL query to retrieve episodes table</p> Source code in <code>src\\pyhbr\\data_source\\hic_covid.py</code> <pre><code>def episodes_query(engine: Engine) -&gt; Select:\n    \"\"\"Get the episodes list in the HIC data\n\n    This table does not contain any episode information,\n    just a patient and an episode id for linking to diagnosis\n    and procedure information in other tables.\n\n    Args:\n        engine: the connection to the database\n        start_date: first valid consultant-episode start date\n        end_date: last valid consultant-episode start date\n\n    Returns:\n        SQL query to retrieve episodes table\n    \"\"\"\n    table = CheckedTable(\"cv_covid_episodes\", engine)\n    return select(\n        table.col(\"NHS_NUMBER\").cast(String).label(\"nhs_number\"),\n        table.col(\"Other Number\").cast(String).label(\"t_number\"),\n        table.col(\"episode_identifier\").cast(String).label(\"episode_id\"),\n    )\n</code></pre>"},{"location":"reference/#pyhbr.data_source.hic_icb","title":"<code>hic_icb</code>","text":"<p>SQL queries and functions for HIC (ICB version)</p> <p>Most data available in the HIC tables is fetched in the  queries below, apart from columns which are all-NULL, provide keys/IDs that will not be used, or provide duplicate information (e.g. duplicated in two tables). </p> <p>Note that the lab results/pharmacy queries are in the hic.py module, because there are no changes to the query apart from the table name.</p>"},{"location":"reference/#pyhbr.data_source.hic_icb.episode_id_query","title":"<code>episode_id_query(engine)</code>","text":"<p>Get the episodes list in the HIC data</p> <p>This table is just a list of IDs to identify the data in other ICB tables.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>the connection to the database</p> required <p>Returns:</p> Type Description <code>Select</code> <p>SQL query to retrieve episodes table</p> Source code in <code>src\\pyhbr\\data_source\\hic_icb.py</code> <pre><code>def episode_id_query(engine: Engine) -&gt; Select:\n    \"\"\"Get the episodes list in the HIC data\n\n    This table is just a list of IDs to identify the data in other ICB tables.\n\n    Args:\n        engine: the connection to the database\n\n    Returns:\n        SQL query to retrieve episodes table\n    \"\"\"\n    table = CheckedTable(\"hic_episodes\", engine)\n    return select(\n        table.col(\"nhs_number\").cast(String).label(\"patient_id\"),\n        table.col(\"episode_identified\").cast(String).label(\"episode_id\"),\n    )\n</code></pre>"},{"location":"reference/#pyhbr.data_source.hic_icb.pathology_blood_query","title":"<code>pathology_blood_query(engine, test_names)</code>","text":"<p>Get the table of blood test results in the HIC data</p> <p>Since blood tests in this table are not associated with an episode directly by key, it is necessary to link them based on the patient identifier and date. This operation can be quite slow if the blood tests table is large. One way to reduce the size is to filter by investigation using the investigations parameter. The investigation codes in the HIC data are shown below:</p> <code>investigation</code> Description OBR_BLS_UL LFT OBR_BLS_UE UREA,CREAT + ELECTROLYTES OBR_BLS_FB FULL BLOOD COUNT OBR_BLS_UT THYROID FUNCTION TEST OBR_BLS_TP TOTAL PROTEIN OBR_BLS_CR C-REACTIVE PROTEIN OBR_BLS_CS CLOTTING SCREEN OBR_BLS_FI FIB-4 OBR_BLS_AS AST OBR_BLS_CA CALCIUM GROUP OBR_BLS_TS TSH AND FT4 OBR_BLS_FO SERUM FOLATE OBR_BLS_PO PHOSPHATE OBR_BLS_LI LIPID PROFILE OBR_POC_VG POCT BLOOD GAS VENOUS SAMPLE OBR_BLS_HD HDL CHOLESTEROL OBR_BLS_FT FREE T4 OBR_BLS_FE SERUM FERRITIN OBR_BLS_GP ELECTROLYTES NO POTASSIUM OBR_BLS_CH CHOLESTEROL OBR_BLS_MG MAGNESIUM OBR_BLS_CO CORTISOL <p>Each test is similarly encoded. The valid test codes in the full blood count and U+E investigations are shown below:</p> <code>investigation</code> <code>test</code> Description OBR_BLS_FB OBX_BLS_NE Neutrophils OBR_BLS_FB OBX_BLS_PL Platelets OBR_BLS_FB OBX_BLS_WB White Cell Count OBR_BLS_FB OBX_BLS_LY Lymphocytes OBR_BLS_FB OBX_BLS_MC MCV OBR_BLS_FB OBX_BLS_HB Haemoglobin OBR_BLS_FB OBX_BLS_HC Haematocrit OBR_BLS_UE OBX_BLS_NA Sodium OBR_BLS_UE OBX_BLS_UR Urea OBR_BLS_UE OBX_BLS_K Potassium OBR_BLS_UE OBX_BLS_CR Creatinine OBR_BLS_UE OBX_BLS_EP eGFR/1.73m2 (CKD-EPI) <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>the connection to the database</p> required <code>test_names</code> <code>list[str]</code> <p>Unlike the UHBW version of this table, there are no investigation names here. Instead, restrict directly using the test_name field.</p> required <p>Returns:</p> Type Description <code>Engine</code> <p>SQL query to retrieve blood tests table</p> Source code in <code>src\\pyhbr\\data_source\\hic_icb.py</code> <pre><code>def pathology_blood_query(engine: Engine, test_names: list[str]) -&gt; Engine:\n    \"\"\"Get the table of blood test results in the HIC data\n\n    Since blood tests in this table are not associated with an episode\n    directly by key, it is necessary to link them based on the patient\n    identifier and date. This operation can be quite slow if the blood\n    tests table is large. One way to reduce the size is to filter by\n    investigation using the investigations parameter. The investigation\n    codes in the HIC data are shown below:\n\n    | `investigation` | Description                 |\n    |-----------------|-----------------------------|\n    | OBR_BLS_UL      |                          LFT|\n    | OBR_BLS_UE      |    UREA,CREAT + ELECTROLYTES|\n    | OBR_BLS_FB      |             FULL BLOOD COUNT|\n    | OBR_BLS_UT      |        THYROID FUNCTION TEST|\n    | OBR_BLS_TP      |                TOTAL PROTEIN|\n    | OBR_BLS_CR      |           C-REACTIVE PROTEIN|\n    | OBR_BLS_CS      |              CLOTTING SCREEN|\n    | OBR_BLS_FI      |                        FIB-4|\n    | OBR_BLS_AS      |                          AST|\n    | OBR_BLS_CA      |                CALCIUM GROUP|\n    | OBR_BLS_TS      |                  TSH AND FT4|\n    | OBR_BLS_FO      |                SERUM FOLATE|\n    | OBR_BLS_PO      |                    PHOSPHATE|\n    | OBR_BLS_LI      |                LIPID PROFILE|\n    | OBR_POC_VG      | POCT BLOOD GAS VENOUS SAMPLE|\n    | OBR_BLS_HD      |              HDL CHOLESTEROL|\n    | OBR_BLS_FT      |                      FREE T4|\n    | OBR_BLS_FE      |               SERUM FERRITIN|\n    | OBR_BLS_GP      |    ELECTROLYTES NO POTASSIUM|\n    | OBR_BLS_CH      |                  CHOLESTEROL|\n    | OBR_BLS_MG      |                    MAGNESIUM|\n    | OBR_BLS_CO      |                     CORTISOL|\n\n    Each test is similarly encoded. The valid test codes in the full\n    blood count and U+E investigations are shown below:\n\n    | `investigation` | `test`     | Description          |\n    |-----------------|------------|----------------------|\n    | OBR_BLS_FB      | OBX_BLS_NE |           Neutrophils|\n    | OBR_BLS_FB      | OBX_BLS_PL |             Platelets|\n    | OBR_BLS_FB      | OBX_BLS_WB |      White Cell Count|\n    | OBR_BLS_FB      | OBX_BLS_LY |           Lymphocytes|\n    | OBR_BLS_FB      | OBX_BLS_MC |                   MCV|\n    | OBR_BLS_FB      | OBX_BLS_HB |           Haemoglobin|\n    | OBR_BLS_FB      | OBX_BLS_HC |           Haematocrit|\n    | OBR_BLS_UE      | OBX_BLS_NA |                Sodium|\n    | OBR_BLS_UE      | OBX_BLS_UR |                  Urea|\n    | OBR_BLS_UE      | OBX_BLS_K  |             Potassium|\n    | OBR_BLS_UE      | OBX_BLS_CR |            Creatinine|\n    | OBR_BLS_UE      | OBX_BLS_EP | eGFR/1.73m2 (CKD-EPI)|\n\n    Args:\n        engine: the connection to the database\n        test_names: Unlike the UHBW version of this table, there are no\n            investigation names here. Instead, restrict directly using\n            the test_name field.\n\n    Returns:\n        SQL query to retrieve blood tests table\n    \"\"\"\n\n    table = CheckedTable(\"HIC_BLoods\", engine)\n    return select(\n        table.col(\"nhs_number\").cast(String).label(\"patient_id\"),\n        table.col(\"test_name\"),\n        table.col(\"test_result\").label(\"result\"),\n        table.col(\"test_result_unit\").label(\"unit\"),\n        table.col(\"sample_collected_date_time\").label(\"sample_date\"),\n        table.col(\"result_available_date_time\").label(\"result_date\"),\n        table.col(\"result_lower_range\"),\n        table.col(\"result_upper_range\"),\n    ).where(table.col(\"test_name\").in_(test_names))\n</code></pre>"},{"location":"reference/#pyhbr.data_source.icb","title":"<code>icb</code>","text":"<p>Data sources available from the BNSSG ICB This file contains queries that fetch the raw data from the BNSSG ICB, which includes hospital episode statistics (HES) and primary care data.</p> <p>This file does not include the HIC data transferred to the ICB.</p>"},{"location":"reference/#pyhbr.data_source.icb.clinical_code_column_name","title":"<code>clinical_code_column_name(kind, position)</code>","text":"<p>Make the primary/secondary diagnosis/procedure column names</p> <p>Parameters:</p> Name Type Description Default <code>kind</code> <code>str</code> <p>Either \"diagnosis\" or \"procedure\".</p> required <code>position</code> <code>int</code> <p>0 for primary, 1 and higher for secondaries.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The column name for the clinical code compatible with the ICB HES tables.</p> Source code in <code>src\\pyhbr\\data_source\\icb.py</code> <pre><code>def clinical_code_column_name(kind: str, position: int) -&gt; str:\n    \"\"\"Make the primary/secondary diagnosis/procedure column names\n\n    Args:\n        kind: Either \"diagnosis\" or \"procedure\".\n        position: 0 for primary, 1 and higher for secondaries.\n\n    Returns:\n        The column name for the clinical code compatible with\n            the ICB HES tables.\n    \"\"\"\n\n    if kind == \"diagnosis\":\n        if position == 0:\n            return \"DiagnosisPrimary_ICD\"\n\n        return f\"Diagnosis{ordinal(position)}Secondary_ICD\"\n    else:\n        if position == 0:\n            # reversed compared to diagnoses\n            return \"PrimaryProcedure_OPCS\"\n\n        # Secondaries are offset by one compared to diagnoses\n        return f\"Procedure{ordinal(position+1)}_OPCS\"\n</code></pre>"},{"location":"reference/#pyhbr.data_source.icb.mortality_query","title":"<code>mortality_query(engine, start_date, end_date)</code>","text":"<p>Get the mortality query, including cause of death</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>The connection to the database</p> required <code>start_date</code> <code>date</code> <p>First date of death that will be included</p> required <code>end_date</code> <code>date</code> <p>Last date of death that will be included</p> required <p>Returns:</p> Type Description <code>Select</code> <p>SQL query to retrieve episodes table</p> Source code in <code>src\\pyhbr\\data_source\\icb.py</code> <pre><code>def mortality_query(engine: Engine, start_date: date, end_date: date) -&gt; Select:\n    \"\"\"Get the mortality query, including cause of death\n\n    Args:\n        engine: The connection to the database\n        start_date: First date of death that will be included\n        end_date: Last date of death that will be included\n\n    Returns:\n        SQL query to retrieve episodes table\n    \"\"\"\n\n    table = CheckedTable(\"mortality\", engine, schema=\"civil_registration\")\n\n    # Secondary cause of death columns\n    cause_of_death_columns = [\n        table.col(f\"S_COD_CODE_{n}\").label(f\"cause_of_death_{n+1}\")\n        for n in range(1, 16)\n    ]\n\n    return select(\n        table.col(\"Derived_Pseudo_NHS\").cast(String).label(\"patient_id\"),\n        table.col(\"REG_DATE_OF_DEATH\").cast(DateTime).label(\"date_of_death\"),\n        table.col(\"S_UNDERLYING_COD_ICD10\").label(\"cause_of_death_1\"),\n        *cause_of_death_columns,\n    ).where(\n        table.col(\"REG_DATE_OF_DEATH\") &gt;= start_date,\n        table.col(\"REG_DATE_OF_DEATH\") &lt;= end_date,\n        table.col(\"Derived_Pseudo_NHS\").is_not(None),\n        table.col(\"Derived_Pseudo_NHS\") != 9000219621,  # Invalid-patient marker    \n    )\n</code></pre>"},{"location":"reference/#pyhbr.data_source.icb.ordinal","title":"<code>ordinal(n)</code>","text":"<p>Make an an ordinal like \"2nd\" from a number n</p> <p>See https://stackoverflow.com/a/20007730.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The integer to convert to an ordinal string.</p> required <p>Returns:</p> Type Description <code>str</code> <p>For an integer (e.g. 5), the ordinal string (e.g. \"5th\")</p> Source code in <code>src\\pyhbr\\data_source\\icb.py</code> <pre><code>def ordinal(n: int) -&gt; str:\n    \"\"\"Make an an ordinal like \"2nd\" from a number n\n\n    See https://stackoverflow.com/a/20007730.\n\n    Args:\n        n: The integer to convert to an ordinal string.\n\n    Returns:\n        For an integer (e.g. 5), the ordinal string (e.g. \"5th\")\n    \"\"\"\n    if 11 &lt;= (n % 100) &lt;= 13:\n        suffix = \"th\"\n    else:\n        suffix = [\"th\", \"st\", \"nd\", \"rd\", \"th\"][min(n % 10, 4)]\n    return str(n) + suffix\n</code></pre>"},{"location":"reference/#pyhbr.data_source.icb.primary_care_attributes_query","title":"<code>primary_care_attributes_query(engine, patient_ids, gp_opt_outs)</code>","text":"<p>Get primary care patient information</p> <p>This is translated into an IN clause, which has an item limit.  If patient_ids is longer than 2000, an error is raised. If  more patient IDs are needed, split patient_ids and call this function multiple times.</p> <p>The values in patient_ids must be valid (they should come from a query such as sus_query).</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>The connection to the database</p> required <code>patient_ids</code> <code>list[str]</code> <p>The list of patient identifiers to filter the nhs_number column.</p> required <code>gp_opt_outs</code> <code>list[str]</code> <p>List of practice codes that are excluded from the data fetch (corresponds to the \"practice_code\" column in the table).</p> required <p>Returns:</p> Type Description <code>Select</code> <p>SQL query to retrieve episodes table</p> Source code in <code>src\\pyhbr\\data_source\\icb.py</code> <pre><code>def primary_care_attributes_query(engine: Engine, patient_ids: list[str], gp_opt_outs: list[str]) -&gt; Select:\n    \"\"\"Get primary care patient information\n\n    This is translated into an IN clause, which has an item limit. \n    If patient_ids is longer than 2000, an error is raised. If \n    more patient IDs are needed, split patient_ids and call this\n    function multiple times.\n\n    The values in patient_ids must be valid (they should come from\n    a query such as sus_query).\n\n    Args:\n        engine: The connection to the database\n        patient_ids: The list of patient identifiers to filter\n            the nhs_number column.\n        gp_opt_outs: List of practice codes that are excluded\n            from the data fetch (corresponds to the \"practice_code\"\n            column in the table).\n\n    Returns:\n        SQL query to retrieve episodes table\n    \"\"\"\n    if len(patient_ids) &gt; 2000:\n        raise ValueError(\"The list patient_ids must be less than 2000 long.\")\n\n    table = CheckedTable(\"primary_care_attributes\", engine)\n\n    return select(\n        table.col(\"nhs_number\").cast(String).label(\"patient_id\"),\n        table.col(\"attribute_period\").cast(DateTime).label(\"date\"),\n        table.col(\"homeless\"),\n\n        # No need for these, available in episodes data\n        #table.col(\"age\"),\n        #table.col(\"sex\"),\n\n        table.col(\"abortion\"),\n        table.col(\"adhd\"),\n        table.col(\"af\"),\n        table.col(\"alcohol_cscore\"),\n        table.col(\"alcohol_units\"),\n        table.col(\"amputations\"),\n        table.col(\"anaemia_iron\"),\n        table.col(\"anaemia_other\"),\n        table.col(\"angio_anaph\"),\n        table.col(\"arrhythmia_other\"),\n        table.col(\"asthma\"),\n        table.col(\"autism\"),\n        table.col(\"back_pain\"),\n        table.col(\"bmi\"),\n        table.col(\"bp_date\"),\n        table.col(\"bp_reading\"),\n        table.col(\"cancer_bladder_year\"),\n        table.col(\"cancer_bladder\"),\n        table.col(\"cancer_bowel_year\"),\n        table.col(\"cancer_bowel\"),\n        table.col(\"cancer_breast_year\"),\n        table.col(\"cancer_breast\"),\n        table.col(\"cancer_cervical_year\"),\n        table.col(\"cancer_cervical\"),\n        table.col(\"cancer_giliver_year\"),\n        table.col(\"cancer_giliver\"),\n        table.col(\"cancer_headneck_year\"),\n        table.col(\"cancer_headneck\"),\n        table.col(\"cancer_kidney_year\"),\n        table.col(\"cancer_kidney\"),\n        table.col(\"cancer_leuklymph_year\"),\n        table.col(\"cancer_leuklymph\"),\n        table.col(\"cancer_lung_year\"),\n        table.col(\"cancer_lung\"),\n        table.col(\"cancer_melanoma_year\"),\n        table.col(\"cancer_melanoma\"),\n        table.col(\"cancer_metase_year\"),\n        table.col(\"cancer_metase\"),\n        table.col(\"cancer_nonmaligskin_year\"),\n        table.col(\"cancer_nonmaligskin\"),\n        table.col(\"cancer_other_year\"),\n        table.col(\"cancer_other\"),\n        table.col(\"cancer_ovarian_year\"),\n        table.col(\"cancer_ovarian\"),\n        table.col(\"cancer_prostate_year\"),\n        table.col(\"cancer_prostate\"),\n        table.col(\"cardio_other\"),\n        table.col(\"cataracts\"),\n        table.col(\"ckd\"),\n        table.col(\"coag\"),\n        table.col(\"coeliac\"),\n        table.col(\"contraception\"),\n        table.col(\"copd\"),\n        table.col(\"cystic_fibrosis\"),\n        table.col(\"dementia\"),\n        table.col(\"dep_alcohol\"),\n        table.col(\"dep_benzo\"),\n        table.col(\"dep_cannabis\"),\n        table.col(\"dep_cocaine\"),\n        table.col(\"dep_opioid\"),\n        table.col(\"dep_other\"),\n        table.col(\"depression\"),\n        table.col(\"diabetes_1\"),\n        table.col(\"diabetes_2\"),\n        table.col(\"diabetes_gest\"),\n        table.col(\"diabetes_retina\"),\n        table.col(\"disorder_eating\"),\n        table.col(\"disorder_pers\"),\n        table.col(\"dna_cpr\"),\n        table.col(\"eczema\"),\n        table.col(\"efi_category\"),\n        table.col(\"egfr\"),\n        table.col(\"endocrine_other\"),\n        table.col(\"endometriosis\"),\n        table.col(\"eol_plan\"),\n        table.col(\"epaccs\"),\n        table.col(\"epilepsy\"),\n        table.col(\"ethnicity\"),\n        table.col(\"fatigue\"),\n        table.col(\"fev1\"),\n        table.col(\"fragility\"),\n        table.col(\"gender_identity\"),\n        table.col(\"gout\"),\n        table.col(\"gppaq\"),\n        table.col(\"has_carer\"),\n        table.col(\"health_check\"),\n        table.col(\"hearing_impair\"),\n        table.col(\"hep_b\"),\n        table.col(\"hep_c\"),\n        table.col(\"hf\"),\n        table.col(\"hiv\"),\n        table.col(\"housebound\"),\n        table.col(\"ht\"),\n        table.col(\"ibd\"),\n        table.col(\"ibs\"),\n        table.col(\"ihd_mi\"),\n        table.col(\"ihd_nonmi\"),\n        table.col(\"incont_urinary\"),\n        table.col(\"infant_feeding\"),\n        table.col(\"inflam_arthritic\"),\n        table.col(\"is_carer\"),\n        table.col(\"learning_diff\"),\n        table.col(\"learning_dis\"),\n        table.col(\"live_birth\"),\n        table.col(\"liver_alcohol\"),\n        table.col(\"liver_nafl\"),\n        table.col(\"liver_other\"),\n        table.col(\"lsoa\"),\n        table.col(\"lung_restrict\"),\n        table.col(\"macular_degen\"),\n        table.col(\"marital\"),\n        table.col(\"measles_mumps\"),\n        table.col(\"migraine\"),\n        table.col(\"miscarriage\"),\n        table.col(\"mmr1\"),\n        table.col(\"mmr2\"),\n        table.col(\"mnd\"),\n        table.col(\"mrc_dyspnoea\"),\n        table.col(\"ms\"),\n        table.col(\"neuro_pain\"),\n        table.col(\"neuro_various\"),\n        table.col(\"newborn_check\"),\n        table.col(\"newborn_weight\"),\n        table.col(\"nh_rh\"),\n        table.col(\"nose\"),\n        table.col(\"obesity\"),\n        table.col(\"organ_transplant\"),\n        table.col(\"osteoarthritis\"),\n        table.col(\"osteoporosis\"),\n        table.col(\"parkinsons\"),\n        table.col(\"pelvic\"),\n        table.col(\"phys_disability\"),\n        table.col(\"poly_ovary\"),\n        table.col(\"polypharmacy_acute\"),\n        table.col(\"polypharmacy_repeat\"),\n        table.col(\"pre_diabetes\"),\n        table.col(\"pref_death\"),\n        table.col(\"pregnancy\"),\n        table.col(\"prim_language\"),\n        table.col(\"psoriasis\"),\n        table.col(\"ptsd\"),\n        table.col(\"qof_af\"),\n        table.col(\"qof_asthma\"),\n        table.col(\"qof_cancer\"),\n        table.col(\"qof_chd\"),\n        table.col(\"qof_ckd\"),\n        table.col(\"qof_copd\"),\n        table.col(\"qof_dementia\"),\n        table.col(\"qof_depression\"),\n        table.col(\"qof_diabetes\"),\n        table.col(\"qof_epilepsy\"),\n        table.col(\"qof_hf\"),\n        table.col(\"qof_ht\"),\n        table.col(\"qof_learndis\"),\n        table.col(\"qof_mental\"),\n        table.col(\"qof_obesity\"),\n        table.col(\"qof_osteoporosis\"),\n        table.col(\"qof_pad\"),\n        table.col(\"qof_pall\"),\n        table.col(\"qof_rheumarth\"),\n        table.col(\"qof_stroke\"),\n\n        # Excluding a cardiovascular risk score as not wanting to use\n        # a feature that may require hidden variables to calculate.\n        #table.col(\"qrisk2_3\"),\n\n        table.col(\"religion\"),\n        table.col(\"ricketts\"),\n        table.col(\"sad\"),\n        table.col(\"screen_aaa\"),\n        table.col(\"screen_bowel\"),\n        table.col(\"screen_breast\"),\n        table.col(\"screen_cervical\"),\n        table.col(\"screen_eye\"),\n        table.col(\"self_harm\"),\n        table.col(\"sexual_orient\"),\n        table.col(\"sickle\"),\n        table.col(\"smi\"),\n        table.col(\"smoking\"),\n        table.col(\"stomach\"),\n        table.col(\"stroke\"),\n        table.col(\"tb\"),\n        table.col(\"thyroid\"),\n        table.col(\"uterine\"),\n        table.col(\"vasc_dis\"),\n        table.col(\"veteran\"),\n        table.col(\"visual_impair\"),\n    ).where(\n        table.col(\"nhs_number\").in_(patient_ids),\n        table.col(\"practice_code\").not_in(gp_opt_outs),\n    )\n</code></pre>"},{"location":"reference/#pyhbr.data_source.icb.primary_care_measurements_query","title":"<code>primary_care_measurements_query(engine, patient_ids, gp_opt_outs)</code>","text":"<p>Get physiological measurements performed in primary care</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>the connection to the database</p> required <code>patient_ids</code> <code>list[str]</code> <p>The list of patient identifiers to filter the nhs_number column.</p> required <code>gp_opt_outs</code> <code>list[str]</code> <p>List of practice codes that are excluded from the data fetch (corresponds to the \"practice_code\" column in the table).</p> required <p>Returns:</p> Type Description <code>Select</code> <p>SQL query to retrieve episodes table</p> Source code in <code>src\\pyhbr\\data_source\\icb.py</code> <pre><code>def primary_care_measurements_query(\n    engine: Engine, patient_ids: list[str], gp_opt_outs: list[str]\n) -&gt; Select:\n    \"\"\"Get physiological measurements performed in primary care\n\n    Args:\n        engine: the connection to the database\n        patient_ids: The list of patient identifiers to filter\n            the nhs_number column.\n        gp_opt_outs: List of practice codes that are excluded\n            from the data fetch (corresponds to the \"practice_code\"\n            column in the table).\n\n    Returns:\n        SQL query to retrieve episodes table\n    \"\"\"\n    table = CheckedTable(\"measurement\", engine, schema=\"swd\")\n\n    return select(\n        table.col(\"nhs_number\").cast(String).label(\"patient_id\"),\n        table.col(\"measurement_date\").label(\"date\"),\n        table.col(\"measurement_name\").label(\"name\"),\n        table.col(\"measurement_value\").label(\"result\"),\n        table.col(\"measurement_group\").label(\"group\"),\n    ).where(\n        table.col(\"nhs_number\").in_(patient_ids),\n        table.col(\"practice_code\").not_in(gp_opt_outs),\n    )\n</code></pre>"},{"location":"reference/#pyhbr.data_source.icb.primary_care_prescriptions_query","title":"<code>primary_care_prescriptions_query(engine, patient_ids, gp_opt_outs)</code>","text":"<p>Get medications dispensed in primary care</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>the connection to the database</p> required <code>patient_ids</code> <code>list[str]</code> <p>The list of patient identifiers to filter the nhs_number column.</p> required <code>gp_opt_outs</code> <code>list[str]</code> <p>List of practice codes that are excluded from the data fetch (corresponds to the \"practice_code\" column in the table).</p> required <p>Returns:</p> Type Description <code>Select</code> <p>SQL query to retrieve episodes table</p> Source code in <code>src\\pyhbr\\data_source\\icb.py</code> <pre><code>def primary_care_prescriptions_query(\n    engine: Engine, patient_ids: list[str], gp_opt_outs: list[str]\n) -&gt; Select:\n    \"\"\"Get medications dispensed in primary care\n\n    Args:\n        engine: the connection to the database\n        patient_ids: The list of patient identifiers to filter\n            the nhs_number column.\n        gp_opt_outs: List of practice codes that are excluded\n            from the data fetch (corresponds to the \"practice_code\"\n            column in the table).\n\n    Returns:\n        SQL query to retrieve episodes table\n    \"\"\"\n    table = CheckedTable(\"prescription\", engine, schema=\"swd\")\n\n    return select(\n        table.col(\"nhs_number\").cast(String).label(\"patient_id\"),\n        table.col(\"prescription_date\").cast(DateTime).label(\"date\"),\n        table.col(\"prescription_name\").label(\"name\"),\n        table.col(\"prescription_quantity\").label(\"quantity\"),\n        table.col(\"prescription_type\").label(\"acute_or_repeat\"),\n    ).where(\n        table.col(\"nhs_number\").in_(patient_ids),\n        table.col(\"practice_code\").not_in(gp_opt_outs),\n    )\n</code></pre>"},{"location":"reference/#pyhbr.data_source.icb.score_seg_query","title":"<code>score_seg_query(engine, patient_ids)</code>","text":"<p>Get score segment information from SWD (Charlson/Cambridge score, etc.)</p> <p>This is translated into an IN clause, which has an item limit.  If patient_ids is longer than 2000, an error is raised. If  more patient IDs are needed, split patient_ids and call this function multiple times.</p> <p>The values in patient_ids must be valid (they should come from a query such as sus_query).</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>The connection to the database</p> required <code>patient_ids</code> <code>list[str]</code> <p>The list of patient identifiers to filter the nhs_number column.</p> required <p>Returns:</p> Type Description <code>Select</code> <p>SQL query to retrieve episodes table</p> Source code in <code>src\\pyhbr\\data_source\\icb.py</code> <pre><code>def score_seg_query(engine: Engine, patient_ids: list[str]) -&gt; Select:\n    \"\"\"Get score segment information from SWD (Charlson/Cambridge score, etc.)\n\n    This is translated into an IN clause, which has an item limit. \n    If patient_ids is longer than 2000, an error is raised. If \n    more patient IDs are needed, split patient_ids and call this\n    function multiple times.\n\n    The values in patient_ids must be valid (they should come from\n    a query such as sus_query).\n\n    Args:\n        engine: The connection to the database\n        patient_ids: The list of patient identifiers to filter\n            the nhs_number column.\n\n    Returns:\n        SQL query to retrieve episodes table\n    \"\"\"\n    if len(patient_ids) &gt; 2000:\n        raise ValueError(\"The list patient_ids must be less than 2000 long.\")\n\n    table = CheckedTable(\"score_seg\", engine, schema=\"swd\")\n\n    return select(\n        table.col(\"nhs_number\").cast(String).label(\"patient_id\"),\n        table.col(\"attribute_period\").cast(DateTime).label(\"date\"),\n        table.col(\"cambridge_score\"),\n        table.col(\"charlson_score\"),\n    ).where(\n        table.col(\"nhs_number\").in_(patient_ids),\n    )\n</code></pre>"},{"location":"reference/#pyhbr.data_source.icb.sus_query","title":"<code>sus_query(engine, start_date, end_date)</code>","text":"<p>Get the episodes list in the HES data</p> <p>This table contains one episode per row. Diagnosis/procedure clinical codes are represented in wide format (one clinical code position per columns), and patient demographic information is also included.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>the connection to the database</p> required <code>start_date</code> <code>date</code> <p>first valid consultant-episode start date</p> required <code>end_date</code> <code>date</code> <p>last valid consultant-episode start date</p> required <p>Returns:</p> Type Description <code>Select</code> <p>SQL query to retrieve episodes table</p> Source code in <code>src\\pyhbr\\data_source\\icb.py</code> <pre><code>def sus_query(engine: Engine, start_date: date, end_date: date) -&gt; Select:\n    \"\"\"Get the episodes list in the HES data\n\n    This table contains one episode per row. Diagnosis/procedure clinical\n    codes are represented in wide format (one clinical code position per\n    columns), and patient demographic information is also included.\n\n    Args:\n        engine: the connection to the database\n        start_date: first valid consultant-episode start date\n        end_date: last valid consultant-episode start date\n\n    Returns:\n        SQL query to retrieve episodes table\n    \"\"\"\n    table = CheckedTable(\"vw_apc_sem_001\", engine)\n\n    # Standard columns containing IDs, dates, patient demographics, etc\n    columns = [\n        table.col(\"AIMTC_Pseudo_NHS\").cast(String).label(\"patient_id\"),\n        table.col(\"AIMTC_Age\").cast(String).label(\"age\"),\n        table.col(\"Sex\").cast(String).label(\"gender\"),\n        table.col(\"PBRspellID\").cast(String).label(\"spell_id\"),\n        table.col(\"StartDate_ConsultantEpisode\").label(\"episode_start\"),\n        table.col(\"EndDate_ConsultantEpisode\").label(\"episode_end\"),\n        # Using the start and the end of the spells as admission/discharge\n        # times for the purposes of identifying lab results and prescriptions\n        # within the spell.\n        table.col(\"StartDate_HospitalProviderSpell\").label(\"admission\"),\n        table.col(\"DischargeDate_FromHospitalProviderSpell\").label(\"discharge\"),\n    ]\n\n    # Diagnosis and procedure columns are renamed to (diagnosis|procedure)_n,\n    # where n begins from 1 (for the primary code; secondaries are represented\n    # using n &gt; 1)\n    clinical_code_column_names = {\n        clinical_code_column_name(kind, n): f\"{kind}_{n+1}\"\n        for kind, n in product([\"diagnosis\", \"procedure\"], range(24))\n    }\n\n    clinical_code_columns = [\n        table.col(real_name).cast(String).label(new_name)\n        for real_name, new_name in clinical_code_column_names.items()\n    ]\n\n    # Append the clinical code columns to the other data columns\n    columns += clinical_code_columns\n\n    # Valid rows must have one of the following commissioner codes\n    #\n    # These commissioner codes are used to restrict the system-wide dataset\n    # to just in-area patients (those registered with BNSSG GP practices).\n    # When linking to primary care data \n    valid_list = [\"5M8\", \"11T\", \"5QJ\", \"11H\", \"5A3\", \"12A\", \"15C\", \"14F\", \"Q65\"]\n\n    return select(*columns).where(\n        table.col(\"StartDate_ConsultantEpisode\") &gt;= start_date,\n        table.col(\"EndDate_ConsultantEpisode\") &lt;= end_date,\n        table.col(\"AIMTC_Pseudo_NHS\").is_not(None),\n        table.col(\"AIMTC_Pseudo_NHS\") != 9000219621,  # Invalid-patient marker\n        table.col(\"AIMTC_OrganisationCode_Codeofcommissioner\").in_(valid_list),\n    )\n</code></pre>"},{"location":"reference/#pyhbr.middle","title":"<code>middle</code>","text":"<p>Routines for interfacing between the data sources and analysis functions</p>"},{"location":"reference/#pyhbr.middle.from_hic","title":"<code>from_hic</code>","text":"<p>Convert HIC tables into the formats required for analysis</p>"},{"location":"reference/#pyhbr.middle.from_hic.calculate_age","title":"<code>calculate_age(episodes, demographics)</code>","text":"<p>Calculate the patient age at each episode</p> <p>The HIC data contains only year_of_birth, which is used here. In order to make an unbiased estimate of the age, birthday is assumed to be 2nd july (halfway through the year).</p> <p>Parameters:</p> Name Type Description Default <code>episodes</code> <code>DataFrame</code> <p>Contains <code>episode_start</code> date and column <code>patient_id</code>, indexed by <code>episode_id</code>.</p> required <code>demographics</code> <code>DataFrame</code> <p>Contains <code>year_of_birth</code> date and index <code>patient_id</code>.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>A series containing age, indexed by <code>episode_id</code>.</p> Source code in <code>src\\pyhbr\\middle\\from_hic.py</code> <pre><code>def calculate_age(episodes: DataFrame, demographics: DataFrame) -&gt; Series:\n    \"\"\"Calculate the patient age at each episode\n\n    The HIC data contains only year_of_birth, which is used here. In order\n    to make an unbiased estimate of the age, birthday is assumed to be\n    2nd july (halfway through the year).\n\n    Args:\n        episodes: Contains `episode_start` date and column `patient_id`,\n            indexed by `episode_id`.\n        demographics: Contains `year_of_birth` date and index `patient_id`.\n\n    Returns:\n        A series containing age, indexed by `episode_id`.\n    \"\"\"\n    df = episodes.merge(demographics, how=\"left\", on=\"patient_id\")\n    age_offset = np.where(\n        (df[\"episode_start\"].dt.month &lt; 7) &amp; (df[\"episode_start\"].dt.day &lt; 2), 1, 0\n    )\n    age = df[\"episode_start\"].dt.year - df[\"year_of_birth\"] - age_offset\n    age.index = episodes.index\n    return age\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_hic.check_const_column","title":"<code>check_const_column(df, col_name, expect)</code>","text":"<p>Raise an error if a column is not constant</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The table to check</p> required <code>col_name</code> <code>str</code> <p>The name of the column which should be constant</p> required <code>expect</code> <code>str</code> <p>The expected constant value of the column</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Raised if the column is not constant with the expected value.</p> Source code in <code>src\\pyhbr\\middle\\from_hic.py</code> <pre><code>def check_const_column(df: pd.DataFrame, col_name: str, expect: str):\n    \"\"\"Raise an error if a column is not constant\n\n    Args:\n        df: The table to check\n        col_name: The name of the column which should be constant\n        expect: The expected constant value of the column\n\n    Raises:\n        RuntimeError: Raised if the column is not constant with\n            the expected value.\n    \"\"\"\n    if not all(df[col_name] == expect):\n        raise RuntimeError(\n            f\"Found unexpected value in '{col_name}' column. \"\n            f\"Expected constant '{expect}', but got: \"\n            f\"{df[col_name].unique()}\"\n        )\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_hic.filter_by_medicine","title":"<code>filter_by_medicine(df)</code>","text":"<p>Filter a dataframe by medicine name</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Contains a column <code>name</code> containing the medicine name</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The dataframe, filtered to the set of medicines of interest, with a new column <code>group</code> containing just the medicine type (e.g. \"oac\", \"nsaid\").</p> Source code in <code>src\\pyhbr\\middle\\from_hic.py</code> <pre><code>def filter_by_medicine(df: DataFrame) -&gt; DataFrame:\n    \"\"\"Filter a dataframe by medicine name\n\n    Args:\n        df: Contains a column `name` containing the medicine\n            name\n\n    Returns:\n        The dataframe, filtered to the set of medicines of interest,\n            with a new column `group` containing just the medicine\n            type (e.g. \"oac\", \"nsaid\").\n    \"\"\"\n\n    prescriptions_of_interest = {\n        \"warfarin\": \"oac\",\n        \"apixaban\": \"oac\",\n        \"dabigatran etexilate\": \"oac\",\n        \"edoxaban\": \"oac\",\n        \"rivaroxaban\": \"oac\",\n        \"ibuprofen\": \"nsaid\",\n        \"naproxen\": \"nsaid\",\n        \"diclofenac\": \"nsaid\",\n        \"diclofenac sodium\": \"nsaid\",\n        \"celecoxib\": \"nsaid\",  # Not present in HIC data\n        \"mefenamic acid\": \"nsaid\",  # Not present in HIC data\n        \"etoricoxib\": \"nsaid\",\n        \"indometacin\": \"nsaid\",  # This spelling is used in HIC data\n        \"indomethacin\": \"nsaid\",  # Alternative spelling\n        # \"aspirin\": \"nsaid\" -- not accounting for high dose\n    }\n\n    # Remove rows with missing medicine name\n    df = df[~df[\"name\"].isna()]\n\n    # This line is really slow (30s for 3.5m rows)\n    df = df[\n        df[\"name\"].str.contains(\n            \"|\".join(prescriptions_of_interest.keys()), case=False, regex=True\n        )\n    ]\n\n    # Add the type of prescription to the table\n    df[\"group\"] = df[\"name\"]\n    for prescription, group in prescriptions_of_interest.items():\n        df[\"group\"] = df[\"group\"].str.replace(\n            \".*\" + prescription + \".*\", group, case=False, regex=True\n        )\n\n    return df\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_hic.get_clinical_codes","title":"<code>get_clinical_codes(engine, diagnoses_file, procedures_file)</code>","text":"<p>Main diagnoses/procedures fetch for the HIC data</p> <p>This function wraps the diagnoses/procedures queries and a filtering operation to reduce the tables to only those rows which contain a code in a group. One table is returned which contains both the diagnoses and procedures in long format, along with the associated episode ID and the primary/secondary position of the code in the episode.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>The connection to the database</p> required <code>diagnoses_file</code> <code>str</code> <p>The diagnoses codes file name (loaded from the package)</p> required <code>procedures_file</code> <code>str</code> <p>The procedures codes file name (loaded from the package)</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A table containing diagnoses/procedures, normalised codes, code groups, diagnosis positions, and associated episode ID.</p> Source code in <code>src\\pyhbr\\middle\\from_hic.py</code> <pre><code>def get_clinical_codes(\n    engine: Engine, diagnoses_file: str, procedures_file: str\n) -&gt; pd.DataFrame:\n    \"\"\"Main diagnoses/procedures fetch for the HIC data\n\n    This function wraps the diagnoses/procedures queries and a filtering\n    operation to reduce the tables to only those rows which contain a code\n    in a group. One table is returned which contains both the diagnoses and\n    procedures in long format, along with the associated episode ID and the\n    primary/secondary position of the code in the episode.\n\n    Args:\n        engine: The connection to the database\n        diagnoses_file: The diagnoses codes file name (loaded from the package)\n        procedures_file: The procedures codes file name (loaded from the package)\n\n    Returns:\n        A table containing diagnoses/procedures, normalised codes, code groups,\n            diagnosis positions, and associated episode ID.\n    \"\"\"\n\n    diagnosis_codes = clinical_codes.load_from_package(diagnoses_file)\n    procedures_codes = clinical_codes.load_from_package(procedures_file)\n\n    # Fetch the data from the server\n    diagnoses = get_data(engine, hic.diagnoses_query)\n    procedures = get_data(engine, hic.procedures_query)\n\n    # Reduce data to only code groups, and combine diagnoses/procedures\n    filtered_diagnoses = clinical_codes.filter_to_groups(diagnoses, diagnosis_codes)\n    filtered_procedures = clinical_codes.filter_to_groups(procedures, procedures_codes)\n\n    # Tag the diagnoses/procedures, and combine the tables\n    filtered_diagnoses[\"type\"] = \"diagnosis\"\n    filtered_procedures[\"type\"] = \"procedure\"\n\n    codes = pd.concat([filtered_diagnoses, filtered_procedures])\n    codes[\"type\"] = codes[\"type\"].astype(\"category\")\n\n    return codes\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_hic.get_demographics","title":"<code>get_demographics(engine)</code>","text":"<p>Get patient demographic information</p> <p>Gender is encoded using the NHS data dictionary values, which is mapped to a category column in the table. (Note that initial values are strings, not integers.)</p> <ul> <li>\"0\": Not known. Mapped to \"unknown\"</li> <li>\"1\": Male: Mapped to \"male\"</li> <li>\"2\": Female. Mapped to \"female\"</li> <li>\"9\": Not specified. Mapped to \"unknown\".</li> </ul> <p>Not mapping 0/9 to NA in case either is related to non-binary genders (i.e. it contains information, rather than being a NULL field).</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>The connection to the database</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A table indexed by patient_id, containing gender, birth year, and death_date (if applicable).</p> Source code in <code>src\\pyhbr\\middle\\from_hic.py</code> <pre><code>def get_demographics(engine: Engine) -&gt; pd.DataFrame:\n    \"\"\"Get patient demographic information\n\n    Gender is encoded using the NHS data dictionary values, which\n    is mapped to a category column in the table. (Note that initial\n    values are strings, not integers.)\n\n    * \"0\": Not known. Mapped to \"unknown\"\n    * \"1\": Male: Mapped to \"male\"\n    * \"2\": Female. Mapped to \"female\"\n    * \"9\": Not specified. Mapped to \"unknown\".\n\n    Not mapping 0/9 to NA in case either is related to non-binary\n    genders (i.e. it contains information, rather than being a NULL field).\n\n    Args:\n        engine: The connection to the database\n\n    Returns:\n        A table indexed by patient_id, containing gender, birth\n            year, and death_date (if applicable).\n\n    \"\"\"\n    df = get_data(engine, hic.demographics_query)\n    df.set_index(\"patient_id\", drop=True, inplace=True)\n\n    # Convert gender to categories\n    df[\"gender\"] = df[\"gender\"].replace(\"9\", \"0\")\n    df[\"gender\"] = df[\"gender\"].astype(\"category\")\n    df[\"gender\"] = df[\"gender\"].cat.rename_categories(\n        {\"0\": \"unknown\", \"1\": \"male\", \"2\": \"female\"}\n    )\n\n    return df\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_hic.get_episodes","title":"<code>get_episodes(engine, start_date, end_date)</code>","text":"<p>Get the table of episodes</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>The connection to the database</p> required <code>start_date</code> <code>date</code> <p>The start date (inclusive) for returned episodes</p> required <code>end_date</code> <code>date</code> <p>The end date (inclusive) for returned episodes</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The episode data, indexed by episode_id. This contains the columns <code>patient_id</code>, <code>spell_id</code>, <code>episode_start</code>, <code>admission</code>, <code>discharge</code>, <code>age</code>, and <code>gender</code></p> Source code in <code>src\\pyhbr\\middle\\from_hic.py</code> <pre><code>def get_episodes(engine: Engine, start_date: date, end_date: date) -&gt; pd.DataFrame:\n    \"\"\"Get the table of episodes\n\n    Args:\n        engine: The connection to the database\n        start_date: The start date (inclusive) for returned episodes\n        end_date:  The end date (inclusive) for returned episodes\n\n    Returns:\n        The episode data, indexed by episode_id. This contains\n            the columns `patient_id`, `spell_id`, `episode_start`,\n            `admission`, `discharge`, `age`, and `gender`\n\n    \"\"\"\n    episodes = get_data(engine, hic.episodes_query, start_date, end_date)\n    episodes = episodes.set_index(\"episode_id\", drop=True)\n    demographics = get_demographics(engine)\n    episodes[\"age\"] = calculate_age(episodes, demographics)\n    episodes[\"gender\"] = get_gender(episodes, demographics) \n\n    return episodes\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_hic.get_gender","title":"<code>get_gender(episodes, demographics)</code>","text":"<p>Get gender from the demographics table for each index event</p> <p>Parameters:</p> Name Type Description Default <code>episodes</code> <code>DataFrame</code> <p>Indexed by <code>episode_id</code> and having column <code>patient_id</code></p> required <code>demographics</code> <code>DataFrame</code> <p>Having columns <code>patient_id</code> and <code>gender</code>.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>A series containing gender indexed by <code>episode_id</code></p> Source code in <code>src\\pyhbr\\middle\\from_hic.py</code> <pre><code>def get_gender(episodes: DataFrame, demographics: DataFrame) -&gt; Series:\n    \"\"\"Get gender from the demographics table for each index event\n\n    Args:\n        episodes: Indexed by `episode_id` and having column `patient_id`\n        demographics: Having columns `patient_id` and `gender`.\n\n    Returns:\n        A series containing gender indexed by `episode_id`\n    \"\"\"\n    gender = episodes.merge(demographics, how=\"left\", on=\"patient_id\")[\"gender\"]\n    gender.index = episodes.index\n    return gender\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_hic.get_lab_results","title":"<code>get_lab_results(engine, episodes)</code>","text":"<p>Get relevant laboratory results from the HIC data, linked to episode</p> <p>For information about the contents of the table, refer to the documentation for get_unlinked_lab_results().</p> <p>This function links each laboratory test to the first episode containing the sample collected date in its date range. For more about this, see link_to_episodes().</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>The connection to the database</p> required <code>episodes</code> <code>DataFrame</code> <p>The episodes table, used for linking. Must contain <code>patient_id</code>, <code>episode_id</code>, <code>episode_start</code> and <code>episode_end</code>.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Table of laboratory results, including Hb (haemoglobin), platelet count, and eGFR (kidney function). The columns are <code>sample_date</code>, <code>test_name</code>, <code>episode_id</code>.</p> Source code in <code>src\\pyhbr\\middle\\from_hic.py</code> <pre><code>def get_lab_results(engine: Engine, episodes: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Get relevant laboratory results from the HIC data, linked to episode\n\n    For information about the contents of the table, refer to the\n    documentation for [get_unlinked_lab_results()][pyhbr.middle.from_hic.get_unlinked_lab_results].\n\n    This function links each laboratory test to the first episode containing\n    the sample collected date in its date range. For more about this, see\n    [link_to_episodes()][pyhbr.middle.from_hic.link_to_episodes].\n\n    Args:\n        engine: The connection to the database\n        episodes: The episodes table, used for linking. Must contain\n            `patient_id`, `episode_id`, `episode_start` and `episode_end`.\n\n    Returns:\n        Table of laboratory results, including Hb (haemoglobin),\n            platelet count, and eGFR (kidney function). The columns are\n            `sample_date`, `test_name`, `episode_id`.\n    \"\"\"\n\n    # Do not link to episodes\n    return get_unlinked_lab_results(engine)\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_hic.get_prescriptions","title":"<code>get_prescriptions(engine, episodes)</code>","text":"<p>Get relevant prescriptions from the HIC data, linked to episode</p> <p>For information about the contents of the table, refer to the documentation for get_unlinked_prescriptions().</p> <p>This function links each prescription to the first episode containing the prescription order date in its date range. For more about this, see link_to_episodes().</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>The connection to the database</p> required <code>episodes</code> <code>DataFrame</code> <p>The episodes table, used for linking. Must contain <code>patient_id</code>, <code>episode_id</code>, <code>episode_start</code> and <code>episode_end</code>.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The table of prescriptions, including the prescription name, prescription group (oac or nsaid), frequency (in doses per day), and link to the associated episode.</p> Source code in <code>src\\pyhbr\\middle\\from_hic.py</code> <pre><code>def get_prescriptions(engine: Engine, episodes: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Get relevant prescriptions from the HIC data, linked to episode\n\n    For information about the contents of the table, refer to the\n    documentation for [get_unlinked_prescriptions()][pyhbr.middle.from_hic.get_unlinked_prescriptions].\n\n    This function links each prescription to the first episode containing\n    the prescription order date in its date range. For more about this, see\n    [link_to_episodes()][pyhbr.middle.from_hic.link_to_episodes].\n\n    Args:\n        engine: The connection to the database\n        episodes: The episodes table, used for linking. Must contain\n            `patient_id`, `episode_id`, `episode_start` and `episode_end`.\n\n    Returns:\n        The table of prescriptions, including the prescription name,\n            prescription group (oac or nsaid), frequency (in doses per day),\n            and link to the associated episode.\n    \"\"\"\n\n    # Do not link the prescriptions to episode\n    return get_unlinked_prescriptions(engine)\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_hic.get_unlinked_lab_results","title":"<code>get_unlinked_lab_results(engine, table_name='cv1_pathology_blood')</code>","text":"<p>Get laboratory results from the HIC database (unlinked to episode)</p> <p>This function returns data for the following three tests, identified by one of these values in the <code>test_name</code> column:</p> <ul> <li><code>hb</code>: haemoglobin (unit: g/dL)</li> <li><code>egfr</code>: eGFR (unit: mL/min)</li> <li><code>platelets</code>: platelet count (unit: 10^9/L)</li> </ul> <p>The test result is associated to a <code>patient_id</code>, and the time when the sample for the test was collected is stored in the <code>sample_date</code> column.</p> <p>Some values in the underlying table contain inequalities in the results column, which have been removed (so egfr &gt;90 becomes 90).</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>The connection to the database</p> required <code>table_name</code> <code>str</code> <p>This defaults to \"cv1_pathology_blood\" for UHBW, but can be overwritten with \"HIC_Bloods\" for ICB.</p> <code>'cv1_pathology_blood'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Table of laboratory results, including Hb (haemoglobin), platelet count, and eGFR (kidney function). The columns are <code>patient_id</code>, <code>test_name</code>, and <code>sample_date</code>.</p> Source code in <code>src\\pyhbr\\middle\\from_hic.py</code> <pre><code>def get_unlinked_lab_results(engine: Engine, table_name: str = \"cv1_pathology_blood\") -&gt; pd.DataFrame:\n    \"\"\"Get laboratory results from the HIC database (unlinked to episode)\n\n    This function returns data for the following three\n    tests, identified by one of these values in the\n    `test_name` column:\n\n    * `hb`: haemoglobin (unit: g/dL)\n    * `egfr`: eGFR (unit: mL/min)\n    * `platelets`: platelet count (unit: 10^9/L)\n\n    The test result is associated to a `patient_id`,\n    and the time when the sample for the test was collected\n    is stored in the `sample_date` column.\n\n    Some values in the underlying table contain inequalities\n    in the results column, which have been removed (so\n    egfr &gt;90 becomes 90).\n\n    Args:\n        engine: The connection to the database\n        table_name: This defaults to \"cv1_pathology_blood\" for UHBW, but\n            can be overwritten with \"HIC_Bloods\" for ICB.\n\n    Returns:\n        Table of laboratory results, including Hb (haemoglobin),\n            platelet count, and eGFR (kidney function). The columns are\n            `patient_id`, `test_name`, and `sample_date`.\n\n    \"\"\"\n    df = get_data(engine, hic.pathology_blood_query, [\"OBR_BLS_UE\", \"OBR_BLS_FB\"])\n\n    df[\"test_name\"] = df[\"investigation\"] + \"_\" + df[\"test\"]\n\n    test_of_interest = {\n        \"OBR_BLS_FB_OBX_BLS_HB\": \"hb\",\n        \"OBR_BLS_UE_OBX_BLS_EP\": \"egfr\",\n        \"OBR_BLS_FB_OBX_BLS_PL\": \"platelets\",\n    }\n\n    # Only keep tests of interest: platelets, egfr, and hb\n    df = df[df[\"test_name\"].isin(test_of_interest.keys())]\n\n    # Rename the items\n    df[\"test_name\"] = df[\"test_name\"].map(test_of_interest)\n\n    # Check egfr unit\n    rows = df[df[\"test_name\"] == \"egfr\"]\n    check_const_column(rows, \"unit\", \"mL/min\")\n\n    # Check hb unit\n    rows = df[df[\"test_name\"] == \"hb\"]\n    check_const_column(rows, \"unit\", \"g/L\")\n\n    # Check platelets unit (note 10*9/L is not a typo)\n    rows = df[df[\"test_name\"] == \"platelets\"]\n    check_const_column(rows, \"unit\", \"10*9/L\")\n\n    # Some values include an inequality; e.g.:\n    # - egfr: &gt;90\n    # - platelets: &lt;3\n    #\n    # Remove instances of &lt; or &gt; to enable conversion\n    # to float.\n    df[\"result\"] = df[\"result\"].str.replace(\"&lt;|&gt;\", \"\", regex=True)\n\n    # Convert results column to float\n    df[\"result\"] = df[\"result\"].astype(float)\n\n    # Convert hb units to g/dL (to match ARC HBR definition)\n    df.loc[df[\"test_name\"] == \"hb\", \"result\"] /= 10.0\n\n    return df[[\"patient_id\", \"sample_date\", \"test_name\", \"result\"]]\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_hic.get_unlinked_prescriptions","title":"<code>get_unlinked_prescriptions(engine, table_name='cv1_pharmacy_prescribing')</code>","text":"<p>Get relevant prescriptions from the HIC data (unlinked to episode)</p> <p>This function is tailored towards the calculation of the ARC HBR score, so it focusses on prescriptions on oral anticoagulants (e.g. warfarin) and non-steroidal anti-inflammatory drugs (NSAIDs, e.g. ibuprofen).</p> <p>The frequency column reflects the maximum allowable doses per day. For the purposes of ARC HBR, where NSAIDs must be prescribed &gt; 4 days/week, all prescriptions in the HIC data indicate frequency &gt; 1 (i.e. at least one per day), and therefore qualify for ARC HBR purposes.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>The connection to the database</p> required <code>table_name</code> <code>str</code> <p>Defaults to \"cv1_pharmacy_prescribing\" for UHBW, but can be overwritten by \"HIC_Pharmacy\" for ICB.</p> <code>'cv1_pharmacy_prescribing'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The table of prescriptions, including the patient_id, order_date (to link to an episode), prescription name, prescription group (oac or nsaid), and frequency (in doses per day).</p> Source code in <code>src\\pyhbr\\middle\\from_hic.py</code> <pre><code>def get_unlinked_prescriptions(engine: Engine, table_name: str = \"cv1_pharmacy_prescribing\") -&gt; pd.DataFrame:\n    \"\"\"Get relevant prescriptions from the HIC data (unlinked to episode)\n\n    This function is tailored towards the calculation of the\n    ARC HBR score, so it focusses on prescriptions on oral\n    anticoagulants (e.g. warfarin) and non-steroidal\n    anti-inflammatory drugs (NSAIDs, e.g. ibuprofen).\n\n    The frequency column reflects the maximum allowable\n    doses per day. For the purposes of ARC HBR, where NSAIDs\n    must be prescribed &gt; 4 days/week, all prescriptions in\n    the HIC data indicate frequency &gt; 1 (i.e. at least one\n    per day), and therefore qualify for ARC HBR purposes.\n\n    Args:\n        engine: The connection to the database\n        table_name: Defaults to \"cv1_pharmacy_prescribing\" for UHBW, but can\n            be overwritten by \"HIC_Pharmacy\" for ICB.\n\n    Returns:\n        The table of prescriptions, including the patient_id,\n            order_date (to link to an episode), prescription name,\n            prescription group (oac or nsaid), and frequency (in\n            doses per day).\n    \"\"\"\n\n    df = get_data(engine, hic.pharmacy_prescribing_query, table_name)\n\n    # Create a new `group` column containing the medicine type\n    df = filter_by_medicine(df)\n\n    # Replace alternative spellings\n    df[\"name\"] = df[\"name\"].str.replace(\"indomethacin\", \"indometacin\")\n\n    # Replace admission medicine column with bool\n    on_admission_map = {\"y\": True, \"n\": False}\n    df[\"on_admission\"] = df[\"on_admission\"].map(on_admission_map)\n\n    # Extra spaces are not typos.\n    per_day = {\n        \"TWICE a day\": 2,\n        \"in the MORNING\": 1,\n        \"THREE times a day\": 3,\n        \"TWICE a day at 08:00 and 22:00\": 2,\n        \"ONCE a day  at 18:00\": 1,\n        \"up to every SIX hours\": 4,\n        \"up to every EIGHT hours\": 3,\n        \"TWICE a day at 08:00 and 20:00\": 2,\n        \"up to every 24 hours\": 1,\n        \"THREE times a day at 08:00 15:00 and 22:00\": 3,\n        \"TWICE a day at 08:00 and 19:00\": 2,\n        \"ONCE a day  at 20:00\": 1,\n        \"ONCE a day  at 08:00\": 1,\n        \"up to every 12 hours\": 2,\n        \"ONCE a day  at 19:00\": 1,\n        \"THREE times a day at 08:00 15:00 and 20:00\": 3,\n        \"THREE times a day at 08:00 14:00 and 22:00\": 3,\n        \"ONCE a day  at 22:00\": 1,\n        \"every EIGHT hours\": 24,\n        \"ONCE a day  at 09:00\": 1,\n        \"up to every FOUR hours\": 6,\n        \"TWICE a day at 06:00 and 18:00\": 2,\n        \"at NIGHT\": 1,\n        \"ONCE a day  at 14:00\": 1,\n        \"ONCE a day  at 12:00\": 1,\n        \"THREE times a day at 08:00 14:00 and 20:00\": 3,\n        \"THREE times a day at 00:00 08:00 and 16:00\": 3,\n    }\n\n    # Replace frequencies strings with doses per day\n    df[\"frequency\"] = df[\"frequency\"].map(per_day)\n\n    return df[\n        [\"patient_id\", \"order_date\", \"name\", \"group\", \"frequency\", \"on_admission\"]\n    ].reset_index(drop=True)\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_hic.link_to_episodes","title":"<code>link_to_episodes(items, episodes, date_col_name)</code>","text":"<p>Link HIC laboratory test/prescriptions to episode by date</p> <p>Use this function to add an episode_id to the laboratory tests table or the prescriptions table. Tests/prescriptions are generically referred to as items below.</p> <p>This function associates each item with the first episode containing the item date in its [episode_start, episode_end) range. The column containing the item date is given by <code>date_col_name</code>.</p> <p>For prescriptions, use the prescription order date for linking. For laboratory tests, use the sample collected date.</p> <p>This function assumes that the episode_id in the episodes table is unique (i.e. no patients share an episode ID).</p> <p>For higher performance, reduce the item table to items of interest before calling this function.</p> <p>Since episodes may slightly overlap, an item may be associated with more than one episode. In this case, the function will associate the item with the earliest episode (the returned table will not contain duplicate items).</p> <p>The final table does not use episode_id as an index, because an episode may contain multiple items.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>DataFrame</code> <p>The prescriptions or laboratory tests table. Must contain a <code>date_col_name</code> column, which is used to compare with episode start/end dates, and the <code>patient_id</code>.</p> required <code>episodes</code> <code>DataFrame</code> <p>The episodes table. Must contain <code>patient_id</code>, <code>episode_id</code>, <code>episode_start</code> and <code>episode_end</code>.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The items table with additional <code>episode_id</code> and <code>spell_id</code> columns.</p> Source code in <code>src\\pyhbr\\middle\\from_hic.py</code> <pre><code>def link_to_episodes(\n    items: pd.DataFrame, episodes: pd.DataFrame, date_col_name: str\n) -&gt; pd.DataFrame:\n    \"\"\"Link HIC laboratory test/prescriptions to episode by date\n\n    Use this function to add an episode_id to the laboratory tests\n    table or the prescriptions table. Tests/prescriptions are generically\n    referred to as items below.\n\n    This function associates each item with the first episode containing\n    the item date in its [episode_start, episode_end) range. The column\n    containing the item date is given by `date_col_name`.\n\n    For prescriptions, use the prescription order date for linking. For\n    laboratory tests, use the sample collected date.\n\n    This function assumes that the episode_id in the episodes table is\n    unique (i.e. no patients share an episode ID).\n\n    For higher performance, reduce the item table to items of interest\n    before calling this function.\n\n    Since episodes may slightly overlap, an item may be associated\n    with more than one episode. In this case, the function will associate\n    the item with the earliest episode (the returned table will\n    not contain duplicate items).\n\n    The final table does not use episode_id as an index, because an episode\n    may contain multiple items.\n\n    Args:\n        items: The prescriptions or laboratory tests table. Must contain a\n            `date_col_name` column, which is used to compare with episode\n            start/end dates, and the `patient_id`.\n\n        episodes: The episodes table. Must contain `patient_id`, `episode_id`,\n            `episode_start` and `episode_end`.\n\n    Returns:\n        The items table with additional `episode_id` and `spell_id` columns.\n    \"\"\"\n\n    # Before linking to episodes, add an item ID. This is to\n    # remove duplicated items in the last step of linking,\n    # due ot overlapping episode time windows.\n    items[\"item_id\"] = range(items.shape[0])\n\n    # Join together all items and episode information by patient. Use\n    # a left join on items (assuming items is narrowed to the item types\n    # of interest) to keep the result smaller. Reset the index to move\n    # episode_id to a column.\n    with_episodes = pd.merge(items, episodes.reset_index(), how=\"left\", on=\"patient_id\")\n\n    # Thinking of each row as both an episode and a item, drop any\n    # rows where the item date does not fall within the start\n    # and end of the episode (start date inclusive).\n    consistent_dates = (\n        with_episodes[date_col_name] &gt;= with_episodes[\"episode_start\"]\n    ) &amp; (with_episodes[date_col_name] &lt; with_episodes[\"episode_end\"])\n    overlapping_episodes = with_episodes[consistent_dates]\n\n    # Since some episodes overlap in time, some items will end up\n    # being associated with more than one episode. Remove any\n    # duplicates by associating only with the earliest episode.\n    deduplicated = (\n        overlapping_episodes.sort_values(\"episode_start\").groupby(\"item_id\").head(1)\n    )\n\n    # Keep episode_id, drop other episodes/unnecessary columns.\n    return deduplicated.drop(columns=[\"item_id\"]).drop(columns=episodes.columns)\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_icb","title":"<code>from_icb</code>","text":""},{"location":"reference/#pyhbr.middle.from_icb.blood_pressure","title":"<code>blood_pressure(swd_index_spells, primary_care_measurements)</code>","text":"<p>Get recent blood pressure readings</p> <p>Parameters:</p> Name Type Description Default <code>primary_care_measurements</code> <code>DataFrame</code> <p>Contains a <code>name</code> column containing the measurement name (expected to contain \"blood_pressure\"), a <code>result</code> column with the format \"systolic/diastolic\" for the blood pressure rows, a <code>date</code>, and a <code>patient_id</code>.</p> required <code>swd_index_spells</code> <code>DataFrame</code> <p>Has Pandas index <code>spell_id</code>, and columns <code>patient_id</code> and <code>spell_start</code>.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe index by <code>spell_id</code> containing <code>bp_systolic</code> and <code>bp_diastolic</code> columns.</p> Source code in <code>src\\pyhbr\\middle\\from_icb.py</code> <pre><code>def blood_pressure(\n    swd_index_spells: DataFrame, primary_care_measurements: DataFrame\n) -&gt; DataFrame:\n    \"\"\"Get recent blood pressure readings\n\n    Args:\n        primary_care_measurements: Contains a `name` column containing\n            the measurement name (expected to contain \"blood_pressure\"),\n            a `result` column with the format \"systolic/diastolic\" for\n            the blood pressure rows, a `date`, and a `patient_id`.\n        swd_index_spells: Has Pandas index `spell_id`, and columns\n            `patient_id` and `spell_start`.\n\n    Returns:\n        A dataframe index by `spell_id` containing `bp_systolic`\n            and `bp_diastolic` columns.\n    \"\"\"\n\n    df = primary_care_measurements\n\n    # Drop rows where the measurement is not known\n    df = df[~df[\"name\"].isna()]\n\n    # Drop rows where the prescription date is not known\n    df = df[~df[\"date\"].isna()]\n\n    blood_pressure = df[df.name.str.contains(\"blood_pressure\")][\n        [\"patient_id\", \"date\", \"result\"]\n    ].copy()\n    blood_pressure[[\"bp_systolic\", \"bp_diastolic\"]] = (\n        df[\"result\"].str.split(\"/\", expand=True).apply(pd.to_numeric, errors=\"coerce\")\n    )\n\n    # Join the prescriptions to the index spells\n    df = (\n        swd_index_spells[[\"spell_start\", \"patient_id\"]]\n        .reset_index()\n        .merge(blood_pressure, how=\"left\", on=\"patient_id\")\n    )\n    df[\"time_to_index_spell\"] = df[\"spell_start\"] - df[\"date\"]\n\n    # Only keep measurements occurring in the year before the index event\n    min_before = dt.timedelta(days=0)\n    max_before = dt.timedelta(days=60)\n    bp_before_index = counting.get_time_window(\n        df, -max_before, -min_before, \"time_to_index_spell\"\n    )\n\n    most_recent_bp = bp_before_index.sort_values(\"date\").groupby(\"spell_id\").tail(1)\n    prior_bp = swd_index_spells.merge(\n        most_recent_bp, how=\"left\", on=\"spell_id\"\n    ).set_index(\"spell_id\")[[\"bp_systolic\", \"bp_diastolic\"]]\n\n    return prior_bp\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_icb.get_clinical_codes","title":"<code>get_clinical_codes(raw_sus_data, code_groups)</code>","text":"<p>Get clinical codes in long format and normalised form.</p> <p>Each row is a code that is contained in some group. Codes in an episode are dropped if they are not in any group, meaning episodes will be dropped if no code in that episode is in any group. </p> <p>Parameters:</p> Name Type Description Default <code>raw_sus_data</code> <code>DataFrame</code> <p>Must contain one row per episode, and contains clinical codes in wide format, with columns <code>diagnosis_n</code> and <code>procedure_n</code>, for n &gt; 0. The value n == 1 is the primary diagnosis or procedure, and n &gt; 1 is for secondary codes.</p> required <code>code_groups</code> <code>DataFrame</code> <p>A table of all the codes in any group, at least containing columns <code>code</code>, <code>group</code> and <code>type</code>.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A table containing diagnoses/procedures, normalised codes, code groups, diagnosis positions, and associated episode ID.</p> Source code in <code>src\\pyhbr\\middle\\from_icb.py</code> <pre><code>def get_clinical_codes(\n    raw_sus_data: DataFrame, code_groups: DataFrame\n) -&gt; DataFrame:\n    \"\"\"Get clinical codes in long format and normalised form.\n\n    Each row is a code that is contained in some group. Codes in\n    an episode are dropped if they are not in any group, meaning\n    episodes will be dropped if no code in that episode is in any\n    group. \n\n    Args:\n        raw_sus_data: Must contain one row per episode, and\n            contains clinical codes in wide format, with\n            columns `diagnosis_n` and `procedure_n`, for\n            n &gt; 0. The value n == 1 is the primary diagnosis\n            or procedure, and n &gt; 1 is for secondary codes.\n        code_groups: A table of all the codes in any group, at least containing\n            columns `code`, `group` and `type`.\n\n    Returns:\n        A table containing diagnoses/procedures, normalised codes, code groups,\n            diagnosis positions, and associated episode ID.\n    \"\"\"\n\n    # Get all the clinical codes for all episodes in long format\n    long_codes = get_long_clinical_codes(raw_sus_data)\n\n    # Join all the code groups, and drop any codes that are not in any\n    # group (inner join in order to retain all keep all codes in long_codes,\n    # but only if they have an entry in code_groups)\n    return long_codes.merge(code_groups, on=[\"code\", \"type\"], how=\"inner\")\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_icb.get_episodes","title":"<code>get_episodes(raw_sus_data)</code>","text":"<p>Get the episodes table</p> <p>Age and gender are also included in each row.</p> <p>Gender is encoded using the NHS data dictionary values, which is mapped to a category column in the table. (Note that initial values are strings, not integers.)</p> <ul> <li>\"0\": Not known. Mapped to \"unknown\"</li> <li>\"1\": Male: Mapped to \"male\"</li> <li>\"2\": Female. Mapped to \"female\"</li> <li>\"9\": Not specified. Mapped to \"unknown\".</li> </ul> <p>Not mapping 0/9 to NA in case either is related to non-binary genders (i.e. it contains information, rather than being a NULL field).</p> <p>Parameters:</p> Name Type Description Default <code>raw_sus_data</code> <code>DataFrame</code> <p>Data returned by sus_query() query.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe indexed by <code>episode_id</code>, with columns <code>episode_start</code>, <code>spell_id</code> and <code>patient_id</code>.</p> Source code in <code>src\\pyhbr\\middle\\from_icb.py</code> <pre><code>def get_episodes(raw_sus_data: DataFrame) -&gt; DataFrame:\n    \"\"\"Get the episodes table\n\n    Age and gender are also included in each row.\n\n    Gender is encoded using the NHS data dictionary values, which\n    is mapped to a category column in the table. (Note that initial\n    values are strings, not integers.)\n\n    * \"0\": Not known. Mapped to \"unknown\"\n    * \"1\": Male: Mapped to \"male\"\n    * \"2\": Female. Mapped to \"female\"\n    * \"9\": Not specified. Mapped to \"unknown\".\n\n    Not mapping 0/9 to NA in case either is related to non-binary\n    genders (i.e. it contains information, rather than being a NULL field).\n\n    Args:\n        raw_sus_data: Data returned by sus_query() query.\n\n    Returns:\n        A dataframe indexed by `episode_id`, with columns\n            `episode_start`, `spell_id` and `patient_id`.\n    \"\"\"\n    df = (\n        raw_sus_data[[\"spell_id\", \"patient_id\", \"episode_start\", \"admission\", \"discharge\", \"age\", \"gender\"]]\n        .reset_index(names=\"episode_id\")\n        .set_index(\"episode_id\")\n    )\n\n    # Convert gender to categories\n    df[\"gender\"] = df[\"gender\"].replace(\"9\", \"0\")\n    valid_values = [\"0\", \"1\", \"2\"]\n    df.loc[~df[\"gender\"].isin(valid_values), \"gender\"] = \"0\"\n    df[\"gender\"] = df[\"gender\"].astype(\"category\")\n    df[\"gender\"] = df[\"gender\"].cat.rename_categories(\n        {\"0\": \"unknown\", \"1\": \"male\", \"2\": \"female\"}\n    )\n\n    # Convert age to numerical\n    df[\"age\"] = df[\"age\"].astype(float)\n\n    return df\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_icb.get_episodes_and_codes","title":"<code>get_episodes_and_codes(raw_sus_data, code_groups)</code>","text":"<p>Get episode and clinical code data</p> <p>This batch of data must be fetched first to find index events, which establishes the patient group of interest. This can then be used to narrow subsequent queries to the data base, to speed them up.</p> <p>Parameters:</p> Name Type Description Default <code>raw_sus_data</code> <code>DataFrame</code> <p>The raw HES data returned by get_raw_sus_data()</p> required <code>code_groups</code> <code>DataFrame</code> <p>A table of all the codes in any group, at least containing columns <code>code</code>, <code>group</code> and <code>type</code>.</p> required <p>Returns:</p> Type Description <code>(DataFrame, DataFrame)</code> <p>A tuple containing the episodes table (also contains age and gender) and the codes table containing the clinical code data in long format for any code that is in a diagnosis or  procedure code group.</p> Source code in <code>src\\pyhbr\\middle\\from_icb.py</code> <pre><code>def get_episodes_and_codes(raw_sus_data: DataFrame, code_groups: DataFrame) -&gt; (DataFrame, DataFrame):\n    \"\"\"Get episode and clinical code data\n\n    This batch of data must be fetched first to find index events,\n    which establishes the patient group of interest. This can then\n    be used to narrow subsequent queries to the data base, to speed\n    them up.\n\n    Args:\n        raw_sus_data: The raw HES data returned by get_raw_sus_data()\n        code_groups: A table of all the codes in any group, at least containing\n            columns `code`, `group` and `type`.\n\n    Returns:\n        A tuple containing the episodes table (also contains age and\n            gender) and the codes table containing the clinical code data\n            in long format for any code that is in a diagnosis or \n            procedure code group.\n    \"\"\"\n\n    # Compared to the data fetch, this part is relatively fast, but still very\n    # slow (approximately 10% of the total runtime).\n    episodes = get_episodes(raw_sus_data)\n    codes = get_clinical_codes(raw_sus_data, code_groups)\n\n    return episodes, codes\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_icb.get_long_cause_of_death","title":"<code>get_long_cause_of_death(mortality)</code>","text":"<p>Get cause-of-death diagnosis codes in normalised long format</p> <p>Parameters:</p> Name Type Description Default <code>mortality</code> <code>DataFrame</code> <p>A table containing <code>patient_id</code>, and columns  with names <code>cause_of_death_n</code>, where n is an integer 1, 2, ...</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A table containing the columns <code>patient_id</code>, <code>code</code> (for ICD-10 cause of death diagnosis), and <code>position</code> (for primary/secondary position of the code)</p> Source code in <code>src\\pyhbr\\middle\\from_icb.py</code> <pre><code>def get_long_cause_of_death(mortality: DataFrame) -&gt; DataFrame:\n    \"\"\"Get cause-of-death diagnosis codes in normalised long format\n\n    Args:\n        mortality: A table containing `patient_id`, and columns \n            with names `cause_of_death_n`, where n is an integer 1, 2, ...\n\n    Returns:\n        A table containing the columns `patient_id`, `code` (for ICD-10\n            cause of death diagnosis), and `position` (for primary/secondary\n            position of the code)\n    \"\"\"\n    df = mortality.filter(regex=\"(id|cause)\").melt(id_vars=\"patient_id\")\n    df[\"position\"] = df[\"variable\"].str.split(\"_\", expand=True).iloc[:, -1].astype(int)\n    df = df[~df[\"value\"].isna()]\n    df[\"code\"] = df[\"value\"].apply(clinical_codes.normalise_code)\n    return df[[\"patient_id\", \"code\", \"position\"]]\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_icb.get_long_clinical_codes","title":"<code>get_long_clinical_codes(raw_sus_data)</code>","text":"<p>Get a table of the clinical codes in normalised long format</p> <p>This is modelled on the format of the HIC data, which works well, and makes it possible to re-use the code for processing that table.</p> <p>Parameters:</p> Name Type Description Default <code>raw_sus_data</code> <code>DataFrame</code> <p>Must contain one row per episode, and contains clinical codes in wide format, with columns <code>diagnosis_n</code> and <code>procedure_n</code>, for n &gt; 0. The value n == 1 is the primary diagnosis or procedure, and n &gt; 1 is for secondary codes.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A table containing <code>episode_id</code>, <code>code</code>, <code>type</code>, and position.</p> Source code in <code>src\\pyhbr\\middle\\from_icb.py</code> <pre><code>def get_long_clinical_codes(raw_sus_data: DataFrame) -&gt; DataFrame:\n    \"\"\"Get a table of the clinical codes in normalised long format\n\n    This is modelled on the format of the HIC data, which works\n    well, and makes it possible to re-use the code for processing\n    that table.\n\n    Args:\n        raw_sus_data: Must contain one row per episode, and\n            contains clinical codes in wide format, with\n            columns `diagnosis_n` and `procedure_n`, for\n            n &gt; 0. The value n == 1 is the primary diagnosis\n            or procedure, and n &gt; 1 is for secondary codes.\n\n    Returns:\n        A table containing `episode_id`, `code`, `type`, and\n            position.\n    \"\"\"\n\n    # Pivot the wide format to long based on the episode_id\n    df = (\n        raw_sus_data.reset_index(names=\"episode_id\")\n        .filter(regex=\"(diagnosis|procedure|episode_id)\")\n        .melt(id_vars=\"episode_id\", value_name=\"code\")\n    )\n\n    # Drop any codes that are empty or whitespace\n    long_codes = df[~df[\"code\"].str.isspace() &amp; (df[\"code\"] != \"\")].copy()\n\n    # Convert the diagnosis/procedure and value of n into separate columns\n    long_codes[[\"type\", \"position\"]] = long_codes[\"variable\"].str.split(\n        \"_\", expand=True\n    )\n\n    long_codes[\"position\"] = long_codes[\"position\"].astype(int)\n    long_codes[\"code\"] = long_codes[\"code\"].apply(clinical_codes.normalise_code)\n\n    # Collect columns of interest and sort for ease of viewing\n    return (\n        long_codes[[\"episode_id\", \"code\", \"type\", \"position\"]]\n        .sort_values([\"episode_id\", \"type\", \"position\"])\n        .reset_index(drop=True)\n    )\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_icb.get_mortality","title":"<code>get_mortality(engine, start_date, end_date, code_groups)</code>","text":"<p>Get date of death and cause of death</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>The connection to the database</p> required <code>start_date</code> <code>date</code> <p>First date of death that will be included</p> required <code>end_date</code> <code>date</code> <p>Last date of death that will be included</p> required <code>code_groups</code> <code>DataFrame</code> <p>A table of all the codes in any group, at least containing columns <code>code</code>, <code>group</code> and <code>type</code>.</p> required <p>Returns:</p> Type Description <code>dict[str, DataFrame]</code> <p>A tuple containing a date of death table, which is indexed by <code>patient_id</code> and has the single column <code>date_of_death</code>, and a cause of death table with columns <code>patient_id</code>, <code>code</code> for the cause of death diagnosis code (ICD-10), and <code>position</code> indicating the primary/secondary position of the code (1 is primary, &gt;1 is secondary).</p> Source code in <code>src\\pyhbr\\middle\\from_icb.py</code> <pre><code>def get_mortality(engine: Engine, start_date: date, end_date: date, code_groups: DataFrame) -&gt; dict[str, DataFrame]:\n    \"\"\"Get date of death and cause of death\n\n    Args:\n        engine: The connection to the database\n        start_date: First date of death that will be included\n        end_date: Last date of death that will be included\n        code_groups: A table of all the codes in any group, at least containing\n            columns `code`, `group` and `type`.\n\n    Returns:\n        A tuple containing a date of death table, which is indexed by `patient_id`\n            and has the single column `date_of_death`, and a cause of death table\n            with columns `patient_id`, `code` for the cause of death\n            diagnosis code (ICD-10), and `position` indicating the primary/secondary\n            position of the code (1 is primary, &gt;1 is secondary).\n    \"\"\"\n\n    # Fetch the mortality data limited by the date range\n    raw_mortality_data = common.get_data(engine, icb.mortality_query, start_date, end_date)\n\n    # Some patient IDs have multiple inconsistent death records. For these cases,\n    # pick the most recent record. This will ensure that no patients recorded in the\n    # mortality tables are dropped, at the expense of some potential inaccuracies in\n    # the date of death.\n    mortality = raw_mortality_data.sort_values(\"date_of_death\").groupby(\"patient_id\").tail(1)\n\n    # Get the date of death.\n    date_of_death = mortality.set_index(\"patient_id\")[[\"date_of_death\"]]\n\n    # Convert the cause of death to a long format, normalise the codes,\n    # and keep only the code and position for each patient.\n    long_cause_of_death = get_long_cause_of_death(mortality)\n\n    # Join the code groups to the codes (does not filter -- leaves\n    # NA group for a code not in any group).\n    diagnosis_code_groups = code_groups[code_groups[\"type\"] == \"diagnosis\"]\n    cause_of_death = long_cause_of_death.merge(\n        diagnosis_code_groups, on=\"code\", how=\"inner\"\n    ).sort_values([\"patient_id\", \"position\"]).reset_index(drop=True)\n\n    return date_of_death, cause_of_death\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_icb.get_raw_sus_data","title":"<code>get_raw_sus_data(engine, start_date, end_date)</code>","text":"<p>Get the raw SUS (secondary uses services hospital episode statistics)</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>The connection to the database</p> required <code>start_date</code> <code>date</code> <p>The start date (inclusive) for returned episodes</p> required <code>end_date</code> <code>date</code> <p>The end date (inclusive) for returned episodes</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe with one row per episode, containing clinical code data and patient demographics at that episode.</p> Source code in <code>src\\pyhbr\\middle\\from_icb.py</code> <pre><code>def get_raw_sus_data(engine: Engine, start_date: date, end_date: date) -&gt; DataFrame:\n    \"\"\"Get the raw SUS (secondary uses services hospital episode statistics)\n\n    Args:\n        engine: The connection to the database\n        start_date: The start date (inclusive) for returned episodes\n        end_date:  The end date (inclusive) for returned episodes\n\n    Returns:\n        A dataframe with one row per episode, containing clinical code\n            data and patient demographics at that episode.\n    \"\"\"\n\n    # The fetch is very slow (and varies depending on the internet connection).\n    # Fetching 5 years of data takes approximately 20 minutes (about 2m episodes).\n    print(\"Starting SUS data fetch...\")\n    raw_sus_data = common.get_data(engine, icb.sus_query, start_date, end_date)\n    print(\"SUS data fetch finished.\")\n\n    return raw_sus_data\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_icb.get_unlinked_lab_results","title":"<code>get_unlinked_lab_results(engine)</code>","text":"<p>Get laboratory results from the HIC database (unlinked to episode)</p> <p>This function returns data for the following three tests, identified by one of these values in the <code>test_name</code> column:</p> <ul> <li><code>hb</code>: haemoglobin (unit: g/dL)</li> <li><code>egfr</code>: eGFR (unit: mL/min)</li> <li><code>platelets</code>: platelet count (unit: 10^9/L)</li> </ul> <p>The test result is associated to a <code>patient_id</code>, and the time when the sample for the test was collected is stored in the <code>sample_date</code> column.</p> <p>Some values in the underlying table contain inequalities in the results column, which have been removed (so egfr &gt;90 becomes 90).</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>The connection to the database</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Table of laboratory results, including Hb (haemoglobin), platelet count, and eGFR (kidney function). The columns are <code>patient_id</code>, <code>test_name</code>, and <code>sample_date</code>.</p> Source code in <code>src\\pyhbr\\middle\\from_icb.py</code> <pre><code>def get_unlinked_lab_results(engine: Engine) -&gt; pd.DataFrame:\n    \"\"\"Get laboratory results from the HIC database (unlinked to episode)\n\n    This function returns data for the following three\n    tests, identified by one of these values in the\n    `test_name` column:\n\n    * `hb`: haemoglobin (unit: g/dL)\n    * `egfr`: eGFR (unit: mL/min)\n    * `platelets`: platelet count (unit: 10^9/L)\n\n    The test result is associated to a `patient_id`,\n    and the time when the sample for the test was collected\n    is stored in the `sample_date` column.\n\n    Some values in the underlying table contain inequalities\n    in the results column, which have been removed (so\n    egfr &gt;90 becomes 90).\n\n    Args:\n        engine: The connection to the database\n\n    Returns:\n        Table of laboratory results, including Hb (haemoglobin),\n            platelet count, and eGFR (kidney function). The columns are\n            `patient_id`, `test_name`, and `sample_date`.\n\n    \"\"\"\n\n    test_of_interest = {\n        \"Haemoglobin\": \"hb\",\n        \"eGFR/1.73m2 (CKD-EPI)\": \"egfr\",\n        \"Platelets\": \"platelets\",\n    }\n\n    df = common.get_data(engine, hic_icb.pathology_blood_query, test_of_interest.keys())\n\n    # Only keep tests of interest: platelets, egfr, and hb\n    df = df[df[\"test_name\"].isin(test_of_interest.keys())]\n\n    # Rename the items\n    df[\"test_name\"] = df[\"test_name\"].map(test_of_interest)\n\n    # Check egfr unit\n    rows = df[df[\"test_name\"] == \"egfr\"]\n    check_const_column(rows, \"unit\", \"mL/min\")\n\n    # Check hb unit\n    rows = df[df[\"test_name\"] == \"hb\"]\n    check_const_column(rows, \"unit\", \"g/L\")\n\n    # Check platelets unit (note 10*9/L is not a typo)\n    rows = df[df[\"test_name\"] == \"platelets\"]\n    check_const_column(rows, \"unit\", \"10*9/L\")\n\n    # Some values include an inequality; e.g.:\n    # - egfr: &gt;90\n    # - platelets: &lt;3\n    #\n    # Remove instances of &lt; or &gt; to enable conversion\n    # to float.\n    df[\"result\"] = df[\"result\"].str.replace(\"&lt;|&gt;\", \"\", regex=True)\n\n    # Convert results column to float\n    df[\"result\"] = df[\"result\"].astype(float)\n\n    # Convert hb units to g/dL (to match ARC HBR definition)\n    df.loc[df[\"test_name\"] == \"hb\", \"result\"] /= 10.0\n\n    return df[[\"patient_id\", \"sample_date\", \"test_name\", \"result\"]]\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_icb.hba1c","title":"<code>hba1c(swd_index_spells, primary_care_measurements)</code>","text":"<p>Get recent HbA1c from the primary care measurements</p> <p>Parameters:</p> Name Type Description Default <code>primary_care_measurements</code> <code>DataFrame</code> <p>Contains a <code>name</code> column containing the measurement name (expected to contain \"blood_pressure\"), a <code>result</code> column with the format \"systolic/diastolic\" for the blood pressure rows, a <code>date</code>, and a <code>patient_id</code>.</p> required <code>swd_index_spells</code> <code>DataFrame</code> <p>Has Pandas index <code>spell_id</code>, and columns <code>patient_id</code> and <code>spell_start</code>.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe indexed by <code>spell_id</code> containing recent (within 2 months) HbA1c values.</p> Source code in <code>src\\pyhbr\\middle\\from_icb.py</code> <pre><code>def hba1c(\n    swd_index_spells: DataFrame, primary_care_measurements: DataFrame\n) -&gt; DataFrame:\n    \"\"\"Get recent HbA1c from the primary care measurements\n\n    Args:\n        primary_care_measurements: Contains a `name` column containing\n            the measurement name (expected to contain \"blood_pressure\"),\n            a `result` column with the format \"systolic/diastolic\" for\n            the blood pressure rows, a `date`, and a `patient_id`.\n        swd_index_spells: Has Pandas index `spell_id`, and columns\n            `patient_id` and `spell_start`.\n\n    Returns:\n        A dataframe indexed by `spell_id` containing recent (within 2 months)\n            HbA1c values.\n    \"\"\"\n\n    df = primary_care_measurements\n\n    # Drop rows where the measurement is not known\n    df = df[~df[\"name\"].isna()]\n\n    # Drop rows where the prescription date is not known\n    df = df[~df[\"date\"].isna()]\n\n    hba1c = df[df.name.str.contains(\"hba1c\")][[\"patient_id\", \"date\", \"result\"]].copy()\n    hba1c[\"hba1c\"] = pd.to_numeric(hba1c[\"result\"], errors=\"coerce\")\n\n    # Join the prescriptions to the index spells\n    df = (\n        swd_index_spells[[\"spell_start\", \"patient_id\"]]\n        .reset_index()\n        .merge(hba1c, how=\"left\", on=\"patient_id\")\n    )\n    df[\"time_to_index_spell\"] = df[\"spell_start\"] - df[\"date\"]\n\n    # Only keep measurements occurring in the year before the index event\n    min_before = dt.timedelta(days=0)\n    max_before = dt.timedelta(days=60)\n    hba1c_before_index = counting.get_time_window(\n        df, -max_before, -min_before, \"time_to_index_spell\"\n    )\n\n    most_recent_hba1c = (\n        hba1c_before_index.sort_values(\"date\").groupby(\"spell_id\").tail(1)\n    )\n    prior_hba1c = swd_index_spells.merge(\n        most_recent_hba1c, how=\"left\", on=\"spell_id\"\n    ).set_index(\"spell_id\")[[\"hba1c\"]]\n\n    return prior_hba1c\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_icb.preprocess_ethnicity","title":"<code>preprocess_ethnicity(column)</code>","text":"<p>Map the ethnicity column to standard ethnicities.</p> <p>Ethnicities were obtained from www.ethnicity-facts-figures.service.gov.uk/style-guide/ethnic-groups, from the 2021 census:</p> <ul> <li>asian_or_asian_british</li> <li>black_black_british_caribbean_or_african</li> <li>mixed_or_multiple_ethnic_groups</li> <li>white</li> <li>other_ethnic_group</li> </ul> <p>Parameters:</p> Name Type Description Default <code>column</code> <code>Series</code> <p>A column of object (\"string\") containing ethnicities from the primary care attributes table.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>A column of type category containing the standard ethnicities (and NaN).</p> Source code in <code>src\\pyhbr\\middle\\from_icb.py</code> <pre><code>def preprocess_ethnicity(column: Series) -&gt; Series:\n    \"\"\"Map the ethnicity column to standard ethnicities.\n\n    Ethnicities were obtained from www.ethnicity-facts-figures.service.gov.uk/style-guide/ethnic-groups,\n    from the 2021 census:\n\n    * asian_or_asian_british\n    * black_black_british_caribbean_or_african\n    * mixed_or_multiple_ethnic_groups\n    * white\n    * other_ethnic_group\n\n    Args:\n        column: A column of object (\"string\") containing\n            ethnicities from the primary care attributes table.\n\n    Returns:\n        A column of type category containing the standard\n            ethnicities (and NaN).\n    \"\"\"\n\n    column = column.str.replace(\" - ethnic category 2001 census\", \"\")\n    column = column.str.replace(\" - England and Wales ethnic category 2011 census\", \"\")\n    column = column.str.replace(\" - 2011 census England and Wales\", \"\")\n    column = column.str.replace(\" - Scotland ethnic category 2011 census\", \"\")\n    column = column.str.replace(\" - 2001 census\", \"\")\n    column = column.str.lower()\n    column = column.str.replace(\"(\\(|\\)|:| )+\", \"_\", regex=True)\n\n    ethnicity_map = {\n        \"white_british\": \"white\",\n        \"british_or_mixed_british\": \"white\",\n        \"white_english_or_welsh_or_scottish_or_northern_irish_or_british\": \"white\",\n        \"english\": \"white\",\n        \"other_white_background\": \"white\",\n        \"white\": \"white\",\n        \"ethnic_category_not_stated\": np.nan,\n        \"pakistani_or_british_pakistani\": \"asian_or_asian_british\",\n        \"refusal_by_patient_to_provide_information_about_ethnic_group\": np.nan,\n        \"ethnic_category\": np.nan,\n        \"indian_or_british_indian\": \"asian_or_asian_british\",\n        \"caribbean\": \"black_black_british_caribbean_or_african\",\n        \"other_asian_background\": \"asian_or_asian_british\",\n        \"african\": \"black_black_british_caribbean_or_african\",\n        \"white_any_other_white_background\": \"white\",\n        \"bangladeshi_or_british_bangladeshi\": \"asian_or_asian_british\",\n        \"irish\": \"white\",\n        \"white_irish\": \"white\",\n        \"white_-_ethnic_group\": \"white\",\n        \"chinese\": \"asian_or_asian_british\",\n        \"polish\": \"white\",\n        \"black_british\": \"black_black_british_caribbean_or_african\",\n        \"white_and_black_caribbean\": \"mixed_or_multiple_ethnic_groups\",\n        \"pakistani\": \"asian_or_asian_british\",\n        \"other\": \"other_ethnic_group\",\n        \"black_african\": \"black_black_british_caribbean_or_african\",\n        \"asian_or_asian_british_indian\": \"asian_or_asian_british\",\n        \"black_caribbean\": \"black_black_british_caribbean_or_african\",\n        \"indian\": \"asian_or_asian_british\",\n        \"asian_or_asian_british_pakistani\": \"asian_or_asian_british\",\n        \"other_white_european_or_european_unspecified_or_mixed_european\": \"white\",\n        \"somali\": \"black_black_british_caribbean_or_african\",\n        \"ethnic_group_not_recorded\": np.nan,\n        \"asian_or_asian_british_any_other_asian_background\": \"asian_or_asian_british\",\n        \"white_and_asian\": \"mixed_or_multiple_ethnic_groups\",\n        \"white_and_black_african\": \"mixed_or_multiple_ethnic_groups\",\n        \"other_black_background\": \"black_black_british_caribbean_or_african\",\n        \"italian\": \"white\",\n        \"scottish\": \"white\",\n        \"other_white_or_white_unspecified\": \"white\",\n        \"other_ethnic_group_any_other_ethnic_group\": \"other_ethnic_group\",\n        \"other_mixed_background\": \"mixed_or_multiple_ethnic_groups\",\n        \"other_european_nmo_\": \"white\",\n        \"welsh\": \"white\",\n        \"greek\": \"white\",\n        \"patient_ethnicity_unknown\": np.nan,\n        \"mixed_multiple_ethnic_groups_any_other_mixed_or_multiple_ethnic_background\": \"mixed_or_multiple_ethnic_groups\",\n        \"black_or_african_or_caribbean_or_black_british_caribbean\": \"black_black_british_caribbean_or_african\",\n        \"filipino\": \"asian_or_asian_british\",\n        \"ethnic_group\": np.nan,\n        \"other_mixed_white\": \"white\",  # Unclear\n        \"british_asian\": \"asian_or_asian_british\",\n        \"iranian\": \"other_ethnic_group\",\n        \"other_asian_ethnic_group\": \"asian_or_asian_british\",\n        \"kurdish\": \"other_ethnic_group\",\n        \"black_or_african_or_caribbean_or_black_british_african\": \"black_black_british_caribbean_or_african\",\n        \"other_asian_nmo_\": \"asian_or_asian_british\",\n        \"moroccan\": \"other_ethnic_group\",\n        \"other_white_british_ethnic_group\": \"white\",\n        \"mixed_multiple_ethnic_groups_white_and_black_caribbean\": \"mixed_or_multiple_ethnic_groups\",\n        \"black_and_white\": \"mixed_or_multiple_ethnic_groups\",\n        \"asian_or_asian_british_bangladeshi\": \"asian_or_asian_british\",\n        \"mixed_multiple_ethnic_groups_white_and_black_african\": \"mixed_or_multiple_ethnic_groups\",\n        \"white_polish\": \"white\",\n        \"asian_and_chinese\": \"asian_or_asian_british\",\n        \"black_or_african_or_caribbean_or_black_british_other_black_or_african_or_caribbean_background\": \"black_black_british_caribbean_or_african\",\n        \"black_and_asian\": \"black_black_british_caribbean_or_african\",\n        \"white_scottish\": \"white\",\n        \"any_other_group\": \"other_ethnic_group\",\n        \"other_ethnic_non-mixed_nmo_\": \"other_ethnic_group\",\n        \"ethnicity_and_other_related_nationality_data\": np.nan,\n        \"caucasian_race\": \"white\",\n        \"multi-ethnic_islands_mauritian_or_seychellois_or_maldivian_or_st_helena\": \"other_ethnic_group\",\n        \"punjabi\": \"asian_or_asian_british\",\n        \"albanian\": \"white\",\n        \"turkish/turkish_cypriot_nmo_\": \"other_ethnic_group\",\n        \"black_-_other_african_country\": \"black_black_british_caribbean_or_african\",\n        \"other_black_or_black_unspecified\": \"black_black_british_caribbean_or_african\",\n        \"sri_lankan\": \"asian_or_asian_british\",\n        \"mixed_asian\": \"asian_or_asian_british\",\n        \"other_black_ethnic_group\": \"black_black_british_caribbean_or_african\",\n        \"bulgarian\": \"white\",\n        \"sikh\": \"asian_or_asian_british\",\n        \"other_ethnic_mixed_origin\": \"other_ethnic_group\",\n        \"n_african_arab/iranian_nmo_\": \"other_ethnic_group\",\n        \"south_and_central_american\": \"other_ethnic_group\",\n        \"asian_or_asian_british_chinese\": \"asian_or_asian_british\",\n        \"ethnic_groups_census_nos\": np.nan,\n        \"arab\": \"other_ethnic_group\",\n        \"ethnic_group_finding\": np.nan,\n        \"white_any_other_white_ethnic_group\": \"white\",\n        \"greek_cypriot\": \"white\",\n        \"latin_american\": \"other_ethnic_group\",\n        \"other_asian_or_asian_unspecified\": \"asian_or_asian_british\",\n        \"cypriot_part_not_stated_\": \"other_ethnic_group\",\n        \"east_african_asian\": \"other_ethnic_group\",\n        \"mixed_multiple_ethnic_groups_white_and_asian\": \"mixed_or_multiple_ethnic_groups\",\n        \"other_ethnic_group_arab_arab_scottish_or_arab_british\": \"other_ethnic_group\",\n        \"other_ethnic_group_arab\": \"other_ethnic_group\",\n        \"turkish\": \"other_ethnic_group\",\n        \"north_african\": \"black_black_british_caribbean_or_african\",\n        \"greek_nmo_\": \"white\",\n        \"bangladeshi\": \"asian_or_asian_british\",\n        \"chinese_and_white\": \"mixed_or_multiple_ethnic_groups\",\n        \"white_gypsy_or_irish_traveller\": \"white\",\n        \"vietnamese\": \"asian_or_asian_british\",\n        \"romanian\": \"white\",\n        \"serbian\": \"white\",\n    }\n\n    return column.map(ethnicity_map).astype(\"category\")\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_icb.preprocess_smoking","title":"<code>preprocess_smoking(column)</code>","text":"<p>Convert the smoking column from string to category</p> <p>The values in the column are \"unknown\", \"ex\", \"Unknown\", \"current\", \"Smoker\", \"Ex\", and \"Never\".</p> <p>Based on the distribution of values in the column, it likely that \"Unknown/unknown\" mostly means \"no\". This makes the percentage of smoking about 15%, which is roughly in line with the average. Without performing this mapping, smokers outnumber non-smokers (\"Never\") approx. 20 to 1.</p> <p>Note that the column does also include NA values, which will be left as NA.</p> <p>Parameters:</p> Name Type Description Default <code>column</code> <code>Series</code> <p>The smoking column from the primary care attributes</p> required <p>Returns:</p> Type Description <code>Series</code> <p>A category column containing \"yes\", \"no\", and \"ex\".</p> Source code in <code>src\\pyhbr\\middle\\from_icb.py</code> <pre><code>def preprocess_smoking(column: Series) -&gt; Series:\n    \"\"\"Convert the smoking column from string to category\n\n    The values in the column are \"unknown\", \"ex\", \"Unknown\",\n    \"current\", \"Smoker\", \"Ex\", and \"Never\".\n\n    Based on the distribution of values in the column, it\n    likely that \"Unknown/unknown\" mostly means \"no\". This\n    makes the percentage of smoking about 15%, which is\n    roughly in line with the average. Without performing this\n    mapping, smokers outnumber non-smokers (\"Never\") approx.\n    20 to 1.\n\n    Note that the column does also include NA values, which\n    will be left as NA.\n\n    Args:\n        column: The smoking column from the primary\n            care attributes\n\n    Returns:\n        A category column containing \"yes\", \"no\", and \"ex\".\n    \"\"\"\n\n    value_map = {\n        \"unknown\": \"no\",\n        \"Unknown\": \"no\",\n        \"current\": \"yes\",\n        \"Smoker\": \"yes\",\n        \"ex\": \"ex\",\n        \"Ex\": \"ex\",\n        \"Never\": \"no\",\n    }\n\n    return column.map(value_map).astype(\"category\")\n</code></pre>"},{"location":"reference/#pyhbr.middle.from_icb.process_flag_columns","title":"<code>process_flag_columns(primary_care_attributes)</code>","text":"<p>Replace NaN with false and convert to bool for a selection of rows</p> <p>Many columns in the primary care attributes encode a flag using 1 for true and NA/NULL for false. These must be replaced with a boolean type so that NA can distinguish missing data.  Instead of using a <code>bool</code>, use Int8 so that NaNs can be stored. (This is important later on for index spells with missing attributes, which need to store NaN in these flag columns.)</p> <p>Parameters:</p> Name Type Description Default <code>primary_care_attributes</code> <code>DataFrame</code> <p>Original table containing 1/NA flag columns</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The primary care attributes with flag columns encoded as Int8.</p> Source code in <code>src\\pyhbr\\middle\\from_icb.py</code> <pre><code>def process_flag_columns(primary_care_attributes: DataFrame) -&gt; DataFrame:\n    \"\"\"Replace NaN with false and convert to bool for a selection of rows\n\n    Many columns in the primary care attributes encode a flag\n    using 1 for true and NA/NULL for false. These must be replaced\n    with a boolean type so that NA can distinguish missing data. \n    Instead of using a `bool`, use Int8 so that NaNs can be stored.\n    (This is important later on for index spells with missing attributes,\n    which need to store NaN in these flag columns.)\n\n    Args:\n        primary_care_attributes: Original table containing\n            1/NA flag columns\n\n    Returns:\n        The primary care attributes with flag columns encoded\n            as Int8.\n\n    \"\"\"\n\n    # Columns interpreted as flags have been taken from\n    # the SWD guide, where the data format column says\n    # 1/Null. SWD documentation has been taken as a proxy\n    # for the primary care attributes table (which does\n    # not have column documentation).\n    flag_columns = [\n        \"abortion\",\n        \"adhd\",\n        \"af\",\n        \"amputations\",\n        \"anaemia_iron\",\n        \"anaemia_other\",\n        \"angio_anaph\",\n        \"arrhythmia_other\",\n        \"asthma\",\n        \"autism\",\n        \"back_pain\",\n        \"cancer_bladder\",\n        # Not sure what *_year means as a flag\n        \"cancer_bladder_year\",\n        \"cancer_bowel\",\n        \"cancer_bowel_year\",\n        \"cancer_breast\",\n        \"cancer_breast_year\",\n        \"cancer_cervical\",\n        \"cancer_cervical_year\",\n        \"cancer_giliver\",\n        \"cancer_giliver_year\",\n        \"cancer_headneck\",\n        \"cancer_headneck_year\",\n        \"cancer_kidney\",\n        \"cancer_kidney_year\",\n        \"cancer_leuklymph\",\n        \"cancer_leuklymph_year\",\n        \"cancer_lung\",\n        \"cancer_lung_year\",\n        \"cancer_melanoma\",\n        \"cancer_melanoma_year\",\n        \"cancer_metase\",\n        \"cancer_metase_year\",\n        \"cancer_other\",\n        \"cancer_other_year\",\n        \"cancer_ovarian\",\n        \"cancer_ovarian_year\",\n        \"cancer_prostate\",\n        \"cancer_prostate_year\",\n        \"cardio_other\",\n        \"cataracts\",        \n        \"ckd\",\n        \"coag\",\n        \"coeliac\",\n        \"contraception\",\n        \"copd\",\n        \"cystic_fibrosis\",\n        \"dementia\",\n        \"dep_alcohol\",\n        \"dep_benzo\",\n        \"dep_cannabis\",\n        \"dep_cocaine\",\n        \"dep_opioid\",\n        \"dep_other\",\n        \"depression\",\n        \"diabetes_1\",\n        \"diabetes_2\",\n        \"diabetes_gest\",\n        \"diabetes_retina\",\n        \"disorder_eating\",\n        \"disorder_pers\",\n        \"dna_cpr\",\n        \"eczema\",\n        \"endocrine_other\",\n        \"endometriosis\",\n        \"eol_plan\",\n        \"epaccs\",\n        \"epilepsy\",\n        \"fatigue\",\n        \"fragility\",\n        \"gout\",\n        \"has_carer\",\n        \"health_check\",\n        \"hearing_impair\",\n        \"hep_b\",\n        \"hep_c\",\n        \"hf\",\n        \"hiv\",\n        \"homeless\",\n        \"housebound\",\n        \"ht\",\n        \"ibd\",\n        \"ibs\",\n        \"ihd_mi\",\n        \"ihd_nonmi\",\n        \"incont_urinary\",\n        \"inflam_arthritic\",\n        \"is_carer\",\n        \"learning_diff\",\n        \"learning_dis\",\n        \"live_birth\",\n        \"liver_alcohol\",\n        \"liver_nafl\",\n        \"liver_other\",\n        \"lung_restrict\",\n        \"macular_degen\",\n        \"measles_mumps\",\n        \"migraine\",\n        \"miscarriage\",\n        \"mmr1\",\n        \"mmr2\",\n        \"mnd\",\n        \"ms\",\n        \"neuro_pain\",\n        \"neuro_various\",\n        \"newborn_check\",\n        \"nh_rh\",\n        \"nose\",\n        \"obesity\",\n        \"organ_transplant\",\n        \"osteoarthritis\",\n        \"osteoporosis\",\n        \"parkinsons\",\n        \"pelvic\",\n        \"phys_disability\",\n        \"poly_ovary\",\n        \"pre_diabetes\",\n        \"pregnancy\",\n        \"psoriasis\",\n        \"ptsd\",\n        \"qof_af\",\n        \"qof_asthma\",\n        \"qof_chd\",\n        \"qof_ckd\",\n        \"qof_copd\",\n        \"qof_dementia\",\n        \"qof_depression\",\n        \"qof_diabetes\",\n        \"qof_epilepsy\",\n        \"qof_hf\",\n        \"qof_ht\",\n        \"qof_learndis\",\n        \"qof_mental\",\n        \"qof_obesity\",\n        \"qof_osteoporosis\",\n        \"qof_pad\",\n        \"qof_pall\",\n        \"qof_rheumarth\",\n        \"qof_stroke\",\n        \"sad\",\n        \"screen_aaa\",\n        \"screen_bowel\",\n        \"screen_breast\",\n        \"screen_cervical\",\n        \"screen_eye\",\n        \"self_harm\",\n        \"sickle\",\n        \"smi\",\n        \"stomach\",\n        \"stroke\",\n        \"tb\",\n        \"thyroid\",\n        \"uterine\",\n        \"vasc_dis\",\n        \"veteran\",\n        \"visual_impair\",\n    ]\n\n    df = primary_care_attributes.copy()\n    df[flag_columns] = (\n        df[flag_columns].astype(\"float\").fillna(0).astype(\"Int8\")\n    )\n    return df\n</code></pre>"},{"location":"reference/#pyhbr.tools","title":"<code>tools</code>","text":""},{"location":"reference/#pyhbr.tools.fetch_data","title":"<code>fetch_data</code>","text":"<p>Fetch raw data from the database and save it to a file</p>"},{"location":"reference/#pyhbr.tools.generate_report","title":"<code>generate_report</code>","text":"<p>Generate the report folder from a config file and model data</p>"},{"location":"reference/#pyhbr.tools.plot_describe","title":"<code>plot_describe</code>","text":""},{"location":"reference/#pyhbr.tools.plot_describe.plot_or_save","title":"<code>plot_or_save(plot, name, save_dir)</code>","text":"<p>Plot the graph interactively or save the figure</p> <p>Parameters:</p> Name Type Description Default <code>plot</code> <code>bool</code> <p>If true, plot interactively and don't save. Otherwise, save</p> required <code>name</code> <code>str</code> <p>The filename (without the .png) to save the figure has</p> required <code>save_dir</code> <code>str</code> <p>The directory in which to save the figure</p> required Source code in <code>src\\pyhbr\\tools\\plot_describe.py</code> <pre><code>def plot_or_save(plot: bool, name: str, save_dir: str):\n    \"\"\"Plot the graph interactively or save the figure\n\n    Args:\n        plot: If true, plot interactively and don't save. Otherwise, save\n        name: The filename (without the .png) to save the figure has\n        save_dir: The directory in which to save the figure\n    \"\"\"\n    if plot:\n        log.info(f\"Plotting {name}, not saving\")\n        plt.show()\n    else:\n        log.info(f\"Saving figure {name} in {save_dir}\")\n        plt.savefig(common.make_new_save_item_path(name, save_dir, \"png\"))\n</code></pre>"},{"location":"reference/#pyhbr.tools.run_model","title":"<code>run_model</code>","text":""},{"location":"reference/#pyhbr.tools.run_model.fit_and_save","title":"<code>fit_and_save(model_name, config, pipe, X_train, y_train, X_test, y_test, data_file, random_state)</code>","text":"<p>Fit the model and save the results</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model, a key under the \"models\" top-level key in the config file</p> required <code>config</code> <code>dict[str, Any]</code> <p>The config file as a dictionary</p> required <code>X_train</code> <code>DataFrame</code> <p>The features training dataframe</p> required <code>y_train</code> <code>DataFrame</code> <p>The outcomes training dataframe</p> required <code>X_test</code> <code>DataFrame</code> <p>The features testing dataframe</p> required <code>y_test</code> <code>DataFrame</code> <p>The outcomes testing dataframe</p> required <code>data_file</code> <code>str</code> <p>The name of the raw data file used for the modelling</p> required <code>random_state</code> <code>RandomState</code> <p>The source of randomness used by the model</p> required Source code in <code>src\\pyhbr\\tools\\run_model.py</code> <pre><code>def fit_and_save(\n    model_name: str,\n    config: dict[str, Any],\n    pipe: Pipeline,\n    X_train: DataFrame,\n    y_train: DataFrame,\n    X_test: DataFrame,\n    y_test: DataFrame,\n    data_file: str,\n    random_state: RandomState,\n) -&gt; None:\n    \"\"\"Fit the model and save the results\n\n    Args:\n        model_name: The name of the model, a key under the \"models\" top-level\n            key in the config file\n        config: The config file as a dictionary\n        X_train: The features training dataframe\n        y_train: The outcomes training dataframe\n        X_test: The features testing dataframe\n        y_test: The outcomes testing dataframe\n        data_file: The name of the raw data file used for the modelling\n        random_state: The source of randomness used by the model\n    \"\"\"\n\n    print(\"Starting fit\")\n\n    # Using a larger number of bootstrap resamples will make\n    # the stability analysis better, but will take longer to fit.\n    num_bootstraps = config[\"num_bootstraps\"]\n\n    # Choose the number of bins for the calibration calculation.\n    # Using more bins will resolve the risk estimates more\n    # precisely, but will reduce the sample size in each bin for\n    # estimating the prevalence.\n    num_bins = config[\"num_bins\"]\n\n    # Fit the model, and also fit bootstrapped models (using resamples\n    # of the training set) to assess stability.\n    fit_results = fit.fit_model(\n        pipe, X_train, y_train, X_test, y_test, num_bootstraps, num_bins, random_state\n    )\n\n    # Save the fitted models\n    model_data = {\n        \"name\": model_name,\n        \"config\": config,\n        \"fit_results\": fit_results,\n        \"X_train\": X_train,\n        \"X_test\": X_test,\n        \"y_train\": y_train,\n        \"y_test\": y_test,\n        \"data_file\": data_file,\n    }\n\n    analysis_name = config[\"analysis_name\"]\n\n    # If the branch is not clean, prompt the user to commit to avoid losing\n    # long-running model results. Take care to only commit if the state of\n    # the repository truly reflects what was run (i.e. if no changes were made\n    # while the script was running).\n    retry_save = True\n    while retry_save:\n        try:\n            common.save_item(\n                model_data, f\"{analysis_name}_{model_name}\", save_dir=config[\"save_dir\"]\n            )\n            # Getting here successfully means that the save worked; exit the loop\n            log.info(\"Saved model\")\n            break\n        except RuntimeError as e:\n            print(e)\n            print(\"You can commit now and then retry the save after committing.\")\n            retry_save = common.query_yes_no(\n                \"Do you want to retry the save? Commit, then select yes, or choose no to exit the script.\"\n            )\n</code></pre>"},{"location":"reference/#pyhbr.tools.run_model.get_pipe_fn","title":"<code>get_pipe_fn(model_config)</code>","text":"<p>Get the pipe function based on the name in the config file</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>dict[str, str]</code> <p>The dictionary in models.{model_name} in the config file</p> required Source code in <code>src\\pyhbr\\tools\\run_model.py</code> <pre><code>def get_pipe_fn(model_config: dict[str, str]) -&gt; Callable:\n    \"\"\"Get the pipe function based on the name in the config file\n\n    Args:\n        model_config: The dictionary in models.{model_name} in\n            the config file\n    \"\"\"\n\n    # Make the preprocessing/fitting pipeline\n    pipe_fn_path = model_config[\"pipe_fn\"]\n    module_name, pipe_fn_name = pipe_fn_path.rsplit(\".\", 1)\n    module = importlib.import_module(module_name)\n    return getattr(module, pipe_fn_name)\n</code></pre>"},{"location":"reference/#analysis","title":"Analysis","text":"<p>Routines for performing statistics, analysis, or fitting models</p>"},{"location":"reference/#common-utilities","title":"Common Utilities","text":"<p>Common utilities for other modules.</p> <p>A collection of routines used by the data source or analysis functions.</p>"},{"location":"reference/#pyhbr.common.CheckedTable","title":"<code>CheckedTable</code>","text":"<p>Wrapper for sqlalchemy table with checks for table/columns</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>class CheckedTable:\n    \"\"\"Wrapper for sqlalchemy table with checks for table/columns\"\"\"\n\n    def __init__(self, table_name: str, engine: Engine, schema=\"dbo\") -&gt; None:\n        \"\"\"Get a CheckedTable by reading from the remote server\n\n        This is a wrapper around the sqlalchemy Table for\n        catching errors when accessing columns through the\n        c attribute.\n\n        Args:\n            table_name: The name of the table whose metadata should be retrieved\n            engine: The database connection\n\n        Returns:\n            The table data for use in SQL queries\n        \"\"\"\n        self.name = table_name\n        metadata_obj = MetaData(schema=schema)\n        try:\n            self.table = Table(self.name, metadata_obj, autoload_with=engine)\n        except NoSuchTableError as e:\n            raise RuntimeError(\n                f\"Could not find table '{e}' in database connection '{engine.url}'\"\n            ) from e\n\n    def col(self, column_name: str) -&gt; Column:\n        \"\"\"Get a column\n\n        Args:\n            column_name: The name of the column to fetch.\n\n        Raises:\n            RuntimeError: Thrown if the column does not exist\n        \"\"\"\n        try:\n            return self.table.c[column_name]\n        except AttributeError as e:\n            raise RuntimeError(\n                f\"Could not find column name '{column_name}' in table '{self.name}'\"\n            ) from e\n</code></pre>"},{"location":"reference/#pyhbr.common.CheckedTable.__init__","title":"<code>__init__(table_name, engine, schema='dbo')</code>","text":"<p>Get a CheckedTable by reading from the remote server</p> <p>This is a wrapper around the sqlalchemy Table for catching errors when accessing columns through the c attribute.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>The name of the table whose metadata should be retrieved</p> required <code>engine</code> <code>Engine</code> <p>The database connection</p> required <p>Returns:</p> Type Description <code>None</code> <p>The table data for use in SQL queries</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def __init__(self, table_name: str, engine: Engine, schema=\"dbo\") -&gt; None:\n    \"\"\"Get a CheckedTable by reading from the remote server\n\n    This is a wrapper around the sqlalchemy Table for\n    catching errors when accessing columns through the\n    c attribute.\n\n    Args:\n        table_name: The name of the table whose metadata should be retrieved\n        engine: The database connection\n\n    Returns:\n        The table data for use in SQL queries\n    \"\"\"\n    self.name = table_name\n    metadata_obj = MetaData(schema=schema)\n    try:\n        self.table = Table(self.name, metadata_obj, autoload_with=engine)\n    except NoSuchTableError as e:\n        raise RuntimeError(\n            f\"Could not find table '{e}' in database connection '{engine.url}'\"\n        ) from e\n</code></pre>"},{"location":"reference/#pyhbr.common.CheckedTable.col","title":"<code>col(column_name)</code>","text":"<p>Get a column</p> <p>Parameters:</p> Name Type Description Default <code>column_name</code> <code>str</code> <p>The name of the column to fetch.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Thrown if the column does not exist</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def col(self, column_name: str) -&gt; Column:\n    \"\"\"Get a column\n\n    Args:\n        column_name: The name of the column to fetch.\n\n    Raises:\n        RuntimeError: Thrown if the column does not exist\n    \"\"\"\n    try:\n        return self.table.c[column_name]\n    except AttributeError as e:\n        raise RuntimeError(\n            f\"Could not find column name '{column_name}' in table '{self.name}'\"\n        ) from e\n</code></pre>"},{"location":"reference/#pyhbr.common.chunks","title":"<code>chunks(patient_ids, n)</code>","text":"<p>Divide a list of patient ids into n-sized chunks</p> <p>The last chunk may be shorter.</p> <p>Parameters:</p> Name Type Description Default <code>patient_ids</code> <code>list[str]</code> <p>The List of IDs to chunk</p> required <code>n</code> <code>int</code> <p>The chunk size.</p> required <p>Returns:</p> Type Description <code>list[list[str]]</code> <p>A list containing chunks (list) of patient IDs</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def chunks(patient_ids: list[str], n: int) -&gt; list[list[str]]:\n    \"\"\"Divide a list of patient ids into n-sized chunks\n\n    The last chunk may be shorter.\n\n    Args:\n        patient_ids: The List of IDs to chunk\n        n: The chunk size.\n\n    Returns:\n        A list containing chunks (list) of patient IDs\n    \"\"\"\n    return [patient_ids[i : i + n] for i in range(0, len(patient_ids), n)]\n</code></pre>"},{"location":"reference/#pyhbr.common.current_commit","title":"<code>current_commit()</code>","text":"<p>Get current commit.</p> <p>Returns:</p> Type Description <code>str</code> <p>Get the first 12 characters of the current commit, using the first repository found above the current working directory. If the working directory is not in a git repository, return \"nogit\".</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def current_commit() -&gt; str:\n    \"\"\"Get current commit.\n\n    Returns:\n        Get the first 12 characters of the current commit,\n            using the first repository found above the current\n            working directory. If the working directory is not\n            in a git repository, return \"nogit\".\n    \"\"\"\n    try:\n        repo = Repo(search_parent_directories=True)\n        sha = repo.head.object.hexsha[0:11]\n        return sha\n    except InvalidGitRepositoryError:\n        return \"nogit\"\n</code></pre>"},{"location":"reference/#pyhbr.common.current_timestamp","title":"<code>current_timestamp()</code>","text":"<p>Get the current timestamp.</p> <p>Returns:</p> Type Description <code>int</code> <p>The current timestamp (since epoch) rounded to the nearest second.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def current_timestamp() -&gt; int:\n    \"\"\"Get the current timestamp.\n\n    Returns:\n        The current timestamp (since epoch) rounded\n            to the nearest second.\n    \"\"\"\n    return int(time())\n</code></pre>"},{"location":"reference/#pyhbr.common.get_data","title":"<code>get_data(engine, query, *args)</code>","text":"<p>Convenience function to make a query and fetch data.</p> <p>Wraps a function like hic.demographics_query with a call to pd.read_data.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>The database connection</p> required <code>query</code> <code>Callable[[Engine, ...], Select]</code> <p>A function returning a sqlalchemy Select statement</p> required <code>*args</code> <code>...</code> <p>Positional arguments to be passed to query in addition to engine (which is passed first). Make sure they are passed in the same order expected by the query function.</p> <code>()</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The pandas dataframe containing the SQL data</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def get_data(\n    engine: Engine, query: Callable[[Engine, ...], Select], *args: ...\n) -&gt; DataFrame:\n    \"\"\"Convenience function to make a query and fetch data.\n\n    Wraps a function like hic.demographics_query with a\n    call to pd.read_data.\n\n    Args:\n        engine: The database connection\n        query: A function returning a sqlalchemy Select statement\n        *args: Positional arguments to be passed to query in addition\n            to engine (which is passed first). Make sure they are passed\n            in the same order expected by the query function.\n\n    Returns:\n        The pandas dataframe containing the SQL data\n    \"\"\"\n    stmt = query(engine, *args)\n    df = read_sql(stmt, engine)\n\n    # Convert the column names to regular strings instead\n    # of sqlalchemy.sql.elements.quoted_name. This avoids\n    # an error down the line in sklearn, which cannot\n    # process sqlalchemy column title tuples.\n    df.columns = [str(col) for col in df.columns]\n\n    return df\n</code></pre>"},{"location":"reference/#pyhbr.common.get_data_by_patient","title":"<code>get_data_by_patient(engine, query, patient_ids, *args)</code>","text":"<p>Fetch data using a query restricted by patient ID</p> <p>The patient_id list is chunked into 2000 long batches to fit within an SQL IN clause, and each chunk is run as a separate query. The results are assembled into a single DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>The database connection</p> required <code>query</code> <code>Callable[[Engine, ...], Select]</code> <p>A function returning a sqlalchemy Select statement. Must take a list[str] as an argument after engine.</p> required <code>patient_ids</code> <code>list[str]</code> <p>A list of patient IDs to restrict the query.</p> required <code>*args</code> <code>...</code> <p>Further positional arguments that will be passed to the query function after the patient_ids positional argument.</p> <code>()</code> <p>Returns:</p> Type Description <code>list[DataFrame]</code> <p>A list of dataframes, one corresponding to each chunk.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def get_data_by_patient(\n    engine: Engine,\n    query: Callable[[Engine, ...], Select],\n    patient_ids: list[str],\n    *args: ...,\n) -&gt; list[DataFrame]:\n    \"\"\"Fetch data using a query restricted by patient ID\n\n    The patient_id list is chunked into 2000 long batches to fit\n    within an SQL IN clause, and each chunk is run as a separate\n    query. The results are assembled into a single DataFrame.\n\n    Args:\n        engine: The database connection\n        query: A function returning a sqlalchemy Select statement. Must\n            take a list[str] as an argument after engine.\n        patient_ids: A list of patient IDs to restrict the query.\n        *args: Further positional arguments that will be passed to the\n            query function after the patient_ids positional argument.\n\n    Returns:\n        A list of dataframes, one corresponding to each chunk.\n    \"\"\"\n    dataframes = []\n    patient_id_chunks = chunks(patient_ids, 2000)\n    num_chunks = len(patient_id_chunks)\n    chunk_count = 1\n    for chunk in patient_id_chunks:\n        print(f\"Fetching chunk {chunk_count}/{num_chunks}\")\n        dataframes.append(get_data(engine, query, chunk, *args))\n        chunk_count += 1\n    return dataframes\n</code></pre>"},{"location":"reference/#pyhbr.common.get_saved_files_by_name","title":"<code>get_saved_files_by_name(name, save_dir, extension)</code>","text":"<p>Get all saved data files matching name</p> <p>Get the list of files in the save_dir folder matching name. Return the result as a table of file path, commit hash, and saved date. The table is sorted by timestamp, with the most recent file first.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If save_dir does not exist, or there are files in save_dir within invalid file names (not in the format name_commit_timestamp.pkl).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the saved file to load. This matches name in the filename name_commit_timestamp.pkl.</p> required <code>save_dir</code> <code>str</code> <p>The directory to search for files.</p> required <code>extension</code> <code>str</code> <p>What file extension to look for. Do not include the dot.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe with columns <code>path</code>, <code>commit</code> and <code>created_data</code>.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def get_saved_files_by_name(name: str, save_dir: str, extension: str) -&gt; DataFrame:\n    \"\"\"Get all saved data files matching name\n\n    Get the list of files in the save_dir folder matching\n    name. Return the result as a table of file path, commit\n    hash, and saved date. The table is sorted by timestamp,\n    with the most recent file first.\n\n    Raises:\n        RuntimeError: If save_dir does not exist, or there are files\n            in save_dir within invalid file names (not in the format\n            name_commit_timestamp.pkl).\n\n    Args:\n        name: The name of the saved file to load. This matches name in\n            the filename name_commit_timestamp.pkl.\n        save_dir: The directory to search for files.\n        extension: What file extension to look for. Do not include the dot.\n\n    Returns:\n        A dataframe with columns `path`, `commit` and `created_data`.\n    \"\"\"\n\n    # Check for missing datasets directory\n    if not os.path.isdir(save_dir):\n        raise RuntimeError(\n            f\"Missing folder '{save_dir}'. Check your working directory.\"\n        )\n\n    # Read all the .pkl files in the directory\n    files = DataFrame({\"path\": os.listdir(save_dir)})\n\n    # Identify the file name part. The horrible regex matches the\n    # expression _[commit_hash]_[timestamp].pkl. It is important to\n    # match this part, because \"anything\" can happen in the name part\n    # (including underscores and letters and numbers), so splitting on\n    # _ would not work. The name can then be removed.\n    files[\"name\"] = files[\"path\"].str.replace(\n        rf\"_([0-9]|[a-zA-Z])*_\\d*\\.{extension}\", \"\", regex=True\n    )\n\n    # Remove all the files whose name does not match, and drop\n    # the name from the path\n    files = files[files[\"name\"] == name]\n    if files.shape[0] == 0:\n        raise ValueError(\n            f\"There is no file with the name '{name}' in the datasets directory\"\n        )\n    files[\"commit_and_timestamp\"] = files[\"path\"].str.replace(name + \"_\", \"\")\n\n    # Split the commit and timestamp up (note also the extension)\n    try:\n        files[[\"commit\", \"timestamp\", \"extension\"]] = files[\n            \"commit_and_timestamp\"\n        ].str.split(r\"_|\\.\", expand=True)\n    except Exception as exc:\n        raise RuntimeError(\n            \"Failed to parse files in the datasets folder. \"\n            \"Ensure that all files have the correct format \"\n            \"name_commit_timestamp.extension, and \"\n            \"remove any files not matching this \"\n            \"pattern. TODO handle this error properly, \"\n            \"see save_datasets.py.\"\n        ) from exc\n\n    files[\"created_date\"] = to_datetime(files[\"timestamp\"].astype(int), unit=\"s\")\n    recent_first = files.sort_values(by=\"timestamp\", ascending=False).reset_index()[\n        [\"path\", \"commit\", \"created_date\"]\n    ]\n    return recent_first\n</code></pre>"},{"location":"reference/#pyhbr.common.load_exact_item","title":"<code>load_exact_item(name, save_dir='save_data')</code>","text":"<p>Load a previously saved item (pickle) from file by exact filename</p> <p>This is similar to load_item, but loads the exact filename given by name instead of looking for the most recent file. name must contain the commit, timestamp, and file extension.</p> <p>A RuntimeError is raised if the file does not exist.</p> <p>To load an item that is an object from a library (e.g. a pandas DataFrame), the library must be installed (otherwise you will get a ModuleNotFound exception). However, you do not have to import the library before calling this function.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the item to load</p> required <code>save_fir</code> <p>Which folder to load the item from.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The data item loaded.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def load_exact_item(\n    name: str, save_dir: str = \"save_data\"\n) -&gt; Any:\n    \"\"\"Load a previously saved item (pickle) from file by exact filename\n\n    This is similar to load_item, but loads the exact filename given by name\n    instead of looking for the most recent file. name must contain the\n    commit, timestamp, and file extension.\n\n    A RuntimeError is raised if the file does not exist.\n\n    To load an item that is an object from a library (e.g. a pandas DataFrame),\n    the library must be installed (otherwise you will get a ModuleNotFound\n    exception). However, you do not have to import the library before calling this\n    function.\n\n    Args:\n        name: The name of the item to load\n        save_fir: Which folder to load the item from.\n\n    Returns:\n        The data item loaded. \n\n    \"\"\"\n\n    # Make the path to the file\n    file_path = Path(save_dir) / Path(name)\n\n    # If the file does not exist, raise an error\n    if not file_path.exists():\n        raise RuntimeError(f\"The file {name} does not exist in the directory {save_dir}\")\n\n    # Load a generic pickle. Note that if this is a pandas dataframe,\n    # pandas must be installed (otherwise you will get module not found).\n    # The same goes for a pickle storing an object from any other library.\n    with open(file_path, \"rb\") as file:\n        return pickle.load(file)\n</code></pre>"},{"location":"reference/#pyhbr.common.load_item","title":"<code>load_item(name, interactive=False, save_dir='save_data')</code>","text":"<p>Load a previously saved item (pickle) from file</p> <p>Use this function to load a file that was previously saved using save_item(). By default, the latest version of the item will be returned (the one with the most recent timestamp).</p> <p>None is returned if an interactive load is cancelled by the user.</p> <p>To load an item that is an object from a library (e.g. a pandas DataFrame), the library must be installed (otherwise you will get a ModuleNotFound exception). However, you do not have to import the library before calling this function.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the item to load</p> required <code>interactive</code> <code>bool</code> <p>If True, let the user pick which item version to load interactively. If False, non-interactively load the most recent item (i.e. with the most recent timestamp). The commit hash is not considered when loading the item.</p> <code>False</code> <code>save_fir</code> <p>Which folder to load the item from.</p> required <p>Returns:</p> Type Description <code>(Any, Path)</code> <p>A tuple, with the python object loaded from file as first element and the Path to the item as the second element, or None if the user cancelled an interactive load.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def load_item(\n    name: str, interactive: bool = False, save_dir: str = \"save_data\"\n) -&gt; (Any, Path):\n    \"\"\"Load a previously saved item (pickle) from file\n\n    Use this function to load a file that was previously saved using\n    save_item(). By default, the latest version of the item will be returned\n    (the one with the most recent timestamp).\n\n    None is returned if an interactive load is cancelled by the user.\n\n    To load an item that is an object from a library (e.g. a pandas DataFrame),\n    the library must be installed (otherwise you will get a ModuleNotFound\n    exception). However, you do not have to import the library before calling this\n    function.\n\n    Args:\n        name: The name of the item to load\n        interactive: If True, let the user pick which item version to load interactively.\n            If False, non-interactively load the most recent item (i.e. with the most\n            recent timestamp). The commit hash is not considered when loading the item.\n        save_fir: Which folder to load the item from.\n\n    Returns:\n        A tuple, with the python object loaded from file as first element and the\n            Path to the item as the second element, or None if the user cancelled\n            an interactive load.\n\n    \"\"\"\n    if interactive:\n        item_path = pick_saved_file_interactive(name, save_dir, \"pkl\")\n    else:\n        item_path = pick_most_recent_saved_file(name, save_dir, \"pkl\")\n\n    if item_path is None:\n        print(\"Aborted (interactive) load item\")\n        return None, None\n\n    print(f\"Loading {item_path}\")\n\n    # Load a generic pickle. Note that if this is a pandas dataframe,\n    # pandas must be installed (otherwise you will get module not found).\n    # The same goes for a pickle storing an object from any other library.\n    with open(item_path, \"rb\") as file:\n        return pickle.load(file), item_path\n</code></pre>"},{"location":"reference/#pyhbr.common.load_most_recent_data_files","title":"<code>load_most_recent_data_files(analysis_name, save_dir)</code>","text":"<p>Load the most recent timestamp data file matching the analysis name</p> <p>The data file is a pickle of a dictionary, containing pandas DataFrames and other metadata. It is expected to contain a \"raw_file\" key, which contains the path to the associated raw data file.</p> <p>Both files are loaded, and a tuple of all the data is returned</p> <p>Parameters:</p> Name Type Description Default <code>analysis_name</code> <code>str</code> <p>The \"analysis_name\" key from the config file, which is the filename prefix</p> required <code>save_dir</code> <code>str</code> <p>The folder to load the data from</p> required <p>Returns:</p> Type Description <code>(dict[str, Any], dict[str, Any], str)</code> <p>(data, raw_data, data_path). data and raw_data are dictionaries containing (mainly) Pandas DataFrames, and data_path is the path to the data file (this can be stored in any output products from this script to record which data file was used to generate the data.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def load_most_recent_data_files(analysis_name: str, save_dir: str) -&gt; (dict[str, Any], dict[str, Any], str):\n    \"\"\"Load the most recent timestamp data file matching the analysis name\n\n    The data file is a pickle of a dictionary, containing pandas DataFrames and\n    other metadata. It is expected to contain a \"raw_file\" key, which contains\n    the path to the associated raw data file.\n\n    Both files are loaded, and a tuple of all the data is returned\n\n    Args:\n        analysis_name: The \"analysis_name\" key from the config file, which is the filename prefix\n        save_dir: The folder to load the data from\n\n    Returns:\n        (data, raw_data, data_path). data and raw_data are dictionaries containing\n            (mainly) Pandas DataFrames, and data_path is the path to the data\n            file (this can be stored in any output products from this script to\n            record which data file was used to generate the data.\n    \"\"\"\n\n    item_name = f\"{analysis_name}_data\"\n    log.info(f\"Loading most recent data file '{item_name}'\")\n    data, data_path = load_item(item_name, save_dir=save_dir)\n\n    raw_file = data[\"raw_file\"]\n    log.info(f\"Loading the underlying raw data file '{raw_file}'\")\n    raw_data = load_exact_item(raw_file, save_dir=save_dir)\n\n    log.info(f\"Items in the data file {data.keys()}\")\n    log.info(f\"Items in the raw data file: {raw_data.keys()}\")\n\n    return data, raw_data, data_path\n</code></pre>"},{"location":"reference/#pyhbr.common.make_engine","title":"<code>make_engine(con_string='mssql+pyodbc://dsn', database='hic_cv_test')</code>","text":"<p>Make a sqlalchemy engine</p> <p>This function is intended for use with Microsoft SQL Server. The preferred method to connect to the server on Windows is to use a Data Source Name (DSN). To use the default connection string argument, set up a data source name called \"dsn\" using the program \"ODBC Data Sources\".</p> <p>If you need to access multiple different databases on the same server, you will need different engines. Specify the database name while creating the engine (this will override a default database in the DSN, if there is one).</p> <p>Parameters:</p> Name Type Description Default <code>con_string</code> <code>str</code> <p>The sqlalchemy connection string.</p> <code>'mssql+pyodbc://dsn'</code> <code>database</code> <code>str</code> <p>The database name to connect to.</p> <code>'hic_cv_test'</code> <p>Returns:</p> Type Description <code>Engine</code> <p>The sqlalchemy engine</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def make_engine(\n    con_string: str = \"mssql+pyodbc://dsn\", database: str = \"hic_cv_test\"\n) -&gt; Engine:\n    \"\"\"Make a sqlalchemy engine\n\n    This function is intended for use with Microsoft SQL\n    Server. The preferred method to connect to the server\n    on Windows is to use a Data Source Name (DSN). To use the\n    default connection string argument, set up a data source\n    name called \"dsn\" using the program \"ODBC Data Sources\".\n\n    If you need to access multiple different databases on the\n    same server, you will need different engines. Specify the\n    database name while creating the engine (this will override\n    a default database in the DSN, if there is one).\n\n    Args:\n        con_string: The sqlalchemy connection string.\n        database: The database name to connect to.\n\n    Returns:\n        The sqlalchemy engine\n    \"\"\"\n    connect_args = {\"database\": database}\n    return create_engine(con_string, connect_args=connect_args)\n</code></pre>"},{"location":"reference/#pyhbr.common.make_new_save_item_path","title":"<code>make_new_save_item_path(name, save_dir, extension)</code>","text":"<p>Make the path to save a new item to the save_dir</p> <p>The name will have the format name_{current_common}_{timestamp}.{extension}.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The base name for the new filename</p> required <code>save_dir</code> <code>str</code> <p>The folder in which to place the item</p> required <code>extension</code> <code>str</code> <p>The file extension (omit the dot)</p> required <p>Returns:</p> Type Description <code>Path</code> <p>The relative path to the new object to be saved</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def make_new_save_item_path(name: str, save_dir: str, extension: str) -&gt; Path:\n    \"\"\"Make the path to save a new item to the save_dir\n\n    The name will have the format name_{current_common}_{timestamp}.{extension}.\n\n    Args:\n        name: The base name for the new filename\n        save_dir: The folder in which to place the item\n        extension: The file extension (omit the dot)\n\n    Returns:\n        The relative path to the new object to be saved\n    \"\"\"\n\n    # Make the file suffix out of the current git\n    # commit hash and the current time\n    filename = f\"{name}_{current_commit()}_{current_timestamp()}.{extension}\"\n    return Path(save_dir) / Path(filename)\n</code></pre>"},{"location":"reference/#pyhbr.common.mean_confidence_interval","title":"<code>mean_confidence_interval(data, confidence=0.95)</code>","text":"<p>Compute the confidence interval around the mean</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Series</code> <p>A series of numerical values to compute the confidence interval.</p> required <code>confidence</code> <code>float</code> <p>The confidence interval to compute.</p> <code>0.95</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>A map containing the keys \"mean\", \"lower\", and \"upper\". The latter keys contain the confidence interval limits.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def mean_confidence_interval(\n    data: Series, confidence: float = 0.95\n) -&gt; dict[str, float]:\n    \"\"\"Compute the confidence interval around the mean\n\n    Args:\n        data: A series of numerical values to compute the confidence interval.\n        confidence: The confidence interval to compute.\n\n    Returns:\n        A map containing the keys \"mean\", \"lower\", and \"upper\". The latter\n            keys contain the confidence interval limits.\n    \"\"\"\n    a = 1.0 * np.array(data)\n    n = len(a)\n    mean = np.mean(a)\n    standard_error = scipy.stats.sem(a)\n\n    # Check this\n    half_width = standard_error * scipy.stats.t.ppf((1 + confidence) / 2.0, n - 1)\n    return {\n        \"mean\": mean,\n        \"confidence\": confidence,\n        \"lower\": mean - half_width,\n        \"upper\": mean + half_width,\n    }\n</code></pre>"},{"location":"reference/#pyhbr.common.median_to_string","title":"<code>median_to_string(instability, unit='%')</code>","text":"<p>Convert the median-quartile DataFrame to a String</p> <p>Parameters:</p> Name Type Description Default <code>instability</code> <code>DataFrame</code> <p>Table containing three rows, indexed by 0.5 (median), 0.25 (lower quartile) and 0.75 (upper quartile).</p> required <code>unit</code> <p>What units to add to the values in the string.</p> <code>'%'</code> <p>Returns:</p> Type Description <code>str</code> <p>A string containing the median, and the lower and upper quartiles.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def median_to_string(instability: DataFrame, unit=\"%\") -&gt; str:\n    \"\"\"Convert the median-quartile DataFrame to a String\n\n    Args:\n        instability: Table containing three rows, indexed by\n            0.5 (median), 0.25 (lower quartile) and 0.75\n            (upper quartile).\n        unit: What units to add to the values in the string.\n\n    Returns:\n        A string containing the median, and the lower and upper\n            quartiles.\n    \"\"\"\n    return f\"{instability.loc[0.5]:.2f}{unit} Q [{instability.loc[0.025]:.2f}{unit}, {instability.loc[0.975]:.2f}{unit}]\"\n</code></pre>"},{"location":"reference/#pyhbr.common.pick_most_recent_saved_file","title":"<code>pick_most_recent_saved_file(name, save_dir, extension='pkl')</code>","text":"<p>Get the path to the most recent file matching name.</p> <p>Like pick_saved_file_interactive, but automatically selects the most recent file in save_data.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the saved file to list</p> required <code>save_dir</code> <code>str</code> <p>The directory to search for files</p> required <code>extension</code> <code>str</code> <p>What file extension to look for. Do not include the dot.</p> <code>'pkl'</code> <p>Returns:</p> Type Description <code>Path</code> <p>The relative path to the most recent matching file.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def pick_most_recent_saved_file(\n    name: str, save_dir: str, extension: str = \"pkl\"\n) -&gt; Path:\n    \"\"\"Get the path to the most recent file matching name.\n\n    Like pick_saved_file_interactive, but automatically selects the most\n    recent file in save_data.\n\n    Args:\n        name: The name of the saved file to list\n        save_dir: The directory to search for files\n        extension: What file extension to look for. Do not include the dot.\n\n    Returns:\n        The relative path to the most recent matching file.\n    \"\"\"\n    recent_first = get_saved_files_by_name(name, save_dir, extension)\n    return Path(save_dir) / Path(recent_first.loc[0, \"path\"])\n</code></pre>"},{"location":"reference/#pyhbr.common.pick_saved_file_interactive","title":"<code>pick_saved_file_interactive(name, save_dir, extension='pkl')</code>","text":"<p>Select a file matching name interactively</p> <p>Print a list of the saved items in the save_dir folder, along with the date and time it was generated, and the commit hash, and let the user pick which item should be loaded interactively. The full filename of the resulting file is returned, which can then be read by the user.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the saved file to list</p> required <code>save_dir</code> <code>str</code> <p>The directory to search for files</p> required <code>extension</code> <code>str</code> <p>What file extension to look for. Do not include the dot.</p> <code>'pkl'</code> <p>Returns:</p> Type Description <code>str | None</code> <p>The absolute path to the interactively selected file, or None if the interactive load was aborted.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def pick_saved_file_interactive(\n    name: str, save_dir: str, extension: str = \"pkl\"\n) -&gt; str | None:\n    \"\"\"Select a file matching name interactively\n\n    Print a list of the saved items in the save_dir folder, along\n    with the date and time it was generated, and the commit hash,\n    and let the user pick which item should be loaded interactively.\n    The full filename of the resulting file is returned, which can\n    then be read by the user.\n\n    Args:\n        name: The name of the saved file to list\n        save_dir: The directory to search for files\n        extension: What file extension to look for. Do not include the dot.\n\n    Returns:\n        The absolute path to the interactively selected file, or None\n            if the interactive load was aborted.\n    \"\"\"\n\n    recent_first = get_saved_files_by_name(name, save_dir, extension)\n    print(recent_first)\n\n    num_datasets = recent_first.shape[0]\n    while True:\n        try:\n            raw_choice = input(\n                f\"Pick a dataset to load: [{0} - {num_datasets-1}] (type q[uit]/exit, then Enter, to quit): \"\n            )\n            if \"exit\" in raw_choice or \"q\" in raw_choice:\n                return None\n            choice = int(raw_choice)\n        except Exception:\n            print(f\"{raw_choice} is not valid; try again.\")\n            continue\n        if choice &lt; 0 or choice &gt;= num_datasets:\n            print(f\"{choice} is not in range; try again.\")\n            continue\n        break\n\n    full_path = os.path.join(save_dir, recent_first.loc[choice, \"path\"])\n    return full_path\n</code></pre>"},{"location":"reference/#pyhbr.common.query_yes_no","title":"<code>query_yes_no(question, default='yes')</code>","text":"<p>Ask a yes/no question via raw_input() and return their answer.</p> <p>From https://stackoverflow.com/a/3041990.</p> <p>\"question\" is a string that is presented to the user. \"default\" is the presumed answer if the user just hits .         It must be \"yes\" (the default), \"no\" or None (meaning         an answer is required of the user). <p>The \"answer\" return value is True for \"yes\" or False for \"no\".</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def query_yes_no(question, default=\"yes\"):\n    \"\"\"Ask a yes/no question via raw_input() and return their answer.\n\n    From https://stackoverflow.com/a/3041990.\n\n    \"question\" is a string that is presented to the user.\n    \"default\" is the presumed answer if the user just hits &lt;Enter&gt;.\n            It must be \"yes\" (the default), \"no\" or None (meaning\n            an answer is required of the user).\n\n    The \"answer\" return value is True for \"yes\" or False for \"no\".\n    \"\"\"\n    valid = {\"yes\": True, \"y\": True, \"ye\": True, \"no\": False, \"n\": False}\n    if default is None:\n        prompt = \" [y/n] \"\n    elif default == \"yes\":\n        prompt = \" [Y/n] \"\n    elif default == \"no\":\n        prompt = \" [y/N] \"\n    else:\n        raise ValueError(\"invalid default answer: '%s'\" % default)\n\n    while True:\n        sys.stdout.write(question + prompt)\n        choice = input().lower()\n        if default is not None and choice == \"\":\n            return valid[default]\n        elif choice in valid:\n            return valid[choice]\n        else:\n            sys.stdout.write(\"Please respond with 'yes' or 'no' \" \"(or 'y' or 'n').\\n\")\n</code></pre>"},{"location":"reference/#pyhbr.common.read_config_file","title":"<code>read_config_file(yaml_path)</code>","text":"<p>Read the configuration file from</p> <p>Parameters:</p> Name Type Description Default <code>yaml_path</code> <code>str</code> <p>The path to the experiment config file</p> required Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def read_config_file(yaml_path: str):\n    \"\"\"Read the configuration file from\n\n    Args:\n        yaml_path: The path to the experiment config file\n    \"\"\"\n    # Read the configuration file\n    with open(yaml_path) as stream:\n        try:\n            return yaml.safe_load(stream)\n        except yaml.YAMLError as exc:\n            print(f\"Failed to load config file: {exc}\")\n            exit(1)\n</code></pre>"},{"location":"reference/#pyhbr.common.requires_commit","title":"<code>requires_commit()</code>","text":"<p>Check whether changes need committing</p> <p>To make most effective use of the commit hash stored with a save_item call, the current branch should be clean (all changes committed). Call this function to check.</p> <p>Returns False if there is no git repository.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the working directory is in a git repository that requires a commit; False otherwise.</p> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def requires_commit() -&gt; bool:\n    \"\"\"Check whether changes need committing\n\n    To make most effective use of the commit hash stored with a\n    save_item call, the current branch should be clean (all changes\n    committed). Call this function to check.\n\n    Returns False if there is no git repository.\n\n    Returns:\n        True if the working directory is in a git repository that requires\n            a commit; False otherwise.\n    \"\"\"\n    try:\n        repo = Repo(search_parent_directories=True)\n        return repo.is_dirty(untracked_files=True)\n    except InvalidGitRepositoryError:\n        # No need to commit if not repository\n        return False\n</code></pre>"},{"location":"reference/#pyhbr.common.save_item","title":"<code>save_item(item, name, save_dir='save_data/', enforce_clean_branch=True, prompt_commit=False)</code>","text":"<p>Save an item to a pickle file</p> <p>Saves a python object (e.g. a pandas DataFrame) dataframe in the save_dir folder, using a filename that includes the current timestamp and the current commit hash. Use load_item to retrieve the file.</p> <p>Important</p> <p>Ensure that <code>save_data/</code> (or your chosen <code>save_dir</code>) is added to the .gitignore of your repository to ensure sensitive data is not committed.</p> <p>By storing the commit hash and timestamp, it is possible to identify when items were created and what code created them. To make most effective use of the commit hash, ensure that you commit, and do not make any further code edits, before running a script that calls save_item (otherwise the commit hash will not quite reflect the state of the running code).</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Any</code> <p>The python object to save (e.g. pandas DataFrame)</p> required <code>name</code> <code>str</code> <p>The name of the item. The filename will be created by adding a suffix for the current commit and the timestamp to show when the data was saved (format: <code>name_commit_timestamp.pkl</code>)</p> required <code>save_dir</code> <code>str</code> <p>Where to save the data, relative to the current working directory. The directory will be created if it does not exist.</p> <code>'save_data/'</code> <code>enforce_clean_branch</code> <p>If True, the function will raise an exception if an attempt is made to save an item when the repository has uncommitted changes.</p> <code>True</code> <code>prompt_commit</code> <p>if enforce_clean_branch is true, choose whether the prompt the user to commit on an unclean branch. This can help avoiding losing the results of a long-running script. Prefer to use false if the script is cheap to run.</p> <code>False</code> Source code in <code>src\\pyhbr\\common.py</code> <pre><code>def save_item(\n    item: Any,\n    name: str,\n    save_dir: str = \"save_data/\",\n    enforce_clean_branch=True,\n    prompt_commit=False,\n) -&gt; None:\n    \"\"\"Save an item to a pickle file\n\n    Saves a python object (e.g. a pandas DataFrame) dataframe in the save_dir\n    folder, using a filename that includes the current timestamp and the current\n    commit hash. Use load_item to retrieve the file.\n\n    !!! important\n        Ensure that `save_data/` (or your chosen `save_dir`) is added to the\n        .gitignore of your repository to ensure sensitive data is not committed.\n\n    By storing the commit hash and timestamp, it is possible to identify when items\n    were created and what code created them. To make most effective use of the\n    commit hash, ensure that you commit, and do not make any further code edits,\n    before running a script that calls save_item (otherwise the commit hash will\n    not quite reflect the state of the running code).\n\n    Args:\n        item: The python object to save (e.g. pandas DataFrame)\n        name: The name of the item. The filename will be created by adding\n            a suffix for the current commit and the timestamp to show when the\n            data was saved (format: `name_commit_timestamp.pkl`)\n        save_dir: Where to save the data, relative to the current working directory.\n            The directory will be created if it does not exist.\n        enforce_clean_branch: If True, the function will raise an exception if an attempt\n            is made to save an item when the repository has uncommitted changes.\n        prompt_commit: if enforce_clean_branch is true, choose whether the prompt the\n            user to commit on an unclean branch. This can help avoiding losing\n            the results of a long-running script. Prefer to use false if the script\n            is cheap to run.\n    \"\"\"\n\n    if enforce_clean_branch:\n\n        abort_msg = \"Aborting save_item() because branch is not clean. Commit your changes before saving item to increase the chance of reproducing the item based on the filename commit hash.\"\n\n        if prompt_commit:\n            # If the branch is not clean, prompt the user to commit to avoid losing\n            # long-running model results. Take care to only commit if the state of\n            # the repository truly reflects what was run (i.e. if no changes were made\n            # while the script was running).\n            while requires_commit():\n                print(abort_msg)\n                print(\n                    \"You can commit now and then retry the save after committing.\"\n                )\n                retry_save = query_yes_no(\n                    \"Do you want to retry the save? Commit, then select yes, or choose no to abort the save.\"\n                )\n\n                if not retry_save:\n                    print(f\"Aborting save of {name}\")\n                    return\n\n            # If we get out the loop without returning, then the branch\n            # is not clean and the save can proceed.\n            print(\"Branch now clean, proceeding to save\")\n\n        else:\n\n            if requires_commit():\n                # In this case, unconditionally throw an error\n                raise RuntimeError(abort_msg)\n\n    if not Path(save_dir).exists():\n        print(f\"Creating missing folder '{save_dir}' for storing item\")\n        Path(save_dir).mkdir(parents=True, exist_ok=True)\n\n    path = make_new_save_item_path(name, save_dir, \"pkl\")\n    with open(path, \"wb\") as file:\n        print(f\"Saving {str(path)}\")\n        pickle.dump(item, file)\n</code></pre>"},{"location":"verification/","title":"Bleeding/Ischaemia Risk Estimate Verification","text":"<p>The purpose of the bleeding and ischaemia models is to assess the trade-off between bleeding and ischaemia risk.</p>"},{"location":"verification/#separate-verification-of-each-model","title":"Separate Verification of Each Model","text":""},{"location":"verification/#verification-of-the-trade-off","title":"Verification of the Trade-off","text":""}]}